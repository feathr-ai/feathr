// LINT_SUPPRESS: namespace.three.parts

namespace com.linkedin.feathr.compute
/**
 * Sliding window aggregation produces feature data by aggregating a collection of data within a given time interval into an aggregate value. It ensures point-in-time correctness, when joining with label data, feathr looks back the configurable time window from each entry's timestamp and compute the aggregagate value.
 */
record SlidingWindowFeature {

  /**
   * The target column to perform aggregation against.
   */
  targetColumn: union[
    //A Spark SQL expression. It can be a simple field reference, or a complex Spark SQL statement.
    SqlExpression
  ]

  /**
   * Represents supported types of aggregation.
   */
  aggregationType: enum AggregationType {
    /** Sum. */
    SUM
    /** Count. */
    COUNT
    /** Max. */
    MAX
    /** Min. */
    MIN
    /** Average. */
    AVG
    /** Pooling is a sample-based discretization process. The objective is to down-sample an input representation and reduce its dimensionality. Max pooling is done by applying a max filter to (usually) non-overlapping subregions of the initial representation. */
    MAX_POOLING
    /** Pooling is a sample-based discretization process. The objective is to down-sample an input representation and reduce its dimensionality. Min pooling is done by applying a min filter to (usually) non-overlapping subregions of the initial representation. */
    MIN_POOLING
    /** Pooling is a sample-based discretization process. The objective is to down-sample an input representation and reduce its dimensionality. Average pooling is done by applying a average filter to (usually) non-overlapping subregions of the initial representation. */
    AVG_POOLING
    /** Latest */
    LATEST
  }

  /**
   * Represents the time window to look back from label data's timestamp.
   */
  window: Window

  /**
   * Represents lateral view statements to be applied before the aggregation. Refer to LateralView for more details.
   */
  lateralViews: array[LateralView] = []

  /**
   * Represents the filter statement before the aggregation.
   */
  filter: optional union[
    //A Spark SQL expression, for example, "channel = 'RECRUITER_SEARCH' AND event = 'SKIP'".
    SqlExpression
  ]

  /**
   * Represents the target to be grouped by before aggregation. If groupBy is not set, the aggregation will be performed over the entire dataset.
   */
  groupBy: optional union[
    //A Spark SQL expression, it can be a simple field reference, or a complex Spark SQL statement.
    SqlExpression
  ]

  /**
   * Represents the max number of groups (with aggregation results) to return.
   */
  limit: optional int
}

