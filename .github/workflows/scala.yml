name: Feathr Scala Tests And Azure E2E Integration

on:
  push:
    branches: [main]
    paths-ignore:
      - 'docs/**'
      - '**/README.md'
  pull_request:
    branches: [main]
    paths-ignore:
      - 'docs/**'
      - '**/README.md'

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
      - name: Set up JDK 8
        uses: actions/setup-java@v2
        with:
          java-version: "8"
          distribution: "temurin"
      - name: Run tests
        run: sbt clean && sbt test
      - name: Build JAR
        run: sbt assembly
      - name: Set up Python 3.8
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install flake8 pytest
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Lint with flake8
        run: |
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      - name: Install Feathr Package
        run: |
          python -m pip install -e ./feathr_project/
      - name: Set env variable for databricks run
        run: |
          echo "SPARK_CONFIG__DATABRICKS__FEATHR_RUNTIME_LOCATION=$(readlink -f ./target/scala-2.12/feathr-assembly-0.1.0.jar)" >> $GITHUB_ENV
          echo "SPARK_CONFIG__AZURE_SYNAPSE__FEATHR_RUNTIME_LOCATION=$(readlink -f ./target/scala-2.12/feathr-assembly-0.1.0.jar)" >> $GITHUB_ENV
      - name: Run Feathr with Databricks
        env:
          PROJECT_CONFIG__PROJECT_NAME: "feathr_github_ci_databricks"
          SPARK_CONFIG__SPARK_CLUSTER: databricks
          SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL: "https://adb-2474129336842816.16.azuredatabricks.net/"
          DATABRICKS_WORKSPACE_TOKEN_VALUE: ${{secrets.DATABRICKS_WORKSPACE_TOKEN_VALUE}}
          SPARK_CONFIG__DATABRICKS__CONFIG_TEMPLATE: '{"run_name":"","new_cluster":{"spark_version":"9.1.x-scala2.12","node_type_id":"Standard_D3_v2","num_workers":2,"spark_conf":{}},"libraries":[{"jar":""}],"spark_jar_task":{"main_class_name":"","parameters":[""]}}'
          SPARK_CONFIG__DATABRICKS__WORK_DIR: "dbfs:/feathr_ci_project"
          REDIS_PASSWORD: ${{secrets.REDIS_PASSWORD}}
          AZURE_CLIENT_ID: ${{secrets.AZURE_CLIENT_ID}}
          AZURE_TENANT_ID: ${{secrets.AZURE_TENANT_ID}}
          AZURE_CLIENT_SECRET: ${{secrets.AZURE_CLIENT_SECRET}}
          S3_ACCESS_KEY: ${{secrets.S3_ACCESS_KEY}}
          S3_SECRET_KEY: ${{secrets.S3_SECRET_KEY}}
          ADLS_ACCOUNT: ${{secrets.ADLS_ACCOUNT}}
          ADLS_KEY: ${{secrets.ADLS_KEY}}
          BLOB_ACCOUNT: ${{secrets.BLOB_ACCOUNT}}
          BLOB_KEY: ${{secrets.BLOB_KEY}}

        run: |
          # run only test with databricks 
          pytest  -k 'databricks'
      - name: Run Feathr with Azure Synapse
        env:
          PROJECT_CONFIG__PROJECT_NAME: "feathr_github_ci_azure_synapse"
          SPARK_CONFIG__SPARK_CLUSTER: azure_synapse
          REDIS_PASSWORD: ${{secrets.REDIS_PASSWORD}}
          AZURE_CLIENT_ID: ${{secrets.AZURE_CLIENT_ID}}
          AZURE_TENANT_ID: ${{secrets.AZURE_TENANT_ID}}
          AZURE_CLIENT_SECRET: ${{secrets.AZURE_CLIENT_SECRET}}
          S3_ACCESS_KEY: ${{secrets.S3_ACCESS_KEY}}
          S3_SECRET_KEY: ${{secrets.S3_SECRET_KEY}}
          ADLS_ACCOUNT: ${{secrets.ADLS_ACCOUNT}}
          ADLS_KEY: ${{secrets.ADLS_KEY}}
          BLOB_ACCOUNT: ${{secrets.BLOB_ACCOUNT}}
          BLOB_KEY: ${{secrets.BLOB_KEY}}

        run: |
          # skip databricks related test as we just ran the test; also seperate databricks and synapse test to make sure there's no write conflict
          pytest  -k 'not databricks'
