name: Feathr Scala Tests And Azure E2E Integration

on:
  push:
    branches: [main]
    paths-ignore:
      - 'docs/**'
      - '**/README.md'
  pull_request:
    branches: [main]
    paths-ignore:
      - 'docs/**'
      - '**/README.md'

jobs:

  sbt_test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up JDK 8
        uses: actions/setup-java@v2
        with:
          java-version: "8"
          distribution: "temurin"
      - name: Run tests
        run: sbt clean && sbt test
  python_lint:
    runs-on: ubuntu-latest
    steps:
      - name: Set up Python 3.8
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install flake8
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Lint with flake8
        run: |
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      
  databricks_test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up JDK 8
        uses: actions/setup-java@v2
        with:
          java-version: "8"
          distribution: "temurin"
      - name: Build JAR
        run: sbt assembly
      - name: Set up Python 3.8
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install Feathr Package
        run: |
          python -m pip install --upgrade pip
          python -m pip install pytest pytest-xdist
          python -m pip install -e ./feathr_project/
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Set env variable
        run: |
          # overwrite corresponding environment variables to utilize feathr to upload the files
          echo "SPARK_CONFIG__DATABRICKS__FEATHR_RUNTIME_LOCATION=$(readlink -f ./target/scala-2.12/feathr-assembly-0.1.0.jar)" >> $GITHUB_ENV
          # use hour, minute, seconds for seperate folders so that different CI pipeline won't share the same runtime
      - name: Run Feathr with Databricks
        env:
          PROJECT_CONFIG__PROJECT_NAME: "feathr_github_ci_databricks"
          SPARK_CONFIG__SPARK_CLUSTER: databricks
          SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL: "https://adb-2474129336842816.16.azuredatabricks.net/"
          DATABRICKS_WORKSPACE_TOKEN_VALUE: ${{secrets.DATABRICKS_WORKSPACE_TOKEN_VALUE}}
          SPARK_CONFIG__DATABRICKS__CONFIG_TEMPLATE: '{"run_name":"","new_cluster":{"spark_version":"9.1.x-scala2.12","node_type_id":"Standard_D3_v2","num_workers":2,"spark_conf":{}},"libraries":[{"jar":""}],"spark_jar_task":{"main_class_name":"","parameters":[""]}}'
          REDIS_PASSWORD: ${{secrets.REDIS_PASSWORD}}
          AZURE_CLIENT_ID: ${{secrets.AZURE_CLIENT_ID}}
          AZURE_TENANT_ID: ${{secrets.AZURE_TENANT_ID}}
          AZURE_CLIENT_SECRET: ${{secrets.AZURE_CLIENT_SECRET}}
          S3_ACCESS_KEY: ${{secrets.S3_ACCESS_KEY}}
          S3_SECRET_KEY: ${{secrets.S3_SECRET_KEY}}
          ADLS_ACCOUNT: ${{secrets.ADLS_ACCOUNT}}
          ADLS_KEY: ${{secrets.ADLS_KEY}}
          BLOB_ACCOUNT: ${{secrets.BLOB_ACCOUNT}}
          BLOB_KEY: ${{secrets.BLOB_KEY}}
          JDBC_SF_PASSWORD: ${{secrets.JDBC_SF_PASSWORD}}

        run: |
          # run only test with databricks. run in 4 parallel jobs
          pytest 

  azure_synapse_test:
      # might be a bit duplication to setup both the azure_synapse test and databricks test, but for now we will keep those to accelerate the test speed
      runs-on: ubuntu-latest
      steps:
      - uses: actions/checkout@v2
      - name: Set up JDK 8
        uses: actions/setup-java@v2
        with:
          java-version: "8"
          distribution: "temurin"
      - name: Build JAR
        run: sbt assembly
      - name: Set up Python 3.8
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Azure Blob Storage Upload (Overwrite)
        uses: fixpoint/azblob-upload-artifact@v4
        run: |
          echo "CI_SPARK_RUNTIME_FOLDER=feathr_jar_github_action_$(date +"_%H_%M_%S")" >> $GITHUB_ENV
        with:
          connection-string: ${{secrets.SPARK_JAR_BLOB_CONNECTION_STRING}}
          name: "${{CI_SPARK_RUNTIME_FOLDER}}"
          path: "target/scala-2.12/feathr-assembly-0.1.0.jar"
          container: ${{secrets.SPARK_JAR_BLOB_CONTAINER}}
          cleanup: "true"
      - name: Install Feathr Package
        run: |
          python -m pip install --upgrade pip
          python -m pip install pytest pytest-xdist
          python -m pip install -e ./feathr_project/
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Set env variable
        run: |
          # overwrite corresponding environment variables to utilize feathr to upload the files 
          # echo "SPARK_CONFIG__AZURE_SYNAPSE__FEATHR_RUNTIME_LOCATION=$(readlink -f ./target/scala-2.12/feathr-assembly-0.1.0.jar)" >> $GITHUB_ENV
          # use hour, minute, seconds for seperate folders so that different CI pipeline won't share the same runtime
          echo "PROJECT_CONFIG__PROJECT_NAME=feathr_github_ci_project$(date +"_%H_%M_%S")" >> $GITHUB_ENV 
      - name: Run Feathr with Azure Synapse
        env:
          PROJECT_CONFIG__PROJECT_NAME: "feathr_github_ci_synapse"
          SPARK_CONFIG__SPARK_CLUSTER: azure_synapse
          REDIS_PASSWORD: ${{secrets.REDIS_PASSWORD}}
          AZURE_CLIENT_ID: ${{secrets.AZURE_CLIENT_ID}}
          AZURE_TENANT_ID: ${{secrets.AZURE_TENANT_ID}}
          AZURE_CLIENT_SECRET: ${{secrets.AZURE_CLIENT_SECRET}}
          S3_ACCESS_KEY: ${{secrets.S3_ACCESS_KEY}}
          S3_SECRET_KEY: ${{secrets.S3_SECRET_KEY}}
          ADLS_ACCOUNT: ${{secrets.ADLS_ACCOUNT}}
          ADLS_KEY: ${{secrets.ADLS_KEY}}
          BLOB_ACCOUNT: ${{secrets.BLOB_ACCOUNT}}
          BLOB_KEY: ${{secrets.BLOB_KEY}}
          JDBC_TABLE: ${{secrets.JDBC_TABLE}}
          JDBC_USER: ${{secrets.JDBC_USER}}
          JDBC_PASSWORD: ${{secrets.JDBC_PASSWORD}}
          JDBC_DRIVER: ${{secrets.JDBC_DRIVER}}
          JDBC_SF_PASSWORD: ${{secrets.JDBC_SF_PASSWORD}
          SPARK_CONFIG__AZURE_SYNAPSE__FEATHR_RUNTIME_LOCATION: "abfss://${{secrets.SPARK_JAR_BLOB_CONTAINER}}@feathrazuretest3storage.dfs.core.windows.net/${{CI_SPARK_RUNTIME_FOLDER}}/feathr-assembly-0.1.0.jar"
        run: |
          # skip databricks related test as we just ran the test; also seperate databricks and synapse test to make sure there's no write conflict
          # run in 4 parallel jobs to make the time shorter
          pytest 