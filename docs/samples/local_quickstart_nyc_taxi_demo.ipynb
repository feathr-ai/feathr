{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feathr Local Spark Quickstart - NYC Taxi Demo\n",
    "This notebook demonstrates how to use Feathr Local to train a model on a local Spark cluster. We will use the NYC Taxi dataset to predict the tip amount for a taxi ride. The dataset is available on [Kaggle](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Feathr and Necessary Dependancies\n",
    "\n",
    "Install feathr and necessary packages by running `pip install feathr[notebook]` if you haven't installed them already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feathr in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: pandavro in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: graphlib-backport<=1.0.3 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (1.0.3)\n",
      "Requirement already satisfied: azure-core<=1.22.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (1.22.1)\n",
      "Requirement already satisfied: avro<=1.11.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (1.11.1)\n",
      "Requirement already satisfied: azure-keyvault-secrets<=4.6.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (4.5.1)\n",
      "Requirement already satisfied: pandas<=1.5.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (1.4.4)\n",
      "Requirement already satisfied: requests<=2.28.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (2.28.1)\n",
      "Requirement already satisfied: databricks-cli<=0.17.3 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.17.3)\n",
      "Requirement already satisfied: pyhocon<=0.3.59 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.3.59)\n",
      "Requirement already satisfied: azure-identity>=1.8.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (4.3.0)\n",
      "Requirement already satisfied: py4j<=0.10.9.7 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.10.9.5)\n",
      "Requirement already satisfied: tqdm<=4.64.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (4.64.1)\n",
      "Requirement already satisfied: azure-synapse-spark<=0.7.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.2.0)\n",
      "Requirement already satisfied: loguru<=0.6.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.6.0)\n",
      "Requirement already satisfied: Jinja2<=3.1.2 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (3.1.2)\n",
      "Requirement already satisfied: pyapacheatlas<=0.14.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.14.0)\n",
      "Requirement already satisfied: protobuf==3.* in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (3.20.2)\n",
      "Requirement already satisfied: python-snappy<=0.6.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.6.1)\n",
      "Requirement already satisfied: pyarrow<=9.0.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (9.0.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake<=12.5.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (12.5.0)\n",
      "Requirement already satisfied: redis<=4.4.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (4.3.4)\n",
      "Requirement already satisfied: pyspark>=3.1.2 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (3.3.0)\n",
      "Requirement already satisfied: click<=8.1.3 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (8.1.3)\n",
      "Requirement already satisfied: deltalake<=0.5.8 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (0.5.8)\n",
      "Requirement already satisfied: confluent-kafka<=1.9.2 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (1.9.2)\n",
      "Requirement already satisfied: pyyaml<=6.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from feathr) (6.0)\n",
      "Requirement already satisfied: fastavro==1.5.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pandavro) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pandavro) (1.23.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-core<=1.22.1->feathr) (1.16.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-identity>=1.8.0->feathr) (1.18.0b1)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-identity>=1.8.0->feathr) (38.0.1)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-identity>=1.8.0->feathr) (1.0.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-keyvault-secrets<=4.6.0->feathr) (1.1.28)\n",
      "Requirement already satisfied: msrest>=0.6.21 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-keyvault-secrets<=4.6.0->feathr) (0.6.21)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.9.0b1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from azure-storage-file-datalake<=12.5.0->feathr) (12.11.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from databricks-cli<=0.17.3->feathr) (2.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from databricks-cli<=0.17.3->feathr) (3.2.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from databricks-cli<=0.17.3->feathr) (0.8.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from Jinja2<=3.1.2->feathr) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pandas<=1.5.0->feathr) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pandas<=1.5.0->feathr) (2022.2.1)\n",
      "Requirement already satisfied: openpyxl>=3.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pyapacheatlas<=0.14.0->feathr) (3.0.10)\n",
      "Requirement already satisfied: pyparsing~=2.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from pyhocon<=0.3.59->feathr) (2.4.7)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from redis<=4.4.0->feathr) (1.2.13)\n",
      "Requirement already satisfied: packaging>=20.4 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from redis<=4.4.0->feathr) (21.3)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from redis<=4.4.0->feathr) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<=2.28.1->feathr) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<=2.28.1->feathr) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<=2.28.1->feathr) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from requests<=2.28.1->feathr) (1.26.12)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from cryptography>=2.5->azure-identity>=1.8.0->feathr) (1.15.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from deprecated>=1.2.3->redis<=4.4.0->feathr) (1.14.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity>=1.8.0->feathr) (2.5.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from msrest>=0.6.21->azure-keyvault-secrets<=4.6.0->feathr) (1.3.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from msrest>=0.6.21->azure-keyvault-secrets<=4.6.0->feathr) (0.6.1)\n",
      "Requirement already satisfied: et-xmlfile in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from openpyxl>=3.0->pyapacheatlas<=0.14.0->feathr) (1.1.0)\n",
      "Requirement already satisfied: pycparser in /Users/yuqing/.pyenv/versions/3.9.13/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.8.0->feathr) (2.21)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Users/yuqing/.pyenv/versions/3.9.13/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U feathr pandavro scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is optional and only required if you want to use the feathr registry\n",
    "#! az login --use-device-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Shareable Features with Feathr Feature Definition Configs\n",
    "\n",
    "In this notebook, we define all the necessary resource key values for authentication. We use the values passed by the databricks widgets at the top of this notebook. Instead of manually entering the values to the widgets, we can also use [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/) to retrieve them.\n",
    "Please refer to [how-to guide documents for granting key-vault access](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html#3-grant-key-vault-and-synapse-access-to-selected-users-optional) and [Databricks' Azure Key Vault-backed scopes](https://learn.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import pandavro as pdx\n",
    "from feathr import FeathrClient\n",
    "from feathr import BOOLEAN, FLOAT, INT32, ValueType\n",
    "from feathr import Feature, DerivedFeature, FeatureAnchor\n",
    "from feathr import BackfillTime, MaterializationSettings\n",
    "from feathr import FeatureQuery, ObservationSettings\n",
    "from feathr import RedisSink\n",
    "from feathr import INPUT_CONTEXT, HdfsSource\n",
    "from feathr import WindowAggTransformation\n",
    "from feathr import TypedKey\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.0\n"
     ]
    }
   ],
   "source": [
    "import feathr\n",
    "print(feathr.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Feathr Client \n",
    "We will use the Feathr Local client to train our model. The client will be initialized with the default configuration. The default configuration can be overwritten by environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "\n",
    "import tempfile\n",
    "yaml_config = f\"\"\"\n",
    "api_version: 1\n",
    "project_config:\n",
    "  project_name: 'local_spark'\n",
    "  required_environment_variables:\n",
    "  optional_environment_variables:\n",
    "    # the environemnt variables are optional, however you will need them if you want to use some of the services:\n",
    "    - ADLS_ACCOUNT\n",
    "    - ADLS_KEY\n",
    "    - WASB_ACCOUNT\n",
    "    - WASB_KEY\n",
    "    - S3_ACCESS_KEY\n",
    "    - S3_SECRET_KEY\n",
    "    - JDBC_TABLE\n",
    "    - JDBC_USER\n",
    "    - JDBC_PASSWORD\n",
    "    - KAFKA_SASL_JAAS_CONFIG\n",
    "\n",
    "\n",
    "spark_config:\n",
    "  # choice for spark runtime. Currently support: azure_synapse, databricks, local\n",
    "  spark_cluster: 'local'\n",
    "  spark_result_output_parts: '1'\n",
    "  local:\n",
    "    master: 'local[*]'\n",
    "    feathr_runtime_location:\n",
    "\n",
    "online_store:\n",
    "  redis:\n",
    "    # Redis configs to access Redis cluster\n",
    "    host: '<redis_host_name>'\n",
    "    port: 6380\n",
    "    ssl_enabled: True\n",
    "\n",
    "feature_registry:\n",
    "  # The API endpoint of the registry service\n",
    "  api_endpoint: \"https://localhost/api/v1\"\n",
    "\"\"\"\n",
    "\n",
    "tmp = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "with open(tmp.name, \"w\") as text_file:\n",
    "    text_file.write(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Feathr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 13:20:52.130 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n",
      "2022-10-31 13:20:52.138 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__s3__s3_enabled not found in the config file.\n",
      "2022-10-31 13:20:52.141 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__adls__adls_enabled not found in the config file.\n",
      "2022-10-31 13:20:52.143 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__wasb__wasb_enabled not found in the config file.\n",
      "2022-10-31 13:20:52.145 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__jdbc__jdbc_enabled not found in the config file.\n",
      "2022-10-31 13:20:52.147 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__snowflake__snowflake_enabled not found in the config file.\n",
      "No offline storage enabled.\n",
      "2022-10-31 13:20:52.154 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__workspace not found in the config file.\n",
      "2022-10-31 13:20:52.156 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - REDIS_PASSWORD is not set in the environment variables.\n",
      "2022-10-31 13:20:52.158 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n"
     ]
    }
   ],
   "source": [
    "client = FeathrClient(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the sample data\n",
    "The sample data will be downloaded to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/3jr6fmn92v1dwn7y6mmfyvl00000gn/T/ipykernel_2678/858538005.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_raw = pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/green_tripdata_2020-04_with_index.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_raw = pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/green_tripdata_2020-04_with_index.csv\")\n",
    "df_raw.to_csv(\"green_tripdata_2020-04_with_index.csv\", index=False)\n",
    "DATA_FILE_PATH = \"./green_tripdata_2020-04_with_index.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining features with Feathr\n",
    "\n",
    "In Feathr, a feature is viewed as a function, mapping a key and timestamp to a feature value. For more details, please see [Feathr Feature Definition Guide](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/feature-definition.md).\n",
    "\n",
    "* The feature key (a.k.a. entity id) identifies the subject of feature, e.g. a user_id or location_id.\n",
    "* The feature name is the aspect of the entity that the feature is indicating, e.g. the age of the user.\n",
    "* The feature value is the actual value of that aspect at a particular time, e.g. the value is 30 at year 2022.\n",
    "\n",
    "Note that, in some cases, a feature could be just a transformation function that has no entity key or timestamp involved, e.g. *the day of week of the request timestamp*.\n",
    "\n",
    "There are two types of features -- anchored features and derivated features:\n",
    "\n",
    "* **Anchored features**: Features that are directly extracted from sources. Could be with or without aggregation. \n",
    "* **Derived features**: Features that are computed on top of other features.\n",
    "\n",
    "#### Define anchored features\n",
    "\n",
    "A feature source is needed for anchored features that describes the raw data in which the feature values are computed from. A source value should be either `INPUT_CONTEXT` (the features that will be extracted from the observation data directly) or `feathr.source.Source` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_COL = \"lpep_dropoff_datetime\"\n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd HH:mm:ss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import pyspark.sql.functions as F\n",
    "    df = df.withColumn(\"fare_amount_cents\", (F.col(\"fare_amount\") * 100.0).cast(\"float\"))\n",
    "    return df\n",
    "\n",
    "batch_source = HdfsSource(\n",
    "    name=\"nycTaxiBatchSource\",\n",
    "    path=DATA_FILE_PATH,\n",
    "    event_timestamp_column=TIMESTAMP_COL,\n",
    "    preprocessing=preprocessing,\n",
    "    timestamp_format=TIMESTAMP_FORMAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define f_trip_distance and f_trip_time_duration features separately\n",
    "# so that we can reuse them later for the derived features.\n",
    "f_trip_distance = Feature(\n",
    "    name=\"f_trip_distance\",\n",
    "    feature_type=FLOAT,\n",
    "    transform=\"trip_distance\",\n",
    ")\n",
    "f_trip_time_duration = Feature(\n",
    "    name=\"f_trip_time_duration\",\n",
    "    feature_type=FLOAT,\n",
    "    transform=\"cast_float((to_unix_timestamp(lpep_dropoff_datetime) - to_unix_timestamp(lpep_pickup_datetime)) / 60)\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "features = [\n",
    "    f_trip_distance,\n",
    "    f_trip_time_duration,\n",
    "    Feature(\n",
    "        name=\"f_is_long_trip_distance\",\n",
    "        feature_type=BOOLEAN,\n",
    "        transform=\"trip_distance > 30.0\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_day_of_week\",\n",
    "        feature_type=INT32,\n",
    "        transform=\"dayofweek(lpep_dropoff_datetime)\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_day_of_month\",\n",
    "        feature_type=INT32,\n",
    "        transform=\"dayofmonth(lpep_dropoff_datetime)\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_hour_of_day\",\n",
    "        feature_type=INT32,\n",
    "        transform=\"hour(lpep_dropoff_datetime)\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# After you have defined features, bring them together to build the anchor to the source.\n",
    "feature_anchor = FeatureAnchor(\n",
    "    name=\"feature_anchor\",\n",
    "    source=INPUT_CONTEXT,  # Pass through source, i.e. observation data.\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the features with aggregation, the supported functions are as follows:\n",
    "\n",
    "| Aggregation Function | Input Type | Description |\n",
    "| --- | --- | --- |\n",
    "|SUM, COUNT, MAX, MIN, AVG\t|Numeric|Applies the the numerical operation on the numeric inputs. |\n",
    "|MAX_POOLING, MIN_POOLING, AVG_POOLING\t| Numeric Vector | Applies the max/min/avg operation on a per entry bassis for a given a collection of numbers.|\n",
    "|LATEST| Any |Returns the latest not-null values from within the defined time window |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_key = TypedKey(\n",
    "    key_column=\"DOLocationID\",\n",
    "    key_column_type=ValueType.INT32,\n",
    "    description=\"location id in NYC\",\n",
    "    full_name=\"nyc_taxi.location_id\",\n",
    ")\n",
    "\n",
    "agg_window = \"90d\"\n",
    "\n",
    "# Anchored features with aggregations\n",
    "agg_features = [\n",
    "    Feature(\n",
    "        name=\"f_location_avg_fare\",\n",
    "        key=agg_key,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"fare_amount_cents\",\n",
    "            agg_func=\"AVG\",\n",
    "            window=agg_window,\n",
    "        ),\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"f_location_max_fare\",\n",
    "        key=agg_key,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"fare_amount_cents\",\n",
    "            agg_func=\"MAX\",\n",
    "            window=agg_window,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "agg_feature_anchor = FeatureAnchor(\n",
    "    name=\"agg_feature_anchor\",\n",
    "    source=batch_source,  # External data source for feature. Typically a data table.\n",
    "    features=agg_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define derived features\n",
    "\n",
    "We also define a derived feature, `f_trip_distance_rounded`, from the anchored features `f_trip_distance` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_trip_time_distance = DerivedFeature(name=\"f_trip_time_distance\",\n",
    "                                          feature_type=FLOAT,\n",
    "                                          input_features=[\n",
    "                                              f_trip_distance, f_trip_time_duration],\n",
    "                                          transform=\"f_trip_distance * f_trip_time_duration\")\n",
    "\n",
    "f_trip_time_rounded = DerivedFeature(name=\"f_trip_time_rounded\",\n",
    "                                         feature_type=INT32,\n",
    "                                         input_features=[f_trip_time_duration],\n",
    "                                         transform=\"f_trip_time_duration % 10\")\n",
    "\n",
    "derived_feature = [f_trip_time_distance, f_trip_time_rounded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build features\n",
    "\n",
    "Finally, we build the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[feature_anchor, agg_feature_anchor],\n",
    "    derived_feature_list=derived_feature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional if you want to use the feathr registry.\n",
    "#client.register_features()\n",
    "#client.list_registered_features(client.project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Data Using Point-in-Time Correct Feature Join\n",
    "\n",
    "After the feature producers have defined the features (as described in the Feature Definition part), the feature consumers may want to consume those features. Feature consumers will use observation data to query from different feature tables using Feature Query.\n",
    "\n",
    "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
    "what features and how these features should be joined to the observation data. \n",
    "\n",
    "To learn more on this topic, please refer to [Point-in-time Correctness](https://github.com/linkedin/feathr/blob/main/docs/concepts/point-in-time-join.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f_trip_distance',\n",
       " 'f_trip_time_duration',\n",
       " 'f_is_long_trip_distance',\n",
       " 'f_day_of_week',\n",
       " 'f_day_of_month',\n",
       " 'f_hour_of_day',\n",
       " 'f_location_avg_fare',\n",
       " 'f_location_max_fare']"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [feature.name for feature in features + agg_features]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "output_path = os.path.join(\"debug\", f\"test_output_{now}\")\n",
    "\n",
    "offline_features_path = output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 13:20:57.909 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:73 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-10-31 13:20:57.909 | INFO     | feathr.spark_provider._localspark_submission:_init_args:202 - Spark job: local_spark_feathr_feature_join_job is running on local spark with master: local[*].\n",
      "2022-10-31 13:20:57.917 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:116 - Detail job stdout and stderr are in debug/local_spark_feathr_feature_join_job20221031132057/log.\n",
      "2022-10-31 13:20:57.919 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:126 - Local Spark job submit with pid: 13378.\n",
      "2022-10-31 13:20:57.920 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:134 - 1 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-10-31 13:20:57.920 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:135 - Please check auto generated spark command in debug/local_spark_feathr_feature_join_job20221031132057/command.sh and detail logs in debug/local_spark_feathr_feature_join_job20221031132057/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug/local_spark_feathr_feature_join_job20221031132057\n",
      "['/var/folders/93/3jr6fmn92v1dwn7y6mmfyvl00000gn/T/tmpr4astj7p/feathr_pyspark_driver.py']\n",
      "x"
     ]
    }
   ],
   "source": [
    "# Features that we want to request. Can use a subset of features\n",
    "query = FeatureQuery(\n",
    "    feature_list=feature_names,\n",
    "    key=agg_key,\n",
    ")\n",
    "settings = ObservationSettings(\n",
    "    observation_path=DATA_FILE_PATH,\n",
    "    event_timestamp_column=TIMESTAMP_COL,\n",
    "    timestamp_format=TIMESTAMP_FORMAT,\n",
    ")\n",
    "client.get_offline_features(\n",
    "    observation_settings=settings,\n",
    "    feature_query=query,\n",
    "    output_path=offline_features_path,\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>f_is_long_trip_distance</th>\n",
       "      <th>f_day_of_month</th>\n",
       "      <th>f_trip_time_duration</th>\n",
       "      <th>f_day_of_week</th>\n",
       "      <th>f_hour_of_day</th>\n",
       "      <th>f_trip_distance</th>\n",
       "      <th>f_location_max_fare</th>\n",
       "      <th>f_location_avg_fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-04-05 07:17:58</td>\n",
       "      <td>2020-04-05 08:14:41</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>242</td>\n",
       "      <td>204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>56.716667</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>6620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1591</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-04-02 12:03:38</td>\n",
       "      <td>2020-04-02 12:26:13</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>22.583334</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>3220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9532</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-11 18:01:19</td>\n",
       "      <td>2020-04-11 18:18:25</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "      <td>175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.18</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>2510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28128</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-04-12 18:49:00</td>\n",
       "      <td>2020-04-12 19:13:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>76</td>\n",
       "      <td>175</td>\n",
       "      <td>None</td>\n",
       "      <td>12.04</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12.04</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>2570.666748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16763</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-21 12:14:54</td>\n",
       "      <td>2020-04-21 12:41:34</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74</td>\n",
       "      <td>175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.82</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>26.666666</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14.82</td>\n",
       "      <td>4150.0</td>\n",
       "      <td>2965.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>23381</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-29 16:44:36</td>\n",
       "      <td>2020-04-29 16:51:39</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>7.050000</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1.05</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1232.594971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>23413</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-29 17:45:24</td>\n",
       "      <td>2020-04-29 18:07:24</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.23</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>3.23</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1234.905640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>24087</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-30 14:54:32</td>\n",
       "      <td>2020-04-30 15:07:12</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75</td>\n",
       "      <td>179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>30</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>4.31</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1236.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>35229</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-04-30 15:56:00</td>\n",
       "      <td>2020-04-30 16:31:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>188</td>\n",
       "      <td>179</td>\n",
       "      <td>None</td>\n",
       "      <td>11.97</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>30</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>11.97</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1250.236084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>24374</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-04-30 18:33:21</td>\n",
       "      <td>2020-04-30 18:39:10</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>30</td>\n",
       "      <td>5.816667</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1.19</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1246.222168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35612 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    trip_id VendorID lpep_pickup_datetime lpep_dropoff_datetime  \\\n",
       "0      4254      1.0  2020-04-05 07:17:58   2020-04-05 08:14:41   \n",
       "0      1591      1.0  2020-04-02 12:03:38   2020-04-02 12:26:13   \n",
       "1      9532      2.0  2020-04-11 18:01:19   2020-04-11 18:18:25   \n",
       "2     28128     None  2020-04-12 18:49:00   2020-04-12 19:13:00   \n",
       "3     16763      2.0  2020-04-21 12:14:54   2020-04-21 12:41:34   \n",
       "..      ...      ...                  ...                   ...   \n",
       "157   23381      2.0  2020-04-29 16:44:36   2020-04-29 16:51:39   \n",
       "158   23413      2.0  2020-04-29 17:45:24   2020-04-29 18:07:24   \n",
       "159   24087      2.0  2020-04-30 14:54:32   2020-04-30 15:07:12   \n",
       "160   35229     None  2020-04-30 15:56:00   2020-04-30 16:31:00   \n",
       "161   24374      2.0  2020-04-30 18:33:21   2020-04-30 18:39:10   \n",
       "\n",
       "    store_and_fwd_flag RatecodeID PULocationID DOLocationID passenger_count  \\\n",
       "0                    N        1.0          242          204             1.0   \n",
       "0                    N        1.0            3          175             1.0   \n",
       "1                    N        1.0           92          175             1.0   \n",
       "2                 None       None           76          175            None   \n",
       "3                    N        1.0           74          175             1.0   \n",
       "..                 ...        ...          ...          ...             ...   \n",
       "157                  N        1.0            7          179             1.0   \n",
       "158                  N        1.0            7          179             1.0   \n",
       "159                  N        1.0           75          179             1.0   \n",
       "160               None       None          188          179            None   \n",
       "161                  N        1.0            7          179             1.0   \n",
       "\n",
       "    trip_distance  ... trip_type congestion_surcharge f_is_long_trip_distance  \\\n",
       "0             0.0  ...       1.0                  0.0                   False   \n",
       "0             0.0  ...       1.0                  0.0                   False   \n",
       "1            5.18  ...       1.0                  0.0                   False   \n",
       "2           12.04  ...      None                 None                   False   \n",
       "3           14.82  ...       1.0                  0.0                   False   \n",
       "..            ...  ...       ...                  ...                     ...   \n",
       "157          1.05  ...       1.0                  0.0                   False   \n",
       "158          3.23  ...       1.0                  0.0                   False   \n",
       "159          4.31  ...       1.0                  0.0                   False   \n",
       "160         11.97  ...      None                 None                   False   \n",
       "161          1.19  ...       1.0                  0.0                   False   \n",
       "\n",
       "    f_day_of_month f_trip_time_duration f_day_of_week f_hour_of_day  \\\n",
       "0                5            56.716667             1             8   \n",
       "0                2            22.583334             5            12   \n",
       "1               11            17.100000             7            18   \n",
       "2               12            24.000000             1            19   \n",
       "3               21            26.666666             3            12   \n",
       "..             ...                  ...           ...           ...   \n",
       "157             29             7.050000             4            16   \n",
       "158             29            22.000000             4            18   \n",
       "159             30            12.666667             5            15   \n",
       "160             30            35.000000             5            16   \n",
       "161             30             5.816667             5            18   \n",
       "\n",
       "    f_trip_distance f_location_max_fare f_location_avg_fare  \n",
       "0              0.00              6620.0         6620.000000  \n",
       "0              0.00              3220.0         3220.000000  \n",
       "1              5.18              3220.0         2510.000000  \n",
       "2             12.04              3220.0         2570.666748  \n",
       "3             14.82              4150.0         2965.500000  \n",
       "..              ...                 ...                 ...  \n",
       "157            1.05              6000.0         1232.594971  \n",
       "158            3.23              6000.0         1234.905640  \n",
       "159            4.31              6000.0         1236.250000  \n",
       "160           11.97              6000.0         1250.236084  \n",
       "161            1.19              6000.0         1246.222168  \n",
       "\n",
       "[35612 rows x 29 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_list = []\n",
    "vertical_concat_df = None\n",
    "for file in glob.glob(os.path.join(output_path, '*.avro')):\n",
    "    dataframe_list.append(pdx.read_avro(file))\n",
    "    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
    "\n",
    "vertical_concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate a Prediction Model\n",
    "\n",
    "After generating all the features, we train and evaluate a machine learning model to predict the NYC taxi fare prediction. In this example, we use Spark MLlib's [GBTRegressor](https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-regression).\n",
    "\n",
    "Note that designing features, training prediction models and evaluating them are an iterative process where the models' performance maybe used to modify the features as a part of the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "final_df = vertical_concat_df\n",
    "final_df.drop([\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\",\n",
    "              \"store_and_fwd_flag\"], axis=1, inplace=True, errors='ignore')\n",
    "final_df.fillna(0, inplace=True)\n",
    "final_df['fare_amount'] = final_df['fare_amount'].astype(\"float64\")\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(final_df.drop([\"fare_amount\"], axis=1),\n",
    "                                                    final_df[\"fare_amount\"],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "y_predict = model.predict(test_x)\n",
    "\n",
    "y_actual = test_y.values.flatten().tolist()\n",
    "rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
    "\n",
    "sum_actuals = sum_errors = 0\n",
    "\n",
    "for actual_val, predict_val in zip(y_actual, y_predict):\n",
    "    abs_error = actual_val - predict_val\n",
    "    if abs_error < 0:\n",
    "        abs_error = abs_error * -1\n",
    "\n",
    "    sum_errors = sum_errors + abs_error\n",
    "    sum_actuals = sum_actuals + actual_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MAPE:\n",
      "0.028841802082324054\n",
      "\n",
      "Model Accuracy:\n",
      "0.971158197917676\n"
     ]
    }
   ],
   "source": [
    "mean_abs_percent_error = sum_errors / sum_actuals\n",
    "print(\"Model MAPE:\")\n",
    "print(mean_abs_percent_error)\n",
    "print()\n",
    "print(\"Model Accuracy:\")\n",
    "print(1 - mean_abs_percent_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Materialize Feature Values for Online Scoring\n",
    "\n",
    "While we computed feature values on-the-fly at request time via Feathr, we can pre-compute the feature values and materialize them to offline or online storages such as Redis.\n",
    "\n",
    "Note, only the features anchored to offline data source can be materialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f_location_avg_fare', 'f_location_max_fare']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "materialized_feature_names = [feature.name for feature in agg_features]\n",
    "materialized_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-31 13:20:07.063 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - KAFKA_SASL_JAAS_CONFIG is not set in the environment variables.\n",
      "2022-10-31 13:20:07.063 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - REDIS_PASSWORD is not set in the environment variables.\n",
      "2022-10-31 13:20:07.065 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__url not found in the config file.\n",
      "2022-10-31 13:20:07.068 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__user not found in the config file.\n",
      "2022-10-31 13:20:07.069 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - MONITORING_DATABASE_SQL_PASSWORD is not set in the environment variables.\n",
      "2022-10-31 13:20:07.069 | WARNING  | feathr.spark_provider._localspark_submission:submit_feathr_job:73 - Local Spark Mode only support basic params right now and should be used only for testing purpose.\n",
      "2022-10-31 13:20:07.071 | INFO     | feathr.spark_provider._localspark_submission:_init_args:202 - Spark job: local_spark_feathr_feature_materialization_job is running on local spark with master: local[*].\n",
      "2022-10-31 13:20:07.078 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:116 - Detail job stdout and stderr are in debug/local_spark_feathr_feature_materialization_job20221031132007/log.\n",
      "2022-10-31 13:20:07.080 | INFO     | feathr.spark_provider._localspark_submission:submit_feathr_job:126 - Local Spark job submit with pid: 13327.\n",
      "2022-10-31 13:20:07.080 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:134 - 4 local spark job(s) in this Launcher, only the latest will be monitored.\n",
      "2022-10-31 13:20:07.081 | INFO     | feathr.spark_provider._localspark_submission:wait_for_completion:135 - Please check auto generated spark command in debug/local_spark_feathr_feature_materialization_job20221031132007/command.sh and detail logs in debug/local_spark_feathr_feature_materialization_job20221031132007/log.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug/local_spark_feathr_feature_materialization_job20221031132007\n",
      "['/var/folders/93/3jr6fmn92v1dwn7y6mmfyvl00000gn/T/tmp1m74z05l/feathr_pyspark_driver.py']\n",
      "x>x>>>>"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [284], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m settings \u001b[39m=\u001b[39m MaterializationSettings(FEATURE_TABLE_NAME \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.job\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                    sinks\u001b[39m=\u001b[39m[redisSink],\n\u001b[1;32m      9\u001b[0m                                    feature_names\u001b[39m=\u001b[39m[\n\u001b[1;32m     10\u001b[0m                                        \u001b[39m\"\u001b[39m\u001b[39mf_location_avg_fare\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf_location_max_fare\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m                                    backfill_time\u001b[39m=\u001b[39mbackfill_time)\n\u001b[1;32m     12\u001b[0m client\u001b[39m.\u001b[39mmaterialize_features(settings)\n\u001b[0;32m---> 14\u001b[0m client\u001b[39m.\u001b[39;49mwait_job_to_finish(\u001b[39m5000\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/GitHub/feathr/feathr_project/feathr/client.py:722\u001b[0m, in \u001b[0;36mFeathrClient.wait_job_to_finish\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait_job_to_finish\u001b[39m(\u001b[39mself\u001b[39m, timeout_sec: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m    720\u001b[0m     \u001b[39m\"\"\"Waits for the job to finish in a blocking way unless it times out\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeathr_spark_launcher\u001b[39m.\u001b[39;49mwait_for_completion(timeout_sec):\n\u001b[1;32m    723\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/feathr/feathr_project/feathr/spark_provider/_localspark_submission.py:143\u001b[0m, in \u001b[0;36m_FeathrLocalSparkJobLauncher.wait_for_completion\u001b[0;34m(self, timeout_seconds)\u001b[0m\n\u001b[1;32m    141\u001b[0m log_read \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark_job_num\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[39mwhile\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (((timeout_seconds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m<\u001b[39m timeout_seconds))):\n\u001b[0;32m--> 143\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m         \u001b[39mif\u001b[39;00m retry \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if REDIS_KEY:\n",
    "    FEATURE_TABLE_NAME = \"nycTaxiDemoFeature\"\n",
    "\n",
    "    backfill_time = BackfillTime(start=datetime(\n",
    "        2020, 4, 1), end=datetime(2020, 4, 1), step=timedelta(days=1))\n",
    "    redisSink = RedisSink(table_name=FEATURE_TABLE_NAME)\n",
    "    settings = MaterializationSettings(FEATURE_TABLE_NAME + \".job\",\n",
    "                                       sinks=[redisSink],\n",
    "                                       feature_names=[\n",
    "                                           \"f_location_avg_fare\", \"f_location_max_fare\"],\n",
    "                                       backfill_time=backfill_time)\n",
    "    client.materialize_features(settings)\n",
    "\n",
    "    client.wait_job_to_finish(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REDIS_KEY:\n",
    "    # Note, to get a single key, you may use client.get_online_features instead\n",
    "    materialized_feature_values = client.multi_get_online_features(\n",
    "        feature_table=FEATURE_TABLE_NAME,\n",
    "        keys=[\"239\", \"265\"],\n",
    "        feature_names=materialized_feature_names,\n",
    "    )\n",
    "    materialized_feature_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the output of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('debug')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5d1b88564ea095927319e95d120a01ba9530a1c584720276480e541fd6461c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
