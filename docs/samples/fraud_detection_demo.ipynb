{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "7b19a0cd-31da-45b7-91a4-9cd561f3d3d8",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "# Feathr Fraud Detection Sample\n",
                "\n",
                "This notebook illustrates the use of Feature Store to create a model that predicts the fraud status of transactions based on the user account data and trasaction data. All the data that was used in the notebook can be found here: https://github.com/microsoft/r-server-fraud-detection\n",
                "\n",
                "\n",
                "In the following Notebook, we \n",
                "1. Install the latest Feathr code (to include some unreleased features) \n",
                "2. Define Environment Variables & `yaml_config` Settings \n",
                "3. Create `FeathrClient` and Define `FeatureAnchor`\n",
                "4. `build_features` and `get_offline_features` \n",
                "5. Train Fraud Detection Model wih `KNeighborsClassifier`\n",
                "6. `materialize_features` and `multi_get_online_features`\n",
                "7. `register_features` and `list_registered_features`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "0b51153e-40dd-43d5-9d3a-501534156e6d",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Setup Feathr Developer Environment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***Prior to running the notebook, if you have not deployed all the required resources, please refer to the guide here and follow the steps to do so: https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html***"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "b9c63dd5-304e-4797-a230-8fb753710dbc",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting feathr[notebook]\n",
                        "  Cloning https://github.com/feathr-ai/feathr.git to /tmp/pip-install-zsn6kl5o/feathr_36df02ae02da42a0a56397c6e9e058e7\n",
                        "  Running command git clone --filter=blob:none --quiet https://github.com/feathr-ai/feathr.git /tmp/pip-install-zsn6kl5o/feathr_36df02ae02da42a0a56397c6e9e058e7\n",
                        "  Resolved https://github.com/feathr-ai/feathr.git to commit 3ebaa49cf36dac90005fb2cbf5412d9103871e9b\n",
                        "  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hCollecting py4j<=0.10.9.7\n",
                        "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
                        "Collecting Jinja2<=3.1.2\n",
                        "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
                        "Collecting pyspark>=3.1.2\n",
                        "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
                        "\u001b[?25hCollecting confluent-kafka<=1.9.2\n",
                        "  Using cached confluent_kafka-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
                        "Collecting requests<=2.28.1\n",
                        "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
                        "Collecting azure-synapse-spark<=0.7.0\n",
                        "  Using cached azure_synapse_spark-0.7.0-py2.py3-none-any.whl (32 kB)\n",
                        "Collecting databricks-cli<=0.17.3\n",
                        "  Using cached databricks_cli-0.17.3-py3-none-any.whl\n",
                        "Collecting pyyaml<=6.0\n",
                        "  Using cached PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
                        "Collecting pyarrow<=9.0.0\n",
                        "  Using cached pyarrow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
                        "Collecting pandas<=1.5.0\n",
                        "  Using cached pandas-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
                        "Collecting azure-storage-file-datalake<=12.5.0\n",
                        "  Using cached azure_storage_file_datalake-12.5.0-py2.py3-none-any.whl (208 kB)\n",
                        "Collecting pandavro<=1.7.1\n",
                        "  Using cached pandavro-1.7.1-py3-none-any.whl\n",
                        "Collecting tqdm<=4.64.1\n",
                        "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
                        "Collecting redis<=4.4.0\n",
                        "  Downloading redis-4.4.0-py3-none-any.whl (236 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.4/236.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-identity>=1.8.0\n",
                        "  Downloading azure_identity-1.12.0-py3-none-any.whl (135 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-keyvault-secrets<=4.6.0\n",
                        "  Using cached azure_keyvault_secrets-4.6.0-py3-none-any.whl (291 kB)\n",
                        "Collecting graphlib-backport<=1.0.3\n",
                        "  Using cached graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n",
                        "Collecting click<=8.1.3\n",
                        "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
                        "Collecting pyhocon<=0.3.59\n",
                        "  Using cached pyhocon-0.3.59-py3-none-any.whl\n",
                        "Collecting msrest<=0.6.21\n",
                        "  Using cached msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
                        "Collecting azure-core<=1.22.1\n",
                        "  Using cached azure_core-1.22.1-py3-none-any.whl (178 kB)\n",
                        "Collecting protobuf<=3.19.4,>=3.0.0\n",
                        "  Downloading protobuf-3.19.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting loguru<=0.6.0\n",
                        "  Using cached loguru-0.6.0-py3-none-any.whl (58 kB)\n",
                        "Collecting typing-extensions>=4.2.0\n",
                        "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
                        "Collecting deltalake>=0.6.2\n",
                        "  Downloading deltalake-0.6.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting avro<=1.11.1\n",
                        "  Using cached avro-1.11.1-py2.py3-none-any.whl\n",
                        "Collecting python-snappy<=0.6.1\n",
                        "  Using cached python_snappy-0.6.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (55 kB)\n",
                        "Collecting pyapacheatlas<=0.14.0\n",
                        "  Using cached pyapacheatlas-0.14.0-py3-none-any.whl (74 kB)\n",
                        "Collecting plotly\n",
                        "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting jupyter==1.0.0\n",
                        "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
                        "Collecting scrapbook<1.0.0,>=0.5.0\n",
                        "  Using cached scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
                        "Collecting papermill<3,>=2.1.2\n",
                        "  Using cached papermill-2.4.0-py3-none-any.whl (38 kB)\n",
                        "Collecting scikit-learn\n",
                        "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting matplotlib==3.6.1\n",
                        "  Using cached matplotlib-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
                        "Collecting jupyter-console\n",
                        "  Using cached jupyter_console-6.4.4-py3-none-any.whl (22 kB)\n",
                        "Collecting ipywidgets\n",
                        "  Using cached ipywidgets-8.0.2-py3-none-any.whl (134 kB)\n",
                        "Requirement already satisfied: ipykernel in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from jupyter==1.0.0->feathr[notebook]) (6.17.1)\n",
                        "Collecting qtconsole\n",
                        "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting nbconvert\n",
                        "  Downloading nbconvert-7.2.6-py3-none-any.whl (273 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting notebook\n",
                        "  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.1/439.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting cycler>=0.10\n",
                        "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from matplotlib==3.6.1->feathr[notebook]) (21.3)\n",
                        "Collecting fonttools>=4.22.0\n",
                        "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from matplotlib==3.6.1->feathr[notebook]) (2.8.2)\n",
                        "Collecting pillow>=6.2.0\n",
                        "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
                        "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
                        "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
                        "Requirement already satisfied: pyparsing>=2.2.1 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from matplotlib==3.6.1->feathr[notebook]) (3.0.9)\n",
                        "Collecting numpy>=1.19\n",
                        "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting contourpy>=1.0.1\n",
                        "  Downloading contourpy-1.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (296 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: six>=1.11.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from azure-core<=1.22.1->feathr[notebook]) (1.16.0)\n",
                        "Collecting cryptography>=2.5\n",
                        "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hCollecting msal-extensions<2.0.0,>=0.3.0\n",
                        "  Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
                        "Collecting msal<2.0.0,>=1.12.0\n",
                        "  Downloading msal-1.20.0-py2.py3-none-any.whl (90 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting azure-common~=1.1\n",
                        "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
                        "Collecting azure-storage-blob<13.0.0,>=12.9.0b1\n",
                        "  Downloading azure_storage_blob-12.14.1-py3-none-any.whl (383 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting oauthlib>=3.1.0\n",
                        "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting pyjwt>=1.7.0\n",
                        "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
                        "Collecting tabulate>=0.7.7\n",
                        "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
                        "Collecting MarkupSafe>=2.0\n",
                        "  Using cached MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
                        "Collecting requests-oauthlib>=0.5.0\n",
                        "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
                        "Collecting isodate>=0.6.0\n",
                        "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from msrest<=0.6.21->feathr[notebook]) (2022.9.24)\n",
                        "Collecting pytz>=2020.1\n",
                        "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting fastavro==1.5.1\n",
                        "  Using cached fastavro-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
                        "Collecting tenacity\n",
                        "  Using cached tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
                        "Requirement already satisfied: entrypoints in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from papermill<3,>=2.1.2->feathr[notebook]) (0.4)\n",
                        "Collecting nbformat>=5.1.2\n",
                        "  Using cached nbformat-5.7.0-py3-none-any.whl (77 kB)\n",
                        "Collecting nbclient>=0.2.0\n",
                        "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting ansiwrap\n",
                        "  Using cached ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
                        "Collecting openpyxl>=3.0\n",
                        "  Using cached openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
                        "Collecting pyparsing>=2.2.1\n",
                        "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
                        "Collecting py4j<=0.10.9.7\n",
                        "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
                        "Collecting async-timeout>=4.0.2\n",
                        "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
                        "Collecting charset-normalizer<3,>=2\n",
                        "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
                        "Collecting urllib3<1.27,>=1.21.1\n",
                        "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting idna<4,>=2.5\n",
                        "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
                        "Requirement already satisfied: ipython in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (8.7.0)\n",
                        "Collecting jsonschema\n",
                        "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
                        "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
                        "Collecting joblib>=1.0.0\n",
                        "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hCollecting scipy>=1.3.2\n",
                        "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting azure-storage-blob<13.0.0,>=12.9.0b1\n",
                        "  Downloading azure_storage_blob-12.14.0-py3-none-any.whl (383 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h  Using cached azure_storage_blob-12.14.0b2-py3-none-any.whl (383 kB)\n",
                        "  Using cached azure_storage_blob-12.14.0b1-py3-none-any.whl (381 kB)\n",
                        "  Using cached azure_storage_blob-12.13.1-py3-none-any.whl (377 kB)\n",
                        "  Using cached azure_storage_blob-12.13.0-py3-none-any.whl (377 kB)\n",
                        "  Using cached azure_storage_blob-12.13.0b1-py3-none-any.whl (377 kB)\n",
                        "  Using cached azure_storage_blob-12.12.0-py3-none-any.whl (366 kB)\n",
                        "  Using cached azure_storage_blob-12.12.0b1-py3-none-any.whl (365 kB)\n",
                        "  Using cached azure_storage_blob-12.11.0-py3-none-any.whl (346 kB)\n",
                        "Collecting cffi>=1.12\n",
                        "  Using cached cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
                        "Collecting portalocker<3,>=1.0\n",
                        "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
                        "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from nbclient>=0.2.0->papermill<3,>=2.1.2->feathr[notebook]) (5.1.0)\n",
                        "Requirement already satisfied: jupyter-client>=6.1.12 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from nbclient>=0.2.0->papermill<3,>=2.1.2->feathr[notebook]) (7.4.8)\n",
                        "Requirement already satisfied: traitlets>=5.3 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from nbclient>=0.2.0->papermill<3,>=2.1.2->feathr[notebook]) (5.6.0)\n",
                        "Collecting fastjsonschema\n",
                        "  Using cached fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
                        "Collecting attrs>=17.4.0\n",
                        "  Using cached attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
                        "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
                        "  Downloading pyrsistent-0.19.2-py3-none-any.whl (57 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting et-xmlfile\n",
                        "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
                        "Collecting textwrap3>=0.9.2\n",
                        "  Using cached textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
                        "Requirement already satisfied: tornado>=6.1 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->feathr[notebook]) (6.2)\n",
                        "Requirement already satisfied: nest-asyncio in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->feathr[notebook]) (1.5.6)\n",
                        "Requirement already satisfied: matplotlib-inline>=0.1 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->feathr[notebook]) (0.1.6)\n",
                        "Requirement already satisfied: psutil in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->feathr[notebook]) (5.9.4)\n",
                        "Requirement already satisfied: pyzmq>=17 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->feathr[notebook]) (24.0.1)\n",
                        "Requirement already satisfied: debugpy>=1.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipykernel->jupyter==1.0.0->feathr[notebook]) (1.6.4)\n",
                        "Requirement already satisfied: pickleshare in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.7.5)\n",
                        "Requirement already satisfied: decorator in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (5.1.1)\n",
                        "Requirement already satisfied: pexpect>4.3 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (4.8.0)\n",
                        "Requirement already satisfied: jedi>=0.16 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.18.2)\n",
                        "Requirement already satisfied: stack-data in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.6.2)\n",
                        "Requirement already satisfied: backcall in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.2.0)\n",
                        "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (3.0.33)\n",
                        "Requirement already satisfied: pygments>=2.4.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (2.13.0)\n",
                        "Collecting jupyterlab-widgets~=3.0\n",
                        "  Using cached jupyterlab_widgets-3.0.3-py3-none-any.whl (384 kB)\n",
                        "Collecting widgetsnbextension~=4.0\n",
                        "  Using cached widgetsnbextension-4.0.3-py3-none-any.whl (2.0 MB)\n",
                        "Collecting beautifulsoup4\n",
                        "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
                        "Collecting bleach\n",
                        "  Using cached bleach-5.0.1-py3-none-any.whl (160 kB)\n",
                        "Collecting pandocfilters>=1.4.1\n",
                        "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
                        "Collecting defusedxml\n",
                        "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
                        "Collecting mistune<3,>=2.0.3\n",
                        "  Using cached mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
                        "Collecting tinycss2\n",
                        "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
                        "Collecting jupyterlab-pygments\n",
                        "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
                        "Collecting terminado>=0.8.3\n",
                        "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
                        "Collecting argon2-cffi\n",
                        "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
                        "Collecting nbclassic>=0.4.7\n",
                        "  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting prometheus-client\n",
                        "  Using cached prometheus_client-0.15.0-py3-none-any.whl (60 kB)\n",
                        "Collecting Send2Trash>=1.8.0\n",
                        "  Using cached Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
                        "Collecting ipython-genutils\n",
                        "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
                        "Collecting qtpy>=2.0.1\n",
                        "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting pycparser\n",
                        "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
                        "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from jedi>=0.16->ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.8.3)\n",
                        "Requirement already satisfied: platformdirs>=2.5 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill<3,>=2.1.2->feathr[notebook]) (2.5.4)\n",
                        "Collecting notebook-shim>=0.1.0\n",
                        "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
                        "Collecting jupyter-server>=1.8\n",
                        "  Downloading jupyter_server-1.23.3-py3-none-any.whl (346 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: ptyprocess>=0.5 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from pexpect>4.3->ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.7.0)\n",
                        "Requirement already satisfied: wcwidth in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.2.5)\n",
                        "Collecting argon2-cffi-bindings\n",
                        "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
                        "Collecting soupsieve>1.2\n",
                        "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
                        "Collecting webencodings\n",
                        "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
                        "Requirement already satisfied: asttokens>=2.1.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from stack-data->ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (2.2.1)\n",
                        "Requirement already satisfied: pure-eval in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from stack-data->ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (0.2.2)\n",
                        "Requirement already satisfied: executing>=1.2.0 in /home/jumin/miniconda3/envs/feathr_main/lib/python3.10/site-packages (from stack-data->ipython->scrapbook<1.0.0,>=0.5.0->feathr[notebook]) (1.2.0)\n",
                        "Collecting websocket-client\n",
                        "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hCollecting anyio<4,>=3.1.0\n",
                        "  Using cached anyio-3.6.2-py3-none-any.whl (80 kB)\n",
                        "Collecting sniffio>=1.1\n",
                        "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
                        "Building wheels for collected packages: pyspark, feathr\n",
                        "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
                        "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845499 sha256=0f475103c6dfe6a385bf18effc5fa0fca67de759aab1f4250f202173b4b007b9\n",
                        "  Stored in directory: /home/jumin/.cache/pip/wheels/0f/f0/3d/517368b8ce80486e84f89f214e0a022554e4ee64969f46279b\n",
                        "  Building wheel for feathr (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25h  Created wheel for feathr: filename=feathr-0.9.0-py3-none-any.whl size=119311 sha256=4e413243162d4ab56e4de61da2580a86fa761a76f836df8e8b9084bcf4bf0df8\n",
                        "  Stored in directory: /tmp/pip-ephem-wheel-cache-3fzfmubz/wheels/72/6a/79/78f71da32c2bdf8534846eb5255ec37a860842063d636f9193\n",
                        "Successfully built pyspark feathr\n",
                        "Installing collected packages: webencodings, textwrap3, Send2Trash, pytz, python-snappy, py4j, mistune, ipython-genutils, fastjsonschema, confluent-kafka, azure-common, widgetsnbextension, websocket-client, urllib3, typing-extensions, tqdm, tinycss2, threadpoolctl, terminado, tenacity, tabulate, soupsieve, sniffio, pyyaml, pyspark, pyrsistent, pyparsing, pyjwt, pycparser, protobuf, prometheus-client, portalocker, pillow, pandocfilters, oauthlib, numpy, MarkupSafe, loguru, kiwisolver, jupyterlab-widgets, jupyterlab-pygments, joblib, isodate, idna, graphlib-backport, fonttools, fastavro, et-xmlfile, defusedxml, cycler, click, charset-normalizer, bleach, avro, attrs, async-timeout, ansiwrap, scipy, requests, redis, pyhocon, pyarrow, plotly, pandas, openpyxl, jsonschema, Jinja2, contourpy, cffi, beautifulsoup4, anyio, scikit-learn, requests-oauthlib, qtpy, pyapacheatlas, pandavro, nbformat, matplotlib, deltalake, databricks-cli, cryptography, azure-core, argon2-cffi-bindings, nbclient, msrest, argon2-cffi, qtconsole, papermill, nbconvert, msal, jupyter-console, ipywidgets, azure-synapse-spark, azure-storage-blob, azure-keyvault-secrets, scrapbook, msal-extensions, jupyter-server, azure-storage-file-datalake, notebook-shim, azure-identity, nbclassic, feathr, notebook, jupyter\n",
                        "  Attempting uninstall: pyparsing\n",
                        "    Found existing installation: pyparsing 3.0.9\n",
                        "    Uninstalling pyparsing-3.0.9:\n",
                        "      Successfully uninstalled pyparsing-3.0.9\n",
                        "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Send2Trash-1.8.0 ansiwrap-0.8.4 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-timeout-4.0.2 attrs-22.1.0 avro-1.11.1 azure-common-1.1.28 azure-core-1.22.1 azure-identity-1.12.0 azure-keyvault-secrets-4.6.0 azure-storage-blob-12.11.0 azure-storage-file-datalake-12.5.0 azure-synapse-spark-0.7.0 beautifulsoup4-4.11.1 bleach-5.0.1 cffi-1.15.1 charset-normalizer-2.1.1 click-8.1.3 confluent-kafka-1.9.2 contourpy-1.0.6 cryptography-38.0.4 cycler-0.11.0 databricks-cli-0.17.3 defusedxml-0.7.1 deltalake-0.6.4 et-xmlfile-1.1.0 fastavro-1.5.1 fastjsonschema-2.16.2 feathr-0.9.0 fonttools-4.38.0 graphlib-backport-1.0.3 idna-3.4 ipython-genutils-0.2.0 ipywidgets-8.0.2 isodate-0.6.1 joblib-1.2.0 jsonschema-4.17.3 jupyter-1.0.0 jupyter-console-6.4.4 jupyter-server-1.23.3 jupyterlab-pygments-0.2.2 jupyterlab-widgets-3.0.3 kiwisolver-1.4.4 loguru-0.6.0 matplotlib-3.6.1 mistune-2.0.4 msal-1.20.0 msal-extensions-1.0.0 msrest-0.6.21 nbclassic-0.4.8 nbclient-0.7.2 nbconvert-7.2.6 nbformat-5.7.0 notebook-6.5.2 notebook-shim-0.2.2 numpy-1.23.5 oauthlib-3.2.2 openpyxl-3.0.10 pandas-1.5.0 pandavro-1.7.1 pandocfilters-1.5.0 papermill-2.4.0 pillow-9.3.0 plotly-5.11.0 portalocker-2.6.0 prometheus-client-0.15.0 protobuf-3.19.4 py4j-0.10.9.5 pyapacheatlas-0.14.0 pyarrow-9.0.0 pycparser-2.21 pyhocon-0.3.59 pyjwt-2.6.0 pyparsing-2.4.7 pyrsistent-0.19.2 pyspark-3.3.1 python-snappy-0.6.1 pytz-2022.6 pyyaml-6.0 qtconsole-5.4.0 qtpy-2.3.0 redis-4.4.0 requests-2.28.1 requests-oauthlib-1.3.1 scikit-learn-1.1.3 scipy-1.9.3 scrapbook-0.5.0 sniffio-1.3.0 soupsieve-2.3.2.post1 tabulate-0.9.0 tenacity-8.1.0 terminado-0.17.1 textwrap3-0.9.2 threadpoolctl-3.1.0 tinycss2-1.2.1 tqdm-4.64.1 typing-extensions-4.4.0 urllib3-1.26.13 webencodings-0.5.1 websocket-client-1.4.2 widgetsnbextension-4.0.3\n"
                    ]
                }
            ],
            "source": [
                "# Install feathr from the latest codes in the repo. You may use `pip install feathr[notebook]` as well.\n",
                "!pip install \"git+https://github.com/feathr-ai/feathr.git#subdirectory=feathr_project&egg=feathr[notebook]\"  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "69222adf-1cb0-410b-b98d-e22877f358c0",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Feathr version: 0.9.0\n"
                    ]
                }
            ],
            "source": [
                "from datetime import datetime, timedelta\n",
                "import glob\n",
                "from math import sqrt\n",
                "import os\n",
                "import tempfile\n",
                "\n",
                "from azure.identity import DefaultAzureCredential\n",
                "from azure.keyvault.secrets import SecretClient\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "import feathr\n",
                "from feathr import (\n",
                "    FeathrClient,\n",
                "    STRING, BOOLEAN, FLOAT, INT32, ValueType,\n",
                "    Feature, DerivedFeature, FeatureAnchor,\n",
                "    BackfillTime, MaterializationSettings,\n",
                "    FeatureQuery, ObservationSettings,\n",
                "    RedisSink,\n",
                "    HdfsSource,\n",
                "    WindowAggTransformation,\n",
                "    TypedKey,\n",
                ")\n",
                "from feathr.utils.config import generate_config\n",
                "\n",
                "\n",
                "print(f\"Feathr version: {feathr.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "RESOURCE_PREFIX = None  # TODO fill the value used to deploy the resources via ARM template\n",
                "PROJECT_NAME = \"fraud_detection\"\n",
                "\n",
                "# Currently support: 'azure_synapse', 'databricks', and 'local' \n",
                "SPARK_CLUSTER = \"local\"\n",
                "\n",
                "# TODO fill values to use databricks cluster:\n",
                "DATABRICKS_CLUSTER_ID = None             # Set Databricks cluster id to use an existing cluster\n",
                "DATABRICKS_URL = None                    # Set Databricks workspace url to use databricks\n",
                "DATABRICKS_WORKSPACE_TOKEN_VALUE = None  # Set Databricks workspace token to use databricks\n",
                "\n",
                "# TODO fill values to use Azure Synapse cluster:\n",
                "AZURE_SYNAPSE_SPARK_POOL = None  # Set Azure Synapse Spark pool name\n",
                "AZURE_SYNAPSE_URL = None         # Set Azure Synapse workspace url to use Azure Synapse\n",
                "ADLS_KEY = None                  # Set Azure Data Lake Storage key to use Azure Synapse\n",
                "\n",
                "# An existing Feathr config file path. If None, we'll generate a new config based on the constants in this cell.\n",
                "FEATHR_CONFIG_PATH = None\n",
                "\n",
                "# If set True, use an interactive browser authentication to get the redis password.\n",
                "USE_CLI_AUTH = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO remove this cell\n",
                "RESOURCE_PREFIX = \"juntest\"\n",
                "USE_CLI_AUTH = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "if SPARK_CLUSTER == \"azure_synapse\" and not os.environ.get(\"ADLS_KEY\"):\n",
                "    os.environ[\"ADLS_KEY\"] = ADLS_KEY\n",
                "elif SPARK_CLUSTER == \"databricks\" and not os.environ.get(\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"):\n",
                "    os.environ[\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"] = DATABRICKS_WORKSPACE_TOKEN_VALUE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "c0299d67-1103-4aa4-ba57-300498ae2579",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "if USE_CLI_AUTH:\n",
                "    !az login --use-device-code"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Permission\n",
                "To run the cells below, you need additional permission: permission to your managed identity to access the keyvault, and permission to the user to access the Storage Blob. Run the following lines of command in the Cloud Shell in order to grant yourself the access.\n",
                "\n",
                "```\n",
                "userId=<email_id_of_account_requesting_access>\n",
                "resource_prefix=<resource_prefix>\n",
                "synapse_workspace_name=\"${resource_prefix}syws\"\n",
                "keyvault_name=\"${resource_prefix}kv\"\n",
                "objectId=$(az ad user show --id $userId --query id -o tsv)\n",
                "az keyvault update --name $keyvault_name --enable-rbac-authorization false\n",
                "az keyvault set-policy -n $keyvault_name --secret-permissions get list --object-id $objectId\n",
                "az role assignment create --assignee $userId --role \"Storage Blob Data Contributor\"\n",
                "az synapse role assignment create --workspace-name $synapse_workspace_name --role \"Synapse Contributor\" --assignee $userId\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "a8a70f27-d520-4d3c-bb8c-f364f84cb738",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# Redis password\n",
                "if 'REDIS_PASSWORD' not in os.environ:\n",
                "    # Try to get all the required credentials from Azure Key Vault\n",
                "    from azure.identity import AzureCliCredential, DefaultAzureCredential \n",
                "    from azure.keyvault.secrets import SecretClient\n",
                "\n",
                "    # TODO assume the resources are deployed by using the ARM template. If not, please set your vault url name.\n",
                "    vault_url = f\"https://{RESOURCE_PREFIX}kv.vault.azure.net\"\n",
                "    if USE_CLI_AUTH:\n",
                "        credential = AzureCliCredential(additionally_allowed_tenants=['*'],)\n",
                "    else:\n",
                "        credential = DefaultAzureCredential(\n",
                "            exclude_interactive_browser_credential=False,\n",
                "            additionally_allowed_tenants=['*'],\n",
                "        )\n",
                "    secret_client = SecretClient(vault_url=vault_url, credential=credential)\n",
                "    retrieved_secret = secret_client.get_secret('FEATHR-ONLINE-STORE-CONN').value\n",
                "    os.environ['REDIS_PASSWORD'] = retrieved_secret.split(\",\")[1].split(\"password=\", 1)[1]\n",
                "\n",
                "# feathr_output_path = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_output'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "50b2f73e-6380-42c3-91e8-4f3e15bc10d6",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "api_version: 1\n",
                        "feature_registry:\n",
                        "  api_endpoint: https://juntestwebapp.azurewebsites.net/api/v1\n",
                        "offline_store:\n",
                        "  adls:\n",
                        "    adls_enabled: 'true'\n",
                        "  wasb:\n",
                        "    wasb_enabled: 'true'\n",
                        "online_store:\n",
                        "  redis:\n",
                        "    host: juntestredis.redis.cache.windows.net\n",
                        "    port: '6380'\n",
                        "    ssl_enabled: 'true'\n",
                        "project_config:\n",
                        "  project_name: fraud_detection\n",
                        "spark_config:\n",
                        "  spark_cluster: local\n",
                        "  spark_result_output_parts: '1'\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "if FEATHR_CONFIG_PATH:\n",
                "    config_path = FEATHR_CONFIG_PATH\n",
                "else:\n",
                "    config_path = generate_config(\n",
                "        resource_prefix=RESOURCE_PREFIX,\n",
                "        project_name=PROJECT_NAME,\n",
                "        spark_config__spark_cluster=SPARK_CLUSTER,\n",
                "        spark_config__azure_synapse__dev_url=AZURE_SYNAPSE_URL,\n",
                "        spark_config__azure_synapse__pool_name=AZURE_SYNAPSE_SPARK_POOL,\n",
                "        spark_config__databricks__workspace_instance_url=DATABRICKS_URL,\n",
                "        databricks_cluster_id=DATABRICKS_CLUSTER_ID,\n",
                "    )\n",
                "\n",
                "with open(config_path, 'r') as f: \n",
                "    print(f.read())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "eab0957c-c906-4297-a729-8dd8d79cb629",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Initialize `Feathr Client`\n",
                "- `FeathrClient`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "3734eee3-12f9-44db-a440-ad375ef859f0",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2022-12-07 04:28:50.702 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n",
                        "2022-12-07 04:28:50.713 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__s3__s3_enabled not found in the config file.\n",
                        "2022-12-07 04:28:50.719 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__jdbc__jdbc_enabled not found in the config file.\n",
                        "2022-12-07 04:28:50.722 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__snowflake__snowflake_enabled not found in the config file.\n",
                        "2022-12-07 04:28:50.728 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__feathr_runtime_location not found in the config file.\n",
                        "2022-12-07 04:28:50.730 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__workspace not found in the config file.\n",
                        "2022-12-07 04:28:50.733 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__local__master not found in the config file.\n",
                        "2022-12-07 04:28:50.739 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - feature_registry__purview__purview_name not found in the config file.\n",
                        "2022-12-07 04:28:50.739 | INFO     | feathr.client:__init__:196 - Feathr client 0.9.0 initialized successfully.\n"
                    ]
                }
            ],
            "source": [
                "client = FeathrClient(config_path=config_path, credential=credential)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "f6adbca1-5642-4ac1-bff7-e7c9d4d9e5b2",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Define Features\n",
                "- `HdfsSource`\n",
                "- `TypedKey`\n",
                "- `Feature`\n",
                "- `FeatureAnchor`\n",
                "- `DerivedFeature`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "from feathr.datasets.constants import FRAUD_DETECTION_ACCOUNT_INFO_URL, FRAUD_DETECTION_TRANSACTIONS_URL\n",
                "\n",
                "from feathr.datasets.utils import maybe_download\n",
                "\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 13.0k/13.0k [00:00<00:00, 16.3kKB/s]\n",
                        "100%|██████████| 786/786 [00:00<00:00, 2.34kKB/s]\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# upload dataset if needed\n",
                "\n",
                "account_info_file_path = str(Path(PROJECT_NAME, \"account_info.csv\"))\n",
                "transactions_file_path = str(Path(PROJECT_NAME, \"transactions.csv\"))\n",
                "maybe_download(\n",
                "    src_url=FRAUD_DETECTION_ACCOUNT_INFO_URL,\n",
                "    dst_filepath=account_info_file_path,\n",
                ")\n",
                "maybe_download(\n",
                "    src_url=FRAUD_DETECTION_TRANSACTIONS_URL,\n",
                "    dst_filepath=transactions_file_path,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload files to cloud\n",
                "if client.spark_runtime == \"local\" or (client.spark_runtime == \"databricks\" and is_databricks()):\n",
                "    # In local mode, we can use the same data path as the source.\n",
                "    # If the notebook is running on databricks, DATA_FILE_PATH should be already a dbfs path.\n",
                "    data_source_path = DATA_FILE_PATH\n",
                "else:\n",
                "    # Otherwise, upload the local file to the cloud storage (either dbfs or adls).\n",
                "    data_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(DATA_FILE_PATH)    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "b073b509-0f95-4e23-b16b-ffd8190fb6a2",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "### Account Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "b3668eeb-e4a0-4327-baf6-5521c856f51d",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "#Refer to <https://feathr.readthedocs.io/en/latest/feathr.html> to learn more about the details of each method\n",
                "account_info = HdfsSource(\n",
                "    name=\"AccountData\",\n",
                "    path=\"wasbs://frauddata@feathrdatastorage.blob.core.windows.net/account_out_small.csv\",\n",
                "    event_timestamp_column=\"transactionDate\",\n",
                "    timestamp_format=\"yyyyMMdd\",\n",
                ")\n",
                "\n",
                "accountId = TypedKey(key_column=\"accountID\",\n",
                "                       key_column_type=ValueType.INT32,\n",
                "                       description=\"account id\")\n",
                "\n",
                "account_country = Feature(name=\"account_country\",\n",
                "                           key=accountId,\n",
                "                           feature_type=STRING, \n",
                "                           transform=\"accountCountry\")\n",
                "\n",
                "is_user_registered = Feature(name=\"is_user_registered\",\n",
                "                                    key=accountId,\n",
                "                                    feature_type=BOOLEAN,\n",
                "                                    transform=\"isUserRegistered==TRUE\")\n",
                "\n",
                "num_payment_rejects_1d_per_user = Feature(name=\"num_payment_rejects_1d_per_user\",\n",
                "                                    key=accountId,\n",
                "                                    feature_type=INT32,\n",
                "                                    transform=\"numPaymentRejects1dPerUser\")\n",
                "\n",
                "account_age = Feature(name=\"account_age\",\n",
                "                      key=accountId,\n",
                "                      feature_type=INT32,\n",
                "                      transform=\"accountAge\")\n",
                "                                    \n",
                "features = [\n",
                "    account_country,\n",
                "    account_age,\n",
                "    is_user_registered,\n",
                "    num_payment_rejects_1d_per_user\n",
                "]\n",
                "\n",
                "account_anchor = FeatureAnchor(name=\"account_features\",\n",
                "                               source=account_info,\n",
                "                               features=features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "6f12c07e-4faf-4411-8acd-6f5d13b962f8",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "### Transaction Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "280062b9-ae21-4a1a-ae94-86a5c17fd589",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# # #Refer to <https://feathr.readthedocs.io/en/latest/feathr.html> to learn more about the details of each method\n",
                "\n",
                "transaction_data = HdfsSource(name=\"transaction_data\",\n",
                "                          path=\"wasbs://frauddata@feathrdatastorage.blob.core.windows.net/transaction_out_small.csv\",\n",
                "                          event_timestamp_column=\"transactionDate\",\n",
                "                          timestamp_format=\"yyyyMMdd\")\n",
                "\n",
                "transaction_id = Feature(name=\"transaction_id\",\n",
                "                        key=accountId,\n",
                "                        feature_type=STRING,\n",
                "                        transform=\"transactionID\")\n",
                "\n",
                "transaction_currency_code = Feature(name=\"transaction_currency_code\",\n",
                "                                    key=accountId,\n",
                "                                    feature_type=STRING,\n",
                "                                    transform=\"transactionCurrencyCode\")\n",
                "                           \n",
                "transaction_amount = Feature(name=\"transaction_amount\",\n",
                "                            key=accountId,\n",
                "                            feature_type=FLOAT,\n",
                "                            transform=\"transactionAmount\")\n",
                "\n",
                "transaction_device_id = Feature(name=\"transaction_device_id\",\n",
                "                                key=accountId,\n",
                "                                feature_type=FLOAT,\n",
                "                                transform=\"transactionDeviceId\")\n",
                "\n",
                "transaction_ip_address = Feature(name=\"transaction_ip_address\",\n",
                "                                key=accountId,\n",
                "                                feature_type=FLOAT,\n",
                "                                transform=\"transactionIPaddress\")\n",
                "\n",
                "transaction_time = Feature(name=\"transaction_time\",\n",
                "                            key=accountId,\n",
                "                            feature_type=INT32,\n",
                "                            transform=\"transactionTime\")\n",
                "\n",
                "fraud_status = Feature(name=\"fraud_status\",\n",
                "                       key=accountId,\n",
                "                       feature_type=STRING,\n",
                "                       transform=\"fraud_tag\")\n",
                "\n",
                "features = [\n",
                "    transaction_id,\n",
                "    transaction_amount,\n",
                "    transaction_device_id,\n",
                "    transaction_ip_address,\n",
                "    transaction_time,\n",
                "    transaction_currency_code,\n",
                "    fraud_status\n",
                "]\n",
                "\n",
                "transaction_feature_anchor = FeatureAnchor(name=\"transaction_features\",\n",
                "                                            source=transaction_data,\n",
                "                                            features=features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "86ac05e1-26bb-4820-87ea-f547e3561181",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "### Transaction Aggregated Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "4c969554-f690-42f5-b70a-d962bf558b03",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# average amount of transaction in the past week\n",
                "transactions_aggr = HdfsSource(name=\"transactions_aggr\",\n",
                "                          path=\"wasbs://frauddata@feathrdatastorage.blob.core.windows.net/transaction_out_small.csv\",\n",
                "                          event_timestamp_column=\"transactionDate\",\n",
                "                          timestamp_format=\"yyyyMMdd\")\n",
                "\n",
                "# average amount of transaction in that week\n",
                "avg_transaction_amount = Feature(name=\"avg_transaction_amount\",\n",
                "                                key=accountId,\n",
                "                                feature_type=FLOAT,\n",
                "                                transform=WindowAggTransformation(agg_expr=\"cast_float(transactionAmount)\",\n",
                "                                                            agg_func=\"AVG\",\n",
                "                                                            window=\"7d\"))\n",
                "\n",
                "# number of transaction that took place in a day\n",
                "num_trasaction_count_in_day = Feature(name=\"num_trasaction_count_in_day\",\n",
                "                                    key=accountId,\n",
                "                                    feature_type=INT32,\n",
                "                                    transform=WindowAggTransformation(agg_expr=\"transactionID\",\n",
                "                                                                agg_func=\"COUNT\",\n",
                "                                                                window=\"1d\"))\n",
                "\n",
                "# Amount of transaction that took place in a day\n",
                "total_transaction_amount_in_day = Feature(name=\"total_transaction_amount_in_day\",\n",
                "                                    key=accountId,\n",
                "                                    feature_type=FLOAT,\n",
                "                                    transform=WindowAggTransformation(agg_expr=\"cast_float(transactionAmount)\",\n",
                "                                                                agg_func=\"SUM\",\n",
                "                                                                window=\"1d\"))\n",
                "\n",
                "# average time of transaction in the past week\n",
                "avg_transaction_time = Feature(name=\"avg_transaction_time\",\n",
                "                            key=accountId,\n",
                "                            feature_type=INT32,\n",
                "                            transform=WindowAggTransformation(agg_expr=\"cast_float(transactionTime)\",\n",
                "                                                          agg_func=\"AVG\",\n",
                "                                                          window=\"7d\"))                                                            \n",
                "\n",
                "# total number of currency used for transaction in the past week\n",
                "num_currency_type_in_week = Feature(name=\"num_currency_type_in_week\",\n",
                "                                    key=accountId,\n",
                "                                    feature_type=INT32,\n",
                "                                    transform=WindowAggTransformation(agg_expr=\"transactionCurrencyCode\",\n",
                "                                                                    agg_func=\"COUNT\",\n",
                "                                                                    window=\"7d\"))\n",
                "\n",
                "# number of different ip address used for transaction in the past week\n",
                "num_ip_address_count = Feature(name=\"num_ip_address_count\",\n",
                "                        key=accountId,\n",
                "                        feature_type=INT32,\n",
                "                        transform=WindowAggTransformation(agg_expr=\"transactionIPaddress\",\n",
                "                                                          agg_func=\"COUNT\",\n",
                "                                                          window=\"7d\"))\n",
                "\n",
                "# number of devices used for the transaction in the past week\n",
                "num_device_count = Feature(name=\"num_device_count\",\n",
                "                            key=accountId,\n",
                "                            feature_type=INT32,\n",
                "                            transform=WindowAggTransformation(agg_expr=\"transactionDeviceId\",\n",
                "                                                            agg_func=\"COUNT\",\n",
                "                                                            window=\"7d\"))\n",
                "\n",
                "# find the time of most recent transaction\n",
                "time_most_recent_transaction = Feature(name=\"time_most_recent_transaction\",\n",
                "                                        key=accountId,\n",
                "                                        feature_type=INT32,\n",
                "                                        transform=WindowAggTransformation(agg_expr=\"transactionTime\",\n",
                "                                                                        agg_func=\"LATEST\",\n",
                "                                                                        window=\"7d\"))\n",
                "\n",
                "features = [\n",
                "    avg_transaction_amount,\n",
                "    avg_transaction_time,\n",
                "    total_transaction_amount_in_day,\n",
                "    num_trasaction_count_in_day,\n",
                "    num_currency_type_in_week,\n",
                "    num_ip_address_count,\n",
                "    num_device_count,\n",
                "    time_most_recent_transaction\n",
                "]\n",
                "\n",
                "aggr_anchor = FeatureAnchor(name=\"transaction_aggr_features\",\n",
                "                            source=transactions_aggr,\n",
                "                            features=features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "17cc5132-461f-4d3d-b517-1f7e69d23252",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "### Derived Features\n",
                "- `DerivedFeature`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "7ac10ce4-e222-469c-bb2e-1658b45e3eda",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# derived features\n",
                "feature_diff_current_and_avg_amount = DerivedFeature(name=\"feature_diff_current_and_avg_amount\",\n",
                "                                                    key=accountId,\n",
                "                                                    feature_type=FLOAT,\n",
                "                                                    input_features=[\n",
                "                                                        transaction_amount, avg_transaction_amount],\n",
                "                                                    transform=\"transaction_amount - avg_transaction_amount\")\n",
                "\n",
                "feature_time_pass_after_most_recent_transaction = DerivedFeature(name=\"feature_time_pass_after_most_recent_transaction\",\n",
                "                                                                key=accountId,\n",
                "                                                                feature_type=INT32,\n",
                "                                                                input_features=[\n",
                "                                                                    transaction_time, time_most_recent_transaction],\n",
                "                                                                transform=\"cast_int(transaction_time) - cast_int(time_most_recent_transaction)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "a9ec8416-9ac6-4499-b60f-55822265b893",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Build Defined Features\n",
                "- `build_features`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "d9d32d4f-2b60-4978-bb87-c7d2160e98eb",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "client.build_features(anchor_list=[account_anchor, transaction_feature_anchor, aggr_anchor], \n",
                "                      derived_feature_list=[feature_time_pass_after_most_recent_transaction, feature_diff_current_and_avg_amount])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "fa9e53b9-e7d4-4b25-b486-dc9e6801369a",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Get Offline Features\n",
                "- `FeatureQuery`\n",
                "- `ObservationSettings`\n",
                "- `get_offline_features`\n",
                "- `feathr_spark_launcher.download_result`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "b6340f2f-79dc-442b-a202-b2f2078a62ac",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "if client.spark_runtime == 'databricks':\n",
                "    output_path = 'dbfs:/feathrfrauddetection_test.avro'\n",
                "else:\n",
                "    output_path = feathr_output_path\n",
                "\n",
                "feature_query = FeatureQuery(\n",
                "    feature_list=[\"account_country\",\n",
                "                  \"transaction_time\",\n",
                "                  \"num_currency_type_in_week\",\n",
                "                  \"num_trasaction_count_in_day\",\n",
                "                  \"total_transaction_amount_in_day\",\n",
                "                  \"fraud_status\",\n",
                "                  \"is_user_registered\",\n",
                "                  \"avg_transaction_amount\",\n",
                "                  \"num_ip_address_count\",\n",
                "                  \"num_device_count\",\n",
                "                  \"time_most_recent_transaction\",\n",
                "                  \"feature_diff_current_and_avg_amount\",\n",
                "                  \"feature_time_pass_after_most_recent_transaction\"], key=accountId)\n",
                "                    \n",
                "settings = ObservationSettings(\n",
                "    observation_path=\"wasbs://frauddata@feathrdatastorage.blob.core.windows.net/observation_out_small.csv\",\n",
                "    event_timestamp_column=\"transactionDate\",\n",
                "    timestamp_format=\"yyyyMMdd\")\n",
                "    \n",
                "client.get_offline_features(observation_settings=settings,\n",
                "                            feature_query=feature_query,\n",
                "                            output_path=output_path)\n",
                "client.wait_job_to_finish(timeout_sec=10000000000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "5b7603ee-0c81-49ed-8e1f-53161ae57cbf",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import pandavro as pdx\n",
                "import glob\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "from feathr import BackfillTime, MaterializationSettings, RedisSink"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "997db6eb-c7d8-4f5e-b6e0-09733ff706b7",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def get_result_df(client: FeathrClient) -> pd.DataFrame:\n",
                "    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n",
                "    res_url = client.get_job_result_uri(block=True, timeout_sec=600)\n",
                "    tmp_dir = tempfile.TemporaryDirectory()\n",
                "    client.feathr_spark_launcher.download_result(result_path=res_url, local_folder=tmp_dir.name)\n",
                "    dataframe_list = []\n",
                "    # assuming the result are in avro format\n",
                "    for file in glob.glob(os.path.join(tmp_dir.name, '*.avro')):\n",
                "        dataframe_list.append(pdx.read_avro(file))\n",
                "    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
                "    tmp_dir.cleanup()\n",
                "    return vertical_concat_df\n",
                "\n",
                "df_res = get_result_df(client)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "7fff1ac7-90d1-469b-a54c-397904417796",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Feature Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "e482625e-2ecd-45cb-9d43-5baacd445006",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "filepath = Path('./result_out.csv')\n",
                "df_res.to_csv(filepath, index=False) \n",
                "df_res.reset_index()\n",
                "df_res"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "b4f86c53-16cf-4836-969b-7c34f0922057",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Train Fraud Detection Model with Calculated Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "0d9d06b8-01e7-4772-8734-6ebfe1996b03",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.model_selection import train_test_split \n",
                "import seaborn as sns\n",
                "\n",
                "final_df = df_res\n",
                "final_df.drop(['accountID'], axis=1, inplace=True, errors='ignore')\n",
                "final_df.drop(['transactionDate'], axis=1, inplace=True, errors='ignore')\n",
                "final_df.drop(['account_country'], axis=1, inplace=True, errors='ignore')\n",
                "final_df = final_df.fillna(0)\n",
                "\n",
                "x_train, x_test, y_train, y_test = train_test_split(final_df.drop([\"fraud_status\"], axis=1),\n",
                "                                                    final_df[\"fraud_status\"],\n",
                "                                                    test_size=0.20,\n",
                "                                                    random_state=0)\n",
                "  \n",
                "K = []\n",
                "training = []\n",
                "test = []\n",
                "scores = {}\n",
                "  \n",
                "for k in range(2, 21):\n",
                "    clf = KNeighborsClassifier(n_neighbors = k)\n",
                "    clf.fit(x_train, y_train)\n",
                "  \n",
                "    training_score = clf.score(x_train, y_train)\n",
                "    test_score = clf.score(x_test, y_test)\n",
                "    K.append(k)\n",
                "  \n",
                "    training.append(training_score)\n",
                "    test.append(test_score)\n",
                "    scores[k] = [training_score, test_score]\n",
                "\n",
                "for keys, values in scores.items():\n",
                "    print(keys, ':', values)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "83e69f23-aa4e-4893-8907-6d5f0792c23f",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Materialize Features in Redis\n",
                "- `BackfillTime`\n",
                "- `RedisSink`\n",
                "- `materialize_features`\n",
                "- `multi_get_online_features`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "faad23c1-d827-4674-b630-83530574c27d",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "backfill_time = BackfillTime(start=datetime(\n",
                "    2013, 4, 7), end=datetime(2013, 4, 7), step=timedelta(days=1))\n",
                "redisSink = RedisSink(table_name=\"fraudDetectionDemoFeature\")\n",
                "settings = MaterializationSettings(\"fraudDetectionDemoFeature\",\n",
                "                                   backfill_time=backfill_time,\n",
                "                                   sinks=[redisSink],\n",
                "                                   feature_names=[\"fraud_status\"])\n",
                "\n",
                "client.materialize_features(settings, allow_materialize_non_agg_feature =True)\n",
                "client.wait_job_to_finish(timeout_sec=5000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "1f5b191f-b1e8-49e4-b54d-ffc2f8c0a0b8",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "client.multi_get_online_features('fraudDetectionDemoFeature', ['1759222192247110', '914800996051170'], [\n",
                "                                 \"fraud_status\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "9c3b2403-95d6-44a1-b536-d2088608ff58",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "client.multi_get_online_features('fraudDetectionDemoFeature', ['1759222192247110', '914800996051170', '844428033864668'], [\n",
                "                                 \"fraud_status\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "71ba8699-3c42-4f73-be59-95b29f468696",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": [
                "## Register Features with Registry APIs\n",
                "- `register_features`\n",
                "- `list_registered_features`\n",
                "- Above queries are send to a Standard Registry API Service (both `Purview` and `SQL` backend are supported)\n",
                "- More friendly interface with detailed lineage can be found in: [Feathr UI](https://feathr-sql-registry.azurewebsites.net/)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "c5028dd9-01ed-4394-a5c7-623e674125f6",
                    "showTitle": false,
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "client.register_features()\n",
                "client.list_registered_features(project_name=\"fraud_detection_test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "inputWidgets": {},
                    "nuid": "cb814ce7-72b9-4622-8518-106d4acf9008",
                    "showTitle": false,
                    "title": ""
                }
            },
            "source": []
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "dashboards": [],
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "fraud_detection_feathr_test_2",
            "notebookOrigID": 1891349682974490,
            "widgets": {}
        },
        "kernelspec": {
            "display_name": "Python 3.10.8 ('feathr')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "e34a1a57d2e174682770a82d94a178aa36d3ccfaa21227c5d2308e319b7ae532"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
