{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b19a0cd-31da-45b7-91a4-9cd561f3d3d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feathr Fraud Detection Sample\n",
    "\n",
    "This notebook illustrates the use of Feature Store to create a model that predicts the fraud status of transactions based on the user account data and trasaction data. The main focus of this notebook is to depict:\n",
    "* How a feature designer can define heterogenious features from different data sources (user account data and transaction data) with different keys by using Feathr, and\n",
    "* How a feature consumer can extract features using multiple `FeatureQuery`.\n",
    "\n",
    "The sample fraud transaction datasets that are used in the notebook can be found here: https://github.com/microsoft/r-server-fraud-detection.\n",
    "\n",
    "The outline of the notebook is as follows: \n",
    "1. Setup Feathr environment\n",
    "2. Initialize Feathr client \n",
    "3. Define features\n",
    "4. Build features and extract offline features\n",
    "5. Build a fraud detection model\n",
    "6. Materialize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Feathr Environment\n",
    "\n",
    "### Deploy Necessary Azure Resources to run Feathr Feature Store\n",
    "\n",
    "Prior to running the notebook, if you have not deployed all the required resources, please refer to the guide here and follow the steps to do so: https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html\n",
    "\n",
    "### Access to Resources\n",
    "To run the cells below, you need additional permissions for your managed identity to access the keyvault and the Storage Account. You may run the following lines of command in the Cloud Shell in order to grant yourself the access.\n",
    "\n",
    "```\n",
    "userId=<email_id_of_account_requesting_access>\n",
    "resource_prefix=<resource_prefix>\n",
    "synapse_workspace_name=\"${resource_prefix}syws\"\n",
    "keyvault_name=\"${resource_prefix}kv\"\n",
    "objectId=$(az ad user show --id $userId --query id -o tsv)\n",
    "az keyvault update --name $keyvault_name --enable-rbac-authorization false\n",
    "az keyvault set-policy -n $keyvault_name --secret-permissions get list --object-id $objectId\n",
    "az role assignment create --assignee $userId --role \"Storage Blob Data Contributor\"\n",
    "az synapse role assignment create --workspace-name $synapse_workspace_name --role \"Synapse Contributor\" --assignee $userId\n",
    "```\n",
    "\n",
    "### Install Python Packages\n",
    "\n",
    "Uncomment following cell and run it to install Feathr python package and necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9c63dd5-304e-4797-a230-8fb753710dbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install feathr from the latest codes in the repo. You may use `pip install feathr[notebook]` as well.\n",
    "# !pip install \"git+https://github.com/feathr-ai/feathr.git#subdirectory=feathr_project&egg=feathr[notebook]\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Feathr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69222adf-1cb0-410b-b98d-e22877f358c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feathr version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import feathr\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    STRING, BOOLEAN, FLOAT, INT32, ValueType,\n",
    "    Feature, DerivedFeature, FeatureAnchor,\n",
    "    BackfillTime, MaterializationSettings,\n",
    "    FeatureQuery, ObservationSettings,\n",
    "    RedisSink,\n",
    "    HdfsSource,\n",
    "    WindowAggTransformation,\n",
    "    TypedKey,\n",
    ")\n",
    "from feathr.datasets.constants import (\n",
    "    FRAUD_DETECTION_ACCOUNT_INFO_URL,\n",
    "    FRAUD_DETECTION_FRAUD_TRANSACTIONS_URL,\n",
    "    FRAUD_DETECTION_UNTAGGED_TRANSACTIONS_URL,\n",
    ")\n",
    "from feathr.datasets.utils import maybe_download\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from feathr.utils.platform import is_databricks\n",
    "\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RESOURCE_PREFIX = \"\"  # TODO fill the value used to deploy the resources via ARM template\n",
    "PROJECT_NAME = \"fraud_detection\"\n",
    "\n",
    "# Currently support: 'azure_synapse', 'databricks', and 'local' \n",
    "SPARK_CLUSTER = \"local\"\n",
    "\n",
    "# TODO fill values to use databricks cluster:\n",
    "DATABRICKS_CLUSTER_ID = None             # Set Databricks cluster id to use an existing cluster\n",
    "if is_databricks():\n",
    "    # If this notebook is running on Databricks, its context can be used to retrieve token and instance URL\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    DATABRICKS_WORKSPACE_TOKEN_VALUE = ctx.apiToken().get()\n",
    "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = f\"https://{ctx.tags().get('browserHostName').get()}\"\n",
    "else:\n",
    "    DATABRICKS_WORKSPACE_TOKEN_VALUE = None                  # Set Databricks workspace token to use databricks\n",
    "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = None  # Set Databricks workspace url to use databricks\n",
    "\n",
    "# TODO fill values to use Azure Synapse cluster:\n",
    "AZURE_SYNAPSE_SPARK_POOL = None  # Set Azure Synapse Spark pool name\n",
    "AZURE_SYNAPSE_URL = None         # Set Azure Synapse workspace url to use Azure Synapse\n",
    "ADLS_KEY = None                  # Set Azure Data Lake Storage key to use Azure Synapse\n",
    "\n",
    "USE_CLI_AUTH = False  # Set to True to use CLI authentication\n",
    "\n",
    "# An existing Feathr config file path. If None, we'll generate a new config based on the constants in this cell.\n",
    "FEATHR_CONFIG_PATH = None\n",
    "\n",
    "# (For the notebook test pipeline) If true, use ScrapBook package to collect the results.\n",
    "SCRAP_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_PREFIX = \"juntest\"\n",
    "USE_CLI_AUTH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_CLUSTER == \"azure_synapse\" and not os.environ.get(\"ADLS_KEY\"):\n",
    "    os.environ[\"ADLS_KEY\"] = ADLS_KEY\n",
    "elif SPARK_CLUSTER == \"databricks\" and not os.environ.get(\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"):\n",
    "    os.environ[\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"] = DATABRICKS_WORKSPACE_TOKEN_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8a70f27-d520-4d3c-bb8c-f364f84cb738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get an authentication credential to access Azure resources and register features\n",
    "if USE_CLI_AUTH:\n",
    "    # Use AZ CLI interactive browser authentication\n",
    "    !az login --use-device-code\n",
    "    from azure.identity import AzureCliCredential\n",
    "    credential = AzureCliCredential(additionally_allowed_tenants=['*'],)\n",
    "elif \"AZURE_TENANT_ID\" in os.environ and \"AZURE_CLIENT_ID\" in os.environ and \"AZURE_CLIENT_SECRET\" in os.environ:\n",
    "    # Use Environment variable secret\n",
    "    from azure.identity import EnvironmentCredential\n",
    "    credential = EnvironmentCredential()\n",
    "else:\n",
    "    # Try to use the default credential\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    credential = DefaultAzureCredential(\n",
    "        exclude_interactive_browser_credential=False,\n",
    "        additionally_allowed_tenants=['*'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AzureCliCredential.get_token failed: Please run 'az login' to set up an account\n"
     ]
    },
    {
     "ename": "CredentialUnavailableError",
     "evalue": "Please run 'az login' to set up an account",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/identity/_credentials/azure_cli.py:148\u001b[0m, in \u001b[0;36m_run_command\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m    146\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mcheck_output(args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    149\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m    150\u001b[0m     \u001b[39m# non-zero return from shell\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/subprocess.py:411\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39muniversal_newlines\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 411\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39;49mpopenargs, stdout\u001b[39m=\u001b[39;49mPIPE, timeout\u001b[39m=\u001b[39;49mtimeout, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    412\u001b[0m            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mstdout\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/subprocess.py:512\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 512\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    513\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    514\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/bin/sh', '-c', 'az account get-access-token --output json --resource https://vault.azure.net --tenant 72f988bf-86f1-41af-91ab-2d7cd011db47']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCredentialUnavailableError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m vault_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m{\u001b[39;00mRESOURCE_PREFIX\u001b[39m}\u001b[39;00m\u001b[39mkv.vault.azure.net\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m secret_client \u001b[39m=\u001b[39m SecretClient(vault_url\u001b[39m=\u001b[39mvault_url, credential\u001b[39m=\u001b[39mcredential)\n\u001b[0;32m----> 6\u001b[0m retrieved_secret \u001b[39m=\u001b[39m secret_client\u001b[39m.\u001b[39;49mget_secret(\u001b[39m'\u001b[39;49m\u001b[39mFEATHR-ONLINE-STORE-CONN\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mvalue\n\u001b[1;32m      7\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mREDIS_PASSWORD\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m retrieved_secret\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mpassword=\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/tracing/decorator.py:83\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     85\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/keyvault/secrets/_client.py:72\u001b[0m, in \u001b[0;36mSecretClient.get_secret\u001b[0;34m(self, name, version, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m@distributed_trace\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_secret\u001b[39m(\u001b[39mself\u001b[39m, name, version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     54\u001b[0m     \u001b[39m# type: (str, str, **Any) -> KeyVaultSecret\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m\"\"\"Get a secret. Requires the secrets/get permission.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[39m    :param str name: The name of the secret\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m            :dedent: 8\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     bundle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mget_secret(\n\u001b[1;32m     73\u001b[0m         vault_base_url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vault_url,\n\u001b[1;32m     74\u001b[0m         secret_name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m     75\u001b[0m         secret_version\u001b[39m=\u001b[39;49mversion \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     76\u001b[0m         error_map\u001b[39m=\u001b[39;49m_error_map,\n\u001b[1;32m     77\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     78\u001b[0m     )\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m KeyVaultSecret\u001b[39m.\u001b[39m_from_secret_bundle(bundle)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/keyvault/secrets/_generated/_operations_mixin.py:1574\u001b[0m, in \u001b[0;36mKeyVaultClientOperationsMixin.get_secret\u001b[0;34m(self, vault_base_url, secret_name, secret_version, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m mixin_instance\u001b[39m.\u001b[39m_serialize\u001b[39m.\u001b[39mclient_side_validation \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m mixin_instance\u001b[39m.\u001b[39m_deserialize \u001b[39m=\u001b[39m Deserializer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_models_dict(api_version))\n\u001b[0;32m-> 1574\u001b[0m \u001b[39mreturn\u001b[39;00m mixin_instance\u001b[39m.\u001b[39;49mget_secret(vault_base_url, secret_name, secret_version, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/tracing/decorator.py:83\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     85\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/keyvault/secrets/_generated/v7_3/operations/_key_vault_client_operations.py:694\u001b[0m, in \u001b[0;36mKeyVaultClientOperationsMixin.get_secret\u001b[0;34m(self, vault_base_url, secret_name, secret_version, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m path_format_arguments \u001b[39m=\u001b[39m {\n\u001b[1;32m    690\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvaultBaseUrl\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_serialize\u001b[39m.\u001b[39murl(\u001b[39m\"\u001b[39m\u001b[39mvault_base_url\u001b[39m\u001b[39m\"\u001b[39m, vault_base_url, \u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m, skip_quote\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m    691\u001b[0m }\n\u001b[1;32m    692\u001b[0m request\u001b[39m.\u001b[39murl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mformat_url(request\u001b[39m.\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpath_format_arguments)\n\u001b[0;32m--> 694\u001b[0m pipeline_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_pipeline\u001b[39m.\u001b[39;49mrun(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    695\u001b[0m     request,\n\u001b[1;32m    696\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    697\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    699\u001b[0m response \u001b[39m=\u001b[39m pipeline_response\u001b[39m.\u001b[39mhttp_response\n\u001b[1;32m    701\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m200\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/_base.py:211\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m pipeline_request \u001b[39m=\u001b[39m PipelineRequest(\n\u001b[1;32m    204\u001b[0m     request, context\n\u001b[1;32m    205\u001b[0m )  \u001b[39m# type: PipelineRequest[HTTPRequestType]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m first_node \u001b[39m=\u001b[39m (\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies[\u001b[39m0\u001b[39m]\n\u001b[1;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl_policies\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m _TransportRunner(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport)\n\u001b[1;32m    210\u001b[0m )\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m first_node\u001b[39m.\u001b[39;49msend(pipeline_request)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/_base.py:71\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     69\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/_base.py:71\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     69\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "    \u001b[0;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 71 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/_base.py:71\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     69\u001b[0m _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_request, request)\n\u001b[1;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     _await_result(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy\u001b[39m.\u001b[39mon_exception, request)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/policies/_redirect.py:158\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    156\u001b[0m redirect_settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure_redirects(request\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39moptions)\n\u001b[1;32m    157\u001b[0m \u001b[39mwhile\u001b[39;00m retryable:\n\u001b[0;32m--> 158\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    159\u001b[0m     redirect_location \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_redirect_location(response)\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m redirect_location \u001b[39mand\u001b[39;00m redirect_settings[\u001b[39m'\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/policies/_retry.py:445\u001b[0m, in \u001b[0;36mRetryPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    443\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[0;32m--> 445\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_retry(retry_settings, response):\n\u001b[1;32m    447\u001b[0m     retry_active \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mincrement(retry_settings, response\u001b[39m=\u001b[39mresponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/core/pipeline/policies/_authentication.py:117\u001b[0m, in \u001b[0;36mBearerTokenCredentialPolicy.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[1;32m    111\u001b[0m     \u001b[39m# type: (PipelineRequest) -> PipelineResponse\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     \u001b[39m\"\"\"Authorize request with a bearer token and send it to the next policy\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[39m    :param request: The pipeline request object\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m    :type request: ~azure.core.pipeline.PipelineRequest\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_request(request)\n\u001b[1;32m    118\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext\u001b[39m.\u001b[39msend(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/keyvault/secrets/_shared/challenge_auth_policy.py:78\u001b[0m, in \u001b[0;36mChallengeAuthPolicy.on_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_new_token:\n\u001b[1;32m     76\u001b[0m     \u001b[39m# azure-identity credentials require an AADv2 scope but the challenge may specify an AADv1 resource\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     scope \u001b[39m=\u001b[39m challenge\u001b[39m.\u001b[39mget_scope() \u001b[39mor\u001b[39;00m challenge\u001b[39m.\u001b[39mget_resource() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/.default\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_credential\u001b[39m.\u001b[39;49mget_token(scope, tenant_id\u001b[39m=\u001b[39;49mchallenge\u001b[39m.\u001b[39;49mtenant_id)\n\u001b[1;32m     80\u001b[0m \u001b[39m# ignore mypy's warning -- although self._token is Optional, get_token raises when it fails to get a token\u001b[39;00m\n\u001b[1;32m     81\u001b[0m request\u001b[39m.\u001b[39mhttp_request\u001b[39m.\u001b[39mheaders[\u001b[39m\"\u001b[39m\u001b[39mAuthorization\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBearer \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token\u001b[39m.\u001b[39mtoken)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/identity/_internal/decorators.py:32\u001b[0m, in \u001b[0;36mlog_get_token.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         token \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     33\u001b[0m         _LOGGER\u001b[39m.\u001b[39mlog(\n\u001b[1;32m     34\u001b[0m             logging\u001b[39m.\u001b[39mDEBUG \u001b[39mif\u001b[39;00m within_credential_chain\u001b[39m.\u001b[39mget() \u001b[39melse\u001b[39;00m logging\u001b[39m.\u001b[39mINFO, \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m succeeded\u001b[39m\u001b[39m\"\u001b[39m, qualified_name\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m         \u001b[39mif\u001b[39;00m _LOGGER\u001b[39m.\u001b[39misEnabledFor(logging\u001b[39m.\u001b[39mDEBUG):\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/identity/_credentials/azure_cli.py:80\u001b[0m, in \u001b[0;36mAzureCliCredential.get_token\u001b[0;34m(self, *scopes, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m tenant:\n\u001b[1;32m     79\u001b[0m     command \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m --tenant \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m tenant\n\u001b[0;32m---> 80\u001b[0m output \u001b[39m=\u001b[39m _run_command(command)\n\u001b[1;32m     82\u001b[0m token \u001b[39m=\u001b[39m parse_token(output)\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token:\n",
      "File \u001b[0;32m~/miniconda3/envs/feathr/lib/python3.8/site-packages/azure/identity/_credentials/azure_cli.py:154\u001b[0m, in \u001b[0;36m_run_command\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m CredentialUnavailableError(message\u001b[39m=\u001b[39mCLI_NOT_FOUND)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39maz login\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m ex\u001b[39m.\u001b[39mstderr \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39maz account set\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m ex\u001b[39m.\u001b[39mstderr:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m CredentialUnavailableError(message\u001b[39m=\u001b[39mNOT_LOGGED_IN)\n\u001b[1;32m    156\u001b[0m \u001b[39m# return code is from the CLI -> propagate its output\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m ex\u001b[39m.\u001b[39mstderr:\n",
      "\u001b[0;31mCredentialUnavailableError\u001b[0m: Please run 'az login' to set up an account"
     ]
    }
   ],
   "source": [
    "# Redis password\n",
    "if 'REDIS_PASSWORD' not in os.environ:\n",
    "    from azure.keyvault.secrets import SecretClient\n",
    "    vault_url = f\"https://{RESOURCE_PREFIX}kv.vault.azure.net\"\n",
    "    secret_client = SecretClient(vault_url=vault_url, credential=credential)\n",
    "    retrieved_secret = secret_client.get_secret('FEATHR-ONLINE-STORE-CONN').value\n",
    "    os.environ['REDIS_PASSWORD'] = retrieved_secret.split(\",\")[1].split(\"password=\", 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50b2f73e-6380-42c3-91e8-4f3e15bc10d6",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if FEATHR_CONFIG_PATH:\n",
    "    config_path = FEATHR_CONFIG_PATH\n",
    "else:\n",
    "    config_path = generate_config(\n",
    "        resource_prefix=RESOURCE_PREFIX,\n",
    "        project_name=PROJECT_NAME,\n",
    "        spark_config__spark_cluster=SPARK_CLUSTER,\n",
    "        spark_config__azure_synapse__dev_url=AZURE_SYNAPSE_URL,\n",
    "        spark_config__azure_synapse__pool_name=AZURE_SYNAPSE_SPARK_POOL,\n",
    "        spark_config__databricks__workspace_instance_url=SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL,\n",
    "        databricks_cluster_id=DATABRICKS_CLUSTER_ID,\n",
    "    )\n",
    "\n",
    "with open(config_path, 'r') as f: \n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eab0957c-c906-4297-a729-8dd8d79cb629",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initialize Feathr client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3734eee3-12f9-44db-a440-ad375ef859f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = FeathrClient(config_path=config_path, credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Features\n",
    "\n",
    "### Prepare datasets\n",
    "\n",
    "We prepare the fraud detection dataset as follows:\n",
    "\n",
    "1. Download Account info data, fraud transactions data, and untagged transactions data.\n",
    "2. Tag transaction data based on the fraud transactions data.\n",
    "    1. Aggregate the Fraud table on the account level, creating a start and end datetime. \n",
    "    2. Join this data with the untagged data.\n",
    "    3. Tag the data: `is_fraud = 0` for non fraud, `1` for fraud. \n",
    "3. Upload data files to cloud so that the Feathr's target cluster can consume.\n",
    "\n",
    "To learn more about the fraud detection scenario as well as the dataset source we use and the method we tag the transactions, please see [here](https://microsoft.github.io/r-server-fraud-detection/data-scientist.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dbfs if the notebook is running on Databricks\n",
    "if is_databricks():\n",
    "    WORKING_DIR = f\"/dbfs/{PROJECT_NAME}\"\n",
    "else:\n",
    "    WORKING_DIR = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "account_info_file_path = f\"{WORKING_DIR}/account_info.csv\"\n",
    "fraud_transactions_file_path = f\"{WORKING_DIR}/fraud_transactions.csv\"\n",
    "obs_transactions_file_path = f\"{WORKING_DIR}/obs_transactions.csv\"\n",
    "maybe_download(\n",
    "    src_url=FRAUD_DETECTION_ACCOUNT_INFO_URL,\n",
    "    dst_filepath=account_info_file_path,\n",
    ")\n",
    "maybe_download(\n",
    "    src_url=FRAUD_DETECTION_FRAUD_TRANSACTIONS_URL,\n",
    "    dst_filepath=fraud_transactions_file_path,\n",
    ")\n",
    "maybe_download(\n",
    "    src_url=FRAUD_DETECTION_UNTAGGED_TRANSACTIONS_URL,\n",
    "    dst_filepath=obs_transactions_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fraud_df = pd.read_csv(fraud_transactions_file_path)\n",
    "obs_df = pd.read_csv(obs_transactions_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine transactionDate and transactionTime into one column. E.g. \"20130903\", \"013641\" -> \"20130903 013641\"\n",
    "fraud_df[\"timestamp\"] = fraud_df[\"transactionDate\"].astype(str) + \" \" + fraud_df[\"transactionTime\"].astype(str).str.zfill(6)\n",
    "obs_df[\"timestamp\"] = obs_df[\"transactionDate\"].astype(str) + \" \" + obs_df[\"transactionTime\"].astype(str).str.zfill(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, labels are added to the transactional data by referencing the transaction-level fraud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each user in the fraud transaction data, get the timestamp range that the fraud transactions were happened. \n",
    "fraud_labels_df = fraud_df.groupby(\"accountID\").agg({\"timestamp\": ['min', 'max']})\n",
    "fraud_labels_df.columns = [\"_\".join(col) for col in fraud_labels_df.columns.values]\n",
    "fraud_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud and untagged transaction data to generate the tagged transaction data.\n",
    "transactions_df = pd.concat([fraud_df, obs_df], ignore_index=True)\n",
    "transactions_df = transactions_df.merge(\n",
    "    fraud_labels_df,\n",
    "    on=\"accountID\",\n",
    "    how=\"outer\",\n",
    ")\n",
    "# is_fraud = 0 if the transaction is not fraud. Otherwise (if it is a fraud), is_fraud = 1.\n",
    "transactions_df[\"is_fraud\"] = np.logical_and(\n",
    "    transactions_df[\"timestamp_min\"] <= transactions_df[\"timestamp\"],\n",
    "    transactions_df[\"timestamp\"] <= transactions_df[\"timestamp_max\"],\n",
    ").astype(int)\n",
    "\n",
    "transactions_df.drop_duplicates(inplace=True)\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tagged transaction data into file\n",
    "transactions_file_path = f\"{WORKING_DIR}/transactions.csv\"\n",
    "transactions_df.to_csv(transactions_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to cloud if needed\n",
    "if client.spark_runtime == \"local\":\n",
    "    # In local mode, we can use the same data path as the source.\n",
    "    # If the notebook is running on databricks, DATA_FILE_PATH should be already a dbfs path.\n",
    "    account_info_source_path = account_info_file_path\n",
    "    transactions_source_path = transactions_file_path\n",
    "elif client.spark_runtime == \"databricks\" and is_databricks():\n",
    "    # If the notebook is running on databricks, we can use the same data path as the source.\n",
    "    account_info_source_path = account_info_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
    "    transactions_source_path = transactions_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
    "else:\n",
    "    # Otherwise, upload the local file to the cloud storage (either dbfs or adls).\n",
    "    account_info_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(account_info_file_path)\n",
    "    transactions_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(transactions_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6adbca1-5642-4ac1-bff7-e7c9d4d9e5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we define following features:\n",
    "- Account features: Account-level features that will be joined to observation data on accountID\n",
    "- Transaction features: The features that will be joined to observation data on transactionID\n",
    "- Transaction aggregated features: The features aggregated by accountID\n",
    "- Derived features: The features derived from other features\n",
    "\n",
    "Some important concepts include `HdfsSource`, `TypedKey`, `Feature`, `FeatureAnchor`, and `DerivedFeature`. Please refer to feathr [documents](https://feathr.readthedocs.io/en/latest/feathr.html) to learn more about the details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b073b509-0f95-4e23-b16b-ffd8190fb6a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define account features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check account data\n",
    "pd.read_csv(account_info_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_preprocessing(df):\n",
    "    \"\"\"Drop rows with missing values in the account info dataset.\"\"\"\n",
    "    return df.select(\n",
    "        \"accountID\",\n",
    "        \"accountCountry\",\n",
    "        \"isUserRegistered\",\n",
    "        \"numPaymentRejects1dPerUser\",\n",
    "        \"accountAge\",\n",
    "    ).dropna()\n",
    "\n",
    "\n",
    "account_info_source = HdfsSource(\n",
    "    name=\"account_data\",\n",
    "    path=account_info_source_path,\n",
    "    preprocessing=account_preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3668eeb-e4a0-4327-baf6-5521c856f51d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Account features will be joined to observation data on accountID\n",
    "account_id = TypedKey(\n",
    "    key_column=\"accountID\",\n",
    "    key_column_type=ValueType.STRING,\n",
    "    description=\"account id\",\n",
    ")\n",
    "                \n",
    "account_features = [\n",
    "    Feature(\n",
    "        name=\"account_country\",\n",
    "        key=account_id,\n",
    "        feature_type=STRING, \n",
    "        transform=\"accountCountry\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"is_user_registered\",\n",
    "        key=account_id,\n",
    "        feature_type=BOOLEAN,\n",
    "        transform=\"isUserRegistered==TRUE\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"num_payment_rejects_1d_per_user\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=\"numPaymentRejects1dPerUser\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"account_age\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=\"accountAge\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "account_anchor = FeatureAnchor(\n",
    "    name=\"account_features\",\n",
    "    source=account_info_source,\n",
    "    features=account_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f12c07e-4faf-4411-8acd-6f5d13b962f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define transaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check transaction data\n",
    "pd.read_csv(transactions_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_preprocessing(df):\n",
    "    \"\"\"Preprocess the transaction data.\"\"\"\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "    # Drop rows with missing values.\n",
    "    df = df.dropna(subset=[\n",
    "        \"accountID\",\n",
    "        \"transactionID\",\n",
    "        \"transactionCurrencyCode\",\n",
    "        \"transactionAmount\",\n",
    "        \"transactionTime\",\n",
    "        \"ipCountryCode\",\n",
    "        \"timestamp\",\n",
    "    ])\n",
    "\n",
    "    # Convert transactionTime column from \"HHmmss\" to \"HH\" since we only interested in the hour.\n",
    "    return df.withColumn(\n",
    "        \"transactionTime\",\n",
    "        F.lpad(F.col(\"transactionTime\").cast(StringType()), 6, \"0\").substr(1, 2).cast(FloatType()),\n",
    "    )\n",
    "\n",
    "\n",
    "transactions_source = HdfsSource(\n",
    "    name=\"transaction_data\",\n",
    "    path=transactions_source_path,\n",
    "    event_timestamp_column=\"timestamp\",\n",
    "    timestamp_format=\"yyyyMMdd HHmmss\",\n",
    "    preprocessing=transaction_preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "280062b9-ae21-4a1a-ae94-86a5c17fd589",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transaction features will be joined to observation data on transactionID\n",
    "transaction_id = TypedKey(\n",
    "    key_column=\"transactionID\",\n",
    "    key_column_type=ValueType.STRING,\n",
    "    description=\"transaction id\",\n",
    ")\n",
    "\n",
    "transaction_amount = Feature(\n",
    "    name=\"transaction_amount\",\n",
    "    key=transaction_id,\n",
    "    feature_type=FLOAT,\n",
    "    transform=\"transactionAmount\",\n",
    ")\n",
    "\n",
    "transaction_features = [\n",
    "    transaction_amount,\n",
    "    Feature(\n",
    "        name=\"transaction_ip_country_code\",\n",
    "        key=transaction_id,\n",
    "        feature_type=STRING,\n",
    "        transform=\"ipCountryCode\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"transaction_currency_code\",\n",
    "        key=transaction_id,\n",
    "        feature_type=STRING,\n",
    "        transform=\"transactionCurrencyCode\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"transaction_time\",\n",
    "        key=transaction_id,\n",
    "        feature_type=FLOAT,\n",
    "        transform=\"transactionTime\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "transaction_feature_anchor = FeatureAnchor(\n",
    "    name=\"transaction_features\",\n",
    "    source=transactions_source,\n",
    "    features=transaction_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86ac05e1-26bb-4820-87ea-f547e3561181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define transaction aggregated-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c969554-f690-42f5-b70a-d962bf558b03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# average amount of transaction in that week\n",
    "avg_transaction_amount = Feature(\n",
    "    name=\"avg_transaction_amount\",\n",
    "    key=account_id,\n",
    "    feature_type=FLOAT,\n",
    "    transform=WindowAggTransformation(\n",
    "        agg_expr=\"cast_float(transactionAmount)\", agg_func=\"AVG\", window=\"7d\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "agg_features = [\n",
    "    avg_transaction_amount,\n",
    "    # number of transaction that took place in a day\n",
    "    Feature(\n",
    "        name=\"num_transaction_count_in_day\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"transactionID\", agg_func=\"COUNT\", window=\"1d\"\n",
    "        ),\n",
    "    ),\n",
    "    # number of transaction that took place in the past week\n",
    "    Feature(\n",
    "        name=\"num_transaction_count_in_week\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"transactionID\", agg_func=\"COUNT\", window=\"7d\"\n",
    "        ),\n",
    "    ),\n",
    "    # amount of transaction that took place in a day\n",
    "    Feature(\n",
    "        name=\"total_transaction_amount_in_day\",\n",
    "        key=account_id,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"cast_float(transactionAmount)\", agg_func=\"SUM\", window=\"1d\"\n",
    "        ),\n",
    "    ),\n",
    "    # average time of transaction in the past week\n",
    "    Feature(\n",
    "        name=\"avg_transaction_time_in_week\",\n",
    "        key=account_id,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"transactionTime\", agg_func=\"AVG\", window=\"7d\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "agg_anchor = FeatureAnchor(\n",
    "    name=\"transaction_agg_features\",\n",
    "    source=transactions_source,\n",
    "    features=agg_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17cc5132-461f-4d3d-b517-1f7e69d23252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ac10ce4-e222-469c-bb2e-1658b45e3eda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_features = [\n",
    "    DerivedFeature(\n",
    "        name=\"feature_diff_current_and_avg_amount\",\n",
    "        key=[transaction_id, account_id],\n",
    "        feature_type=FLOAT,\n",
    "        input_features=[transaction_amount, avg_transaction_amount],\n",
    "        transform=\"transaction_amount - avg_transaction_amount\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9ec8416-9ac6-4499-b60f-55822265b893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Build Features and Extract Offline Features\n",
    "\n",
    "Now, let's build the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9d32d4f-2b60-4978-bb87-c7d2160e98eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[\n",
    "        account_anchor,\n",
    "        transaction_feature_anchor,\n",
    "        agg_anchor,\n",
    "    ],\n",
    "    derived_feature_list=derived_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_feature_names = [feat.name for feat in account_features] + [feat.name for feat in agg_features]\n",
    "transactions_feature_names = [feat.name for feat in transaction_features]\n",
    "derived_feature_names = [feat.name for feat in derived_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the offline feature values from the features that have different keys, we use multiple `FeatureQuery` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6340f2f-79dc-442b-a202-b2f2078a62ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_feature_query = FeatureQuery(\n",
    "    feature_list=account_feature_names,\n",
    "    key=account_id,\n",
    ")\n",
    "\n",
    "transactions_feature_query = FeatureQuery(\n",
    "    feature_list=transactions_feature_names,\n",
    "    key=transaction_id,\n",
    ")\n",
    "\n",
    "derived_feature_query = FeatureQuery(\n",
    "    feature_list=derived_feature_names,\n",
    "    key=[transaction_id, account_id],\n",
    ")\n",
    "                   \n",
    "settings = ObservationSettings(\n",
    "    observation_path=transactions_source_path,\n",
    "    event_timestamp_column=\"timestamp\",\n",
    "    timestamp_format=\"yyyyMMdd HHmmss\",\n",
    ")\n",
    "    \n",
    "client.get_offline_features(\n",
    "    observation_settings=settings,\n",
    "    feature_query=[account_feature_query, transactions_feature_query, derived_feature_query],\n",
    "    output_path=transactions_source_path.rpartition(\"/\")[0] + f\"/fraud_transactions_features.avro\",\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_result_df(client)[\n",
    "    account_feature_names\n",
    "    + transactions_feature_names\n",
    "    + derived_feature_names\n",
    "    + [\"is_fraud\"]\n",
    "]\n",
    "df.dropna(inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Fraud Detection Model\n",
    "\n",
    "We use [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to build a fraud detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only sub-samples for simplicity\n",
    "NUM_SAMPLES_TO_PLOT = 5000\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df.sample(n=NUM_SAMPLES_TO_PLOT, random_state=42),\n",
    "    dimensions=account_feature_names + transactions_feature_names + derived_feature_names,\n",
    "    color=\"is_fraud\",\n",
    "    title=\"Scatter matrix of transaction dataset\",\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False, marker_size=3)\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    font_size=5,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_fraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"is_fraud\"].astype(int).to_numpy()\n",
    "X_df = df.drop(\"is_fraud\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert categorical features into integer values by using One-hot-encoding\n",
    "categorical_feature_names = [\"account_country\", \"transaction_ip_country_code\", \"transaction_currency_code\"]\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        OneHotEncoder().fit_transform(X_df[categorical_feature_names]).toarray(),\n",
    "        X_df.drop(categorical_feature_names, axis=\"columns\").to_numpy(),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"[Number of fraud samples / total number of samples]\n",
    "training set: {y_train.sum()} / {len(y_train)} \n",
    "test set: {y_test.sum()} / {len(y_test)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(\n",
    "#     n_estimators=50,  #TODO try to decrease to speed up\n",
    "#     max_features=0.5,\n",
    "#     random_state=42,\n",
    "# ).fit(X_train, y_train)\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# clf = GradientBoostingClassifier(\n",
    "    # n_estimators=100, learning_rate=1.0,\n",
    "    # max_depth=1, random_state=0\n",
    "# ).fit(X_train, y_train)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(probability=True).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = clf.predict_proba(X_test)\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the performance, we use recall, precision and F1 score that handle imbalanced data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_predictions(\n",
    "    y_test, y_prob[:, 1], name=\"RandomForestClassifier\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"Fraud Detection Precision-Recall Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\"\"Precision: {precision},\n",
    "Recall: {recall},\n",
    "F1: {f1}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83e69f23-aa4e-4893-8907-6d5f0792c23f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Materialize Features in Redis\n",
    "\n",
    "Now, we materialize features to `RedisSink` so that we can retrieve online features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "faad23c1-d827-4674-b630-83530574c27d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACCOUNT_FEATURE_TABLE_NAME = \"fraudDetectionAccountFeatures\" \n",
    "\n",
    "backfill_time = BackfillTime(\n",
    "    start=datetime(2013, 8, 4),\n",
    "    end=datetime(2013, 8, 4),\n",
    "    step=timedelta(days=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.materialize_features(\n",
    "    MaterializationSettings(\n",
    "        ACCOUNT_FEATURE_TABLE_NAME,\n",
    "        backfill_time=backfill_time,\n",
    "        sinks=[RedisSink(table_name=ACCOUNT_FEATURE_TABLE_NAME)],\n",
    "        feature_names=account_feature_names[1:],\n",
    "    ),\n",
    "    allow_materialize_non_agg_feature=True,\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f5b191f-b1e8-49e4-b54d-ffc2f8c0a0b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "materialized_feature_values = client.get_online_features(\n",
    "    ACCOUNT_FEATURE_TABLE_NAME,\n",
    "    key=\"A1055520452832600\",\n",
    "    feature_names=account_feature_names[1:],\n",
    ")\n",
    "materialized_feature_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap results for unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRAP_RESULTS:\n",
    "    import scrapbook as sb\n",
    "    sb.glue(\"materialized_feature_values\", materialized_feature_values)\n",
    "    sb.glue(\"precision\", precision)\n",
    "    sb.glue(\"recall\", recall)\n",
    "    sb.glue(\"f1\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the output files. CAUTION: this maybe dangerous if you \"reused\" the project name.\n",
    "import shutil\n",
    "shutil.rmtree(WORKING_DIR, ignore_errors=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fraud_detection_feathr_test_2",
   "notebookOrigID": 1891349682974490,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "feathr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (default, Nov  6 2019, 21:49:08) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddb0e38f168d5afaa0b8ab4851ddd8c14364f1d087c15de6ff2ee5a559aec1f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
