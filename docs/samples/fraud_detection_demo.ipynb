{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7b19a0cd-31da-45b7-91a4-9cd561f3d3d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Feathr Fraud Detection Sample\n",
    "\n",
    "This notebook illustrates the use of Feature Store to create a model that predicts the fraud status of transactions based on the user account data and trasaction data. All the data that was used in the notebook can be found here: https://github.com/microsoft/r-server-fraud-detection.\n",
    "\n",
    "\n",
    "In the following Notebook, we \n",
    "1. Install the latest Feathr code (to include some unreleased features) \n",
    "2. Define Environment Variables & `yaml_config` Settings \n",
    "3. Create `FeathrClient` and Define `FeatureAnchor`\n",
    "4. `build_features` and `get_offline_features`\n",
    "5. Visualize features and train Fraud Detection Model\n",
    "6. `materialize_features` and `get_online_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b51153e-40dd-43d5-9d3a-501534156e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup Feathr Developer Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Prior to running the notebook, if you have not deployed all the required resources, please refer to the guide here and follow the steps to do so: https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9c63dd5-304e-4797-a230-8fb753710dbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install feathr from the latest codes in the repo. You may use `pip install feathr[notebook]` as well.\n",
    "# !pip install \"git+https://github.com/feathr-ai/feathr.git#subdirectory=feathr_project&egg=feathr[notebook]\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69222adf-1cb0-410b-b98d-e22877f358c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import feathr\n",
    "from feathr import (\n",
    "    FeathrClient,\n",
    "    STRING, BOOLEAN, FLOAT, INT32, ValueType,\n",
    "    Feature, DerivedFeature, FeatureAnchor,\n",
    "    BackfillTime, MaterializationSettings,\n",
    "    FeatureQuery, ObservationSettings,\n",
    "    RedisSink,\n",
    "    HdfsSource,\n",
    "    WindowAggTransformation,\n",
    "    TypedKey,\n",
    ")\n",
    "from feathr.datasets.constants import (\n",
    "    FRAUD_DETECTION_ACCOUNT_INFO_URL,\n",
    "    FRAUD_DETECTION_FRAUD_TRANSACTIONS_URL,\n",
    "    FRAUD_DETECTION_UNTAGGED_TRANSACTIONS_URL,\n",
    ")\n",
    "from feathr.datasets.utils import maybe_download\n",
    "from feathr.utils.config import generate_config\n",
    "from feathr.utils.job_utils import get_result_df\n",
    "from feathr.utils.platform import is_databricks\n",
    "\n",
    "\n",
    "print(f\"Feathr version: {feathr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "RESOURCE_PREFIX = \"\"  # TODO fill the value used to deploy the resources via ARM template\n",
    "PROJECT_NAME = \"fraud_detection\"\n",
    "\n",
    "# Currently support: 'azure_synapse', 'databricks', and 'local' \n",
    "SPARK_CLUSTER = \"local\"\n",
    "\n",
    "# TODO fill values to use databricks cluster:\n",
    "DATABRICKS_CLUSTER_ID = None             # Set Databricks cluster id to use an existing cluster\n",
    "if is_databricks():\n",
    "    # If this notebook is running on Databricks, its context can be used to retrieve token and instance URL\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    DATABRICKS_WORKSPACE_TOKEN_VALUE = ctx.apiToken().get()\n",
    "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = f\"https://{ctx.tags().get('browserHostName').get()}\"\n",
    "else:\n",
    "    DATABRICKS_WORKSPACE_TOKEN_VALUE = None                  # Set Databricks workspace token to use databricks\n",
    "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = None  # Set Databricks workspace url to use databricks\n",
    "\n",
    "# TODO fill values to use Azure Synapse cluster:\n",
    "AZURE_SYNAPSE_SPARK_POOL = None  # Set Azure Synapse Spark pool name\n",
    "AZURE_SYNAPSE_URL = None         # Set Azure Synapse workspace url to use Azure Synapse\n",
    "ADLS_KEY = None                  # Set Azure Data Lake Storage key to use Azure Synapse\n",
    "\n",
    "USE_CLI_AUTH = False  # Set to True to use CLI authentication\n",
    "\n",
    "# An existing Feathr config file path. If None, we'll generate a new config based on the constants in this cell.\n",
    "FEATHR_CONFIG_PATH = None\n",
    "\n",
    "# (For the notebook test pipeline) If true, use ScrapBook package to collect the results.\n",
    "SCRAP_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK_CLUSTER == \"azure_synapse\" and not os.environ.get(\"ADLS_KEY\"):\n",
    "    os.environ[\"ADLS_KEY\"] = ADLS_KEY\n",
    "elif SPARK_CLUSTER == \"databricks\" and not os.environ.get(\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"):\n",
    "    os.environ[\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"] = DATABRICKS_WORKSPACE_TOKEN_VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permission\n",
    "To run the cells below, you need additional permission: permission to your managed identity to access the keyvault, and permission to the user to access the Storage Blob. Run the following lines of command in the Cloud Shell in order to grant yourself the access.\n",
    "\n",
    "```\n",
    "userId=<email_id_of_account_requesting_access>\n",
    "resource_prefix=<resource_prefix>\n",
    "synapse_workspace_name=\"${resource_prefix}syws\"\n",
    "keyvault_name=\"${resource_prefix}kv\"\n",
    "objectId=$(az ad user show --id $userId --query id -o tsv)\n",
    "az keyvault update --name $keyvault_name --enable-rbac-authorization false\n",
    "az keyvault set-policy -n $keyvault_name --secret-permissions get list --object-id $objectId\n",
    "az role assignment create --assignee $userId --role \"Storage Blob Data Contributor\"\n",
    "az synapse role assignment create --workspace-name $synapse_workspace_name --role \"Synapse Contributor\" --assignee $userId\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8a70f27-d520-4d3c-bb8c-f364f84cb738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get an authentication credential to access Azure resources and register features\n",
    "if USE_CLI_AUTH:\n",
    "    # Use AZ CLI interactive browser authentication\n",
    "    !az login --use-device-code\n",
    "    from azure.identity import AzureCliCredential\n",
    "    credential = AzureCliCredential(additionally_allowed_tenants=['*'],)\n",
    "elif \"AZURE_TENANT_ID\" in os.environ and \"AZURE_CLIENT_ID\" in os.environ and \"AZURE_CLIENT_SECRET\" in os.environ:\n",
    "    # Use Environment variable secret\n",
    "    from azure.identity import EnvironmentCredential\n",
    "    credential = EnvironmentCredential()\n",
    "else:\n",
    "    # Try to use the default credential\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    credential = DefaultAzureCredential(\n",
    "        exclude_interactive_browser_credential=False,\n",
    "        additionally_allowed_tenants=['*'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Redis password\n",
    "if 'REDIS_PASSWORD' not in os.environ:\n",
    "    from azure.keyvault.secrets import SecretClient\n",
    "    vault_url = f\"https://{RESOURCE_PREFIX}kv.vault.azure.net\"\n",
    "    secret_client = SecretClient(vault_url=vault_url, credential=credential)\n",
    "    retrieved_secret = secret_client.get_secret('FEATHR-ONLINE-STORE-CONN').value\n",
    "    os.environ['REDIS_PASSWORD'] = retrieved_secret.split(\",\")[1].split(\"password=\", 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50b2f73e-6380-42c3-91e8-4f3e15bc10d6",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if FEATHR_CONFIG_PATH:\n",
    "    config_path = FEATHR_CONFIG_PATH\n",
    "else:\n",
    "    config_path = generate_config(\n",
    "        resource_prefix=RESOURCE_PREFIX,\n",
    "        project_name=PROJECT_NAME,\n",
    "        spark_config__spark_cluster=SPARK_CLUSTER,\n",
    "        spark_config__azure_synapse__dev_url=AZURE_SYNAPSE_URL,\n",
    "        spark_config__azure_synapse__pool_name=AZURE_SYNAPSE_SPARK_POOL,\n",
    "        spark_config__databricks__workspace_instance_url=SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL,\n",
    "        databricks_cluster_id=DATABRICKS_CLUSTER_ID,\n",
    "    )\n",
    "\n",
    "with open(config_path, 'r') as f: \n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eab0957c-c906-4297-a729-8dd8d79cb629",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initialize Feathr Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3734eee3-12f9-44db-a440-ad375ef859f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = FeathrClient(config_path=config_path, credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "1. Download Account info data, fraud transactions data, and untagged transactions data.\n",
    "2. Merge two transactions data (fraud and untagged) into one\n",
    "3. Upload data files to cloud so that the target cluster can consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dbfs if the notebook is running on Databricks\n",
    "if is_databricks():\n",
    "    WORKING_DIR = f\"/dbfs/{PROJECT_NAME}\"\n",
    "else:\n",
    "    WORKING_DIR = PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "account_info_file_path = f\"{WORKING_DIR}/account_info.csv\"\n",
    "fraud_transactions_file_path = f\"{WORKING_DIR}/fraud_transactions.csv\"\n",
    "obs_transactions_file_path = f\"{WORKING_DIR}/obs_transactions.csv\"\n",
    "maybe_download(\n",
    "    src_url=FRAUD_DETECTION_ACCOUNT_INFO_URL,\n",
    "    dst_filepath=account_info_file_path,\n",
    ")\n",
    "maybe_download(\n",
    "    src_url=FRAUD_DETECTION_FRAUD_TRANSACTIONS_URL,\n",
    "    dst_filepath=fraud_transactions_file_path,\n",
    ")\n",
    "maybe_download(\n",
    "    src_url=FRAUD_DETECTION_UNTAGGED_TRANSACTIONS_URL,\n",
    "    dst_filepath=obs_transactions_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat fraud and obs transactions\n",
    "fraud_df = pd.read_csv(fraud_transactions_file_path)\n",
    "fraud_df[\"fraud_tag\"] = \"Fraud\"\n",
    "obs_df = pd.read_csv(obs_transactions_file_path)\n",
    "obs_df[\"fraud_tag\"] = \"Unknown\"\n",
    "\n",
    "transactions_file_path = f\"{WORKING_DIR}/transactions.csv\"\n",
    "transactions_df = pd.concat([fraud_df, obs_df], ignore_index=True)\n",
    "transactions_df.to_csv(transactions_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to cloud if needed\n",
    "if client.spark_runtime == \"local\":\n",
    "    # In local mode, we can use the same data path as the source.\n",
    "    # If the notebook is running on databricks, DATA_FILE_PATH should be already a dbfs path.\n",
    "    account_info_source_path = account_info_file_path\n",
    "    transactions_source_path = transactions_file_path\n",
    "elif client.spark_runtime == \"databricks\" and is_databricks():\n",
    "    # If the notebook is running on databricks, we can use the same data path as the source.\n",
    "    account_info_source_path = account_info_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
    "    transactions_source_path = transactions_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
    "else:\n",
    "    # Otherwise, upload the local file to the cloud storage (either dbfs or adls).\n",
    "    account_info_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(account_info_file_path)\n",
    "    transactions_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(transactions_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6adbca1-5642-4ac1-bff7-e7c9d4d9e5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Features\n",
    "\n",
    "Now, we define following features:\n",
    "- Account features: Account-level features that will be joined to observation data on accountID\n",
    "- Transaction features: The features that will be joined to observation data on transactionID\n",
    "- Transaction aggregated features: The features aggregated by accountID\n",
    "- Derived features: The features derived from other features\n",
    "\n",
    "Some important concepts include `HdfsSource`, `TypedKey`, `Feature`, `FeatureAnchor`, and `DerivedFeature`. Please refer to feathr [documents](https://feathr.readthedocs.io/en/latest/feathr.html) to learn more about the details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b073b509-0f95-4e23-b16b-ffd8190fb6a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Account Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check account data\n",
    "pd.read_csv(account_info_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_dropna(df):\n",
    "    \"\"\"Drop rows with missing values in the account info dataset.\"\"\"\n",
    "    return df.select(\n",
    "        \"accountID\",\n",
    "        \"transactionDate\",\n",
    "        \"accountCountry\",\n",
    "        \"isUserRegistered\",\n",
    "        \"numPaymentRejects1dPerUser\",\n",
    "        \"accountAge\",\n",
    "    ).dropna()\n",
    "\n",
    "\n",
    "account_info_source = HdfsSource(\n",
    "    name=\"account_data\",\n",
    "    path=account_info_source_path,\n",
    "    event_timestamp_column=\"transactionDate\",\n",
    "    timestamp_format=\"yyyyMMdd\",\n",
    "    preprocessing=account_dropna,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b3668eeb-e4a0-4327-baf6-5521c856f51d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Account features will be joined to observation data on accountID\n",
    "account_id = TypedKey(\n",
    "    key_column=\"accountID\",\n",
    "    key_column_type=ValueType.STRING,\n",
    "    description=\"account id\",\n",
    ")\n",
    "                \n",
    "account_features = [\n",
    "    Feature(\n",
    "        name=\"account_country\",\n",
    "        key=account_id,\n",
    "        feature_type=STRING, \n",
    "        transform=\"accountCountry\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"is_user_registered\",\n",
    "        key=account_id,\n",
    "        feature_type=BOOLEAN,\n",
    "        transform=\"isUserRegistered==TRUE\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"num_payment_rejects_1d_per_user\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=\"numPaymentRejects1dPerUser\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"account_age\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=\"accountAge\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "account_anchor = FeatureAnchor(\n",
    "    name=\"account_features\",\n",
    "    source=account_info_source,\n",
    "    features=account_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f12c07e-4faf-4411-8acd-6f5d13b962f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check transaction data\n",
    "pd.read_csv(transactions_file_path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_dropna(df):\n",
    "    \"\"\"Drop rows with missing values in the transactions dataset.\"\"\"\n",
    "    return df.dropna(subset=[\n",
    "        \"accountID\",\n",
    "        \"transactionDate\",\n",
    "        \"transactionID\",\n",
    "        \"transactionCurrencyCode\",\n",
    "        \"transactionAmount\",\n",
    "        \"transactionTime\",\n",
    "        \"ipCountryCode\",\n",
    "    ])\n",
    "\n",
    "\n",
    "transactions_source = HdfsSource(\n",
    "    name=\"transaction_data\",\n",
    "    path=transactions_source_path,\n",
    "    event_timestamp_column=\"transactionDate\",\n",
    "    timestamp_format=\"yyyyMMdd\",\n",
    "    preprocessing=transaction_dropna,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "280062b9-ae21-4a1a-ae94-86a5c17fd589",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transaction features will be joined to observation data on transactionID\n",
    "transaction_id = TypedKey(\n",
    "    key_column=\"transactionID\",\n",
    "    key_column_type=ValueType.STRING,\n",
    "    description=\"transaction id\",\n",
    ")\n",
    "\n",
    "transaction_amount = Feature(\n",
    "    name=\"transaction_amount\",\n",
    "    key=transaction_id,\n",
    "    feature_type=FLOAT,\n",
    "    transform=\"transactionAmount\",\n",
    ")\n",
    "\n",
    "transaction_features = [\n",
    "    transaction_amount,\n",
    "    Feature(\n",
    "        name=\"transaction_ip_country_code\",\n",
    "        key=transaction_id,\n",
    "        feature_type=STRING,\n",
    "        transform=\"ipCountryCode\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"transaction_currency_code\",\n",
    "        key=transaction_id,\n",
    "        feature_type=STRING,\n",
    "        transform=\"transactionCurrencyCode\",\n",
    "    ),\n",
    "    Feature(\n",
    "        name=\"transaction_time\",\n",
    "        key=transaction_id,\n",
    "        feature_type=INT32,\n",
    "        transform=\"transactionTime\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "transaction_feature_anchor = FeatureAnchor(\n",
    "    name=\"transaction_features\",\n",
    "    source=transactions_source,\n",
    "    features=transaction_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86ac05e1-26bb-4820-87ea-f547e3561181",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transaction Aggregated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c969554-f690-42f5-b70a-d962bf558b03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# average amount of transaction in that week\n",
    "avg_transaction_amount = Feature(\n",
    "    name=\"avg_transaction_amount\",\n",
    "    key=account_id,\n",
    "    feature_type=FLOAT,\n",
    "    transform=WindowAggTransformation(\n",
    "        agg_expr=\"cast_float(transactionAmount)\", agg_func=\"AVG\", window=\"7d\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "agg_features = [\n",
    "    avg_transaction_amount,\n",
    "    # number of transaction that took place in a day\n",
    "    Feature(\n",
    "        name=\"num_transaction_count_in_day\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"transactionID\", agg_func=\"COUNT\", window=\"1d\"\n",
    "        ),\n",
    "    ),\n",
    "    # number of transaction that took place in the past week\n",
    "    Feature(\n",
    "        name=\"num_transaction_count_in_week\",\n",
    "        key=account_id,\n",
    "        feature_type=INT32,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"transactionID\", agg_func=\"COUNT\", window=\"7d\"\n",
    "        ),\n",
    "    ),\n",
    "    # amount of transaction that took place in a day\n",
    "    Feature(\n",
    "        name=\"total_transaction_amount_in_day\",\n",
    "        key=account_id,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"cast_float(transactionAmount)\", agg_func=\"SUM\", window=\"1d\"\n",
    "        ),\n",
    "    ),\n",
    "    # average time of transaction in the past week\n",
    "    Feature(\n",
    "        name=\"avg_transaction_time\",\n",
    "        key=account_id,\n",
    "        feature_type=FLOAT,\n",
    "        transform=WindowAggTransformation(\n",
    "            agg_expr=\"cast_float(transactionTime)\", agg_func=\"AVG\", window=\"7d\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "agg_anchor = FeatureAnchor(\n",
    "    name=\"transaction_agg_features\",\n",
    "    source=transactions_source,\n",
    "    features=agg_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17cc5132-461f-4d3d-b517-1f7e69d23252",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ac10ce4-e222-469c-bb2e-1658b45e3eda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_features = [\n",
    "    DerivedFeature(\n",
    "        name=\"feature_diff_current_and_avg_amount\",\n",
    "        key=[transaction_id, account_id],\n",
    "        feature_type=FLOAT,\n",
    "        input_features=[transaction_amount, avg_transaction_amount],\n",
    "        transform=\"transaction_amount - avg_transaction_amount\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9ec8416-9ac6-4499-b60f-55822265b893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build and Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9d32d4f-2b60-4978-bb87-c7d2160e98eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[\n",
    "        account_anchor,\n",
    "        transaction_feature_anchor,\n",
    "        agg_anchor,\n",
    "    ],\n",
    "    derived_feature_list=derived_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_feature_names = [feat.name for feat in account_features] + [feat.name for feat in agg_features]\n",
    "transactions_feature_names = [feat.name for feat in transaction_features]\n",
    "derived_feature_names = [feat.name for feat in derived_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6340f2f-79dc-442b-a202-b2f2078a62ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_feature_query = FeatureQuery(\n",
    "    feature_list=account_feature_names,\n",
    "    key=account_id,\n",
    ")\n",
    "\n",
    "transactions_feature_query = FeatureQuery(\n",
    "    feature_list=transactions_feature_names,\n",
    "    key=transaction_id,\n",
    ")\n",
    "\n",
    "derived_feature_query = FeatureQuery(\n",
    "    feature_list=derived_feature_names,\n",
    "    key=[transaction_id, account_id],\n",
    ")\n",
    "                   \n",
    "settings = ObservationSettings(\n",
    "    observation_path=transactions_source_path,\n",
    "    event_timestamp_column=\"transactionDate\",\n",
    "    timestamp_format=\"yyyyMMdd\",\n",
    ")\n",
    "    \n",
    "client.get_offline_features(\n",
    "    observation_settings=settings,\n",
    "    feature_query=[account_feature_query, transactions_feature_query, derived_feature_query],\n",
    "    output_path=transactions_source_path.rpartition(\"/\")[0] + f\"/fraud_transactions_features.avro\",\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_result_df(client)[\n",
    "    account_feature_names\n",
    "    + transactions_feature_names\n",
    "    + derived_feature_names\n",
    "    + [\"accountID\", \"transactionID\", \"fraud_tag\"]\n",
    "]\n",
    "\n",
    "# Data cleaning: Remove the records if the account does not exist in the account info dataset\n",
    "df.dropna(subset=[\"is_user_registered\"], inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "NUM_SAMPLES_TO_PLOT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(\n",
    "    df[:NUM_SAMPLES_TO_PLOT],\n",
    "    dimensions=account_feature_names + transactions_feature_names + derived_feature_names,\n",
    "    color=\"fraud_tag\",\n",
    "    title=\"Scatter matrix of transaction dataset\",\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False, marker_size=3)\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    font_size=5,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Fraud Detection Model\n",
    "\n",
    "In this notebook, we train one-class Support Vector Machine (SVM) to score the transactions, where the score results can be used to determine if the transactions are fraud or not.\n",
    "\n",
    "### Feature Preprocessing\n",
    "\n",
    "Before we input the features to the model, we convert categorical features into neumeric vectors (using a simple one-hot encoding technique) and do standard scaling all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split fraud (train) and observation (test) datasets\n",
    "fraud_df = df[df[\"fraud_tag\"]==\"Fraud\"].drop([\"accountID\", \"transactionID\", \"fraud_tag\"], axis=\"columns\") \n",
    "obs_df = df[df[\"fraud_tag\"]==\"Unknown\"].drop([\"accountID\", \"transactionID\", \"fraud_tag\"], axis=\"columns\") \n",
    "print(f\"Num fraud samples = {len(fraud_df)}\", f\"Num untagged samples = {len(obs_df)}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names to encode\n",
    "enc_feature_names = [\"account_country\", \"is_user_registered\", \"transaction_ip_country_code\", \"transaction_currency_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and fit one-hot encoder\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\").fit(fraud_df[enc_feature_names])\n",
    "\n",
    "fraud_features = np.concatenate(\n",
    "    (\n",
    "        # Encoded features\n",
    "        enc.transform(fraud_df[enc_feature_names]).toarray(),\n",
    "        # Other features that don't need to be encoded\n",
    "        fraud_df.drop(enc_feature_names, axis=\"columns\").fillna(0).to_numpy(),\n",
    "        \n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Define and fit standard scaler\n",
    "scaler = StandardScaler().fit(fraud_features)\n",
    "\n",
    "fraud_features = scaler.transform(fraud_features)\n",
    "print(f\"A sample of fraud feature:\\n{fraud_features[0]}\\nData shape = {fraud_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_features = np.concatenate(\n",
    "    (\n",
    "        # Encoded features\n",
    "        enc.transform(obs_df[enc_feature_names]).toarray(),\n",
    "        # Other features that don't need to be encoded\n",
    "        obs_df.drop(enc_feature_names, axis=\"columns\").fillna(0).to_numpy(),\n",
    "        \n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "obs_features = scaler.transform(obs_features)\n",
    "print(f\"A sample of observation feature:\\n{obs_features[0]}\\nData shape = {obs_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7fff1ac7-90d1-469b-a54c-397904417796",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "clf = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1).fit(fraud_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_feature_scores = clf.score_samples(fraud_features)\n",
    "fraud_feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_feature_scores = clf.score_samples(obs_features)\n",
    "obs_feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We subsample observation transaction data for visualization\n",
    "obs_sample_idx = np.random.choice(len(obs_features), size=NUM_SAMPLES_TO_PLOT, replace=False)\n",
    "obs_sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([fraud_feature_scores, obs_feature_scores[obs_sample_idx]], axis=0)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate([fraud_features, obs_features[obs_sample_idx]], axis=0)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=20, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=tsne_results[:, 0], y=tsne_results[:, 1], color=scores)\n",
    "fig.update_traces(marker_size=5, marker_opacity=0.5)\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "83e69f23-aa4e-4893-8907-6d5f0792c23f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Materialize Features in Redis\n",
    "\n",
    "Now, we materialize features to `RedisSink` so that we can retrieve online features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "faad23c1-d827-4674-b630-83530574c27d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACCOUNT_FEATURE_TABLE_NAME = \"fraudDetectionAccountFeatures\" \n",
    "\n",
    "backfill_time = BackfillTime(\n",
    "    start=datetime(2013, 8, 4),\n",
    "    end=datetime(2013, 8, 4),\n",
    "    step=timedelta(days=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.materialize_features(\n",
    "    MaterializationSettings(\n",
    "        ACCOUNT_FEATURE_TABLE_NAME,\n",
    "        backfill_time=backfill_time,\n",
    "        sinks=[RedisSink(table_name=ACCOUNT_FEATURE_TABLE_NAME)],\n",
    "        feature_names=account_feature_names[1:],\n",
    "    ),\n",
    "    allow_materialize_non_agg_feature=True,\n",
    ")\n",
    "\n",
    "client.wait_job_to_finish(timeout_sec=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f5b191f-b1e8-49e4-b54d-ffc2f8c0a0b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "materialized_feature_values = client.get_online_features(\n",
    "    ACCOUNT_FEATURE_TABLE_NAME,\n",
    "    key=\"A1055520452832600\",\n",
    "    feature_names=account_feature_names[1:],\n",
    ")\n",
    "materialized_feature_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap results for unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRAP_RESULTS:\n",
    "    import scrapbook as sb\n",
    "    sb.glue(\"materialized_feature_values\", materialized_feature_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the output files. CAUTION: this maybe dangerous if you \"reused\" the project name.\n",
    "import shutil\n",
    "shutil.rmtree(WORKING_DIR, ignore_errors=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fraud_detection_feathr_test_2",
   "notebookOrigID": 1891349682974490,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "feathr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddb0e38f168d5afaa0b8ab4851ddd8c14364f1d087c15de6ff2ee5a559aec1f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
