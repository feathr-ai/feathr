{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "e5545a38-44a7-4aca-be6d-a66c51c75ec8",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "# Feathr Feature Store on Azure Demo Notebook\n",
            "\n",
            "This notebook illustrates the use of Feathr Feature Store to create a model that predict users' rating for different products for a e-commerce website.\n",
            "\n",
            "## Model Problem Statement\n",
            "The e-commerce website has collected past user ratings for various products. The website also collected data about user and product, like user age, product category etc. Now we want to predict users' product rating for new product so that we can recommend the new product to users that give a high rating for those products.\n",
            "\n",
            "After the model is trained, given a user_id, product_id pair and features, we should be able to predict the product rating that the user will give for this product_id.\n",
            "\n",
            "(Compared with [the beginner version of product recommendation](https://github.com/feathr-ai/feathr/blob/main/docs/samples/azure_synapse/product_recommendation_demo.ipynb), this tutorial expanded the example by predicting ratings for all products.)\n",
            "\n",
            "## Feature Creation Illustration\n",
            "In this example, our observation data has compound entity key where a record is uniquely identified by user_id and product_id. So there might be 3 types of features:\n",
            "* User features that are different for different users but are the same for different products. For example, user age is different for different users but it's the same for all products(or it's product-agnostic).\n",
            "* Product features that are different for different products but are the same for different users.\n",
            "* User-to-product features that are different for different users AND different products. For example, a feature to represent if the user has bought this product before or not.\n",
            "\n",
            "We will focus on the first two in our example.\n",
            "\n",
            "The feature creation flow is as below:\n",
            "![Feature Flow](https://github.com/feathr-ai/feathr/blob/main/docs/images/product_recommendation_advanced.jpg?raw=true)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "52b7d651-19d4-44b0-a7a8-03549f49e524",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Prerequisite: Use Quick Start Template to Provision Azure Resources\n",
            "\n",
            "First step is to provision required cloud resources if you want to use Feathr. Feathr provides a python based client to interact with cloud resources.\n",
            "\n",
            "Please follow the steps [here](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html) to provision required cloud resources. Due to the complexity of the possible cloud environment, it is almost impossible to create a script that works for all the use cases. Because of this, [azure_resource_provision.sh](https://github.com/feathr-ai/feathr/blob/main/docs/how-to-guides/azure_resource_provision.sh) is a full end to end command line to create all the required resources, and you can tailor the script as needed, while [the companion documentation](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-cli.html) can be used as a complete guide for using that shell script. \n",
            "\n",
            "\n",
            "![Architecture](https://github.com/feathr-ai/feathr/blob/main/docs/images/architecture.png?raw=true)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "1ec709d2-62ef-48c7-b915-9790afdac589",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Prerequisite: Install Feathr pip package and notebook dependencies\n",
            "\n",
            "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. Run the code below to install Feathr"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Install feathr from the latest codes in the repo. You may use `pip install feathr[notebook]` as well.\n",
            "# !pip install \"git+https://github.com/feathr-ai/feathr.git#subdirectory=feathr_project&egg=feathr[notebook]\"  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "0f3135eb-15c5-4f46-90ff-881a21cc59df",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "import glob\n",
            "import os\n",
            "import tempfile\n",
            "from datetime import datetime, timedelta\n",
            "from math import sqrt\n",
            "\n",
            "import pandas as pd\n",
            "from pyspark.sql import DataFrame\n",
            "\n",
            "\n",
            "import feathr\n",
            "from feathr import (\n",
            "    FeathrClient,\n",
            "    BOOLEAN, FLOAT, INT32, ValueType,\n",
            "    Feature, DerivedFeature, FeatureAnchor,\n",
            "    BackfillTime, MaterializationSettings,\n",
            "    FeatureQuery, ObservationSettings,\n",
            "    RedisSink,\n",
            "    INPUT_CONTEXT, HdfsSource,\n",
            "    WindowAggTransformation,\n",
            "    TypedKey,\n",
            ")\n",
            "from feathr.datasets.constants import (\n",
            "    PRODUCT_RECOMMENDATION_USER_OBSERVATION_URL,\n",
            "    PRODUCT_RECOMMENDATION_USER_PROFILE_URL,\n",
            "    PRODUCT_RECOMMENDATION_USER_PURCHASE_HISTORY_URL,\n",
            "    PRODUCT_RECOMMENDATION_PRODUCT_DETAIL_URL,\n",
            ")\n",
            "from feathr.datasets.utils import maybe_download\n",
            "from feathr.utils.config import generate_config\n",
            "from feathr.utils.job_utils import get_result_df\n",
            "from feathr.utils.platform import is_databricks\n",
            "\n",
            "\n",
            "print(f\"Feathr version: {feathr.__version__}\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "> If you meet errors like 'cannot import FeatherClient from feathr', it may be caused by incompatible version of 'aiohttp'. Please try to install/upgrade it by running: '! pip install -U aiohttp' or '! pip install aiohttp==3.8.3'"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "tags": [
               "parameters"
            ]
         },
         "outputs": [],
         "source": [
            "RESOURCE_PREFIX = \"\"  # TODO fill the value used to deploy the resources via ARM template\n",
            "PROJECT_NAME = \"product_recommendation\"\n",
            "\n",
            "# Currently support: 'azure_synapse', 'databricks', and 'local' \n",
            "SPARK_CLUSTER = \"local\"\n",
            "\n",
            "# TODO fill values to use databricks cluster:\n",
            "DATABRICKS_CLUSTER_ID = None             # Set Databricks cluster id to use an existing cluster\n",
            "if is_databricks():\n",
            "    # If this notebook is running on Databricks, its context can be used to retrieve token and instance URL\n",
            "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
            "    DATABRICKS_WORKSPACE_TOKEN_VALUE = ctx.apiToken().get()\n",
            "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = f\"https://{ctx.tags().get('browserHostName').get()}\"\n",
            "else:\n",
            "    DATABRICKS_WORKSPACE_TOKEN_VALUE = None                  # Set Databricks workspace token to use databricks\n",
            "    SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL = None  # Set Databricks workspace url to use databricks\n",
            "\n",
            "# TODO fill values to use Azure Synapse cluster:\n",
            "AZURE_SYNAPSE_SPARK_POOL = None  # Set Azure Synapse Spark pool name\n",
            "AZURE_SYNAPSE_URL = None         # Set Azure Synapse workspace url to use Azure Synapse\n",
            "ADLS_KEY = None                  # Set Azure Data Lake Storage key to use Azure Synapse\n",
            "\n",
            "# An existing Feathr config file path. If None, we'll generate a new config based on the constants in this cell.\n",
            "FEATHR_CONFIG_PATH = None\n",
            "\n",
            "USE_CLI_AUTH = False  # Set to True to use CLI authentication\n",
            "\n",
            "# If set True, register the features to Feathr registry.\n",
            "REGISTER_FEATURES = False\n",
            "\n",
            "# (For the notebook test pipeline) If true, use ScrapBook package to collect the results.\n",
            "SCRAP_RESULTS = False"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Setup necessary environment variables (Skip if using the above Quick Start Template)\n",
            "\n",
            "You should setup the environment variables in order to run this sample. More environment variables can be set by referring to [feathr_config.yaml](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It also has more explanations on the meaning of each variable.\n",
            "\n",
            "To run this notebook, for Azure users, you need REDIS_PASSWORD.\n",
            "To run this notebook, for Databricks useres, you need DATABRICKS_WORKSPACE_TOKEN_VALUE and REDIS_PASSWORD."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "if SPARK_CLUSTER == \"azure_synapse\" and not os.environ.get(\"ADLS_KEY\"):\n",
            "    os.environ[\"ADLS_KEY\"] = ADLS_KEY\n",
            "elif SPARK_CLUSTER == \"databricks\" and not os.environ.get(\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"):\n",
            "    os.environ[\"DATABRICKS_WORKSPACE_TOKEN_VALUE\"] = DATABRICKS_WORKSPACE_TOKEN_VALUE"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Permission\n",
            "\n",
            "To proceed with the following steps, you may need additional permission: permission to access the keyvault, permission to access the Storage Blob as a Contributor and permission to submit jobs to Synapse cluster. Skip this step if you have already given yourself the access. Otherwise, run the following lines of command in the Cloud Shell before running the cell below.\n",
            "\n",
            "```\n",
            "userId=<email_id_of_account_requesting_access>\n",
            "resource_prefix=<resource_prefix>\n",
            "synapse_workspace_name=\"${resource_prefix}syws\"\n",
            "keyvault_name=\"${resource_prefix}kv\"\n",
            "objectId=$(az ad user show --id $userId --query id -o tsv)\n",
            "az keyvault update --name $keyvault_name --enable-rbac-authorization false\n",
            "az keyvault set-policy -n $keyvault_name --secret-permissions get list --object-id $objectId\n",
            "az role assignment create --assignee $userId --role \"Storage Blob Data Contributor\"\n",
            "az synapse role assignment create --workspace-name $synapse_workspace_name --role \"Synapse Contributor\" --assignee $userId\n",
            "```"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Get an authentication credential to access Azure resources and register features\n",
            "if USE_CLI_AUTH:\n",
            "    # Use AZ CLI interactive browser authentication\n",
            "    !az login --use-device-code\n",
            "    from azure.identity import AzureCliCredential\n",
            "    credential = AzureCliCredential(additionally_allowed_tenants=['*'],)\n",
            "elif \"AZURE_TENANT_ID\" in os.environ and \"AZURE_CLIENT_ID\" in os.environ and \"AZURE_CLIENT_SECRET\" in os.environ:\n",
            "    # Use Environment variable secret\n",
            "    from azure.identity import EnvironmentCredential\n",
            "    credential = EnvironmentCredential()\n",
            "else:\n",
            "    # Try to use the default credential\n",
            "    from azure.identity import DefaultAzureCredential\n",
            "    credential = DefaultAzureCredential(\n",
            "        exclude_interactive_browser_credential=False,\n",
            "        additionally_allowed_tenants=['*'],\n",
            "    )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Redis password\n",
            "if 'REDIS_PASSWORD' not in os.environ:\n",
            "    from azure.keyvault.secrets import SecretClient\n",
            "    vault_url = f\"https://{RESOURCE_PREFIX}kv.vault.azure.net\"\n",
            "    secret_client = SecretClient(vault_url=vault_url, credential=credential)\n",
            "    retrieved_secret = secret_client.get_secret('FEATHR-ONLINE-STORE-CONN').value\n",
            "    os.environ['REDIS_PASSWORD'] = retrieved_secret.split(\",\")[1].split(\"password=\", 1)[1]"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "4a1f37e9-eb40-4791-9904-19e13a98f5c9",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Prerequisite: Configure the required environment (Don't need to update if using the above Quick Start Template)\n",
            "\n",
            "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. If you use Feathr CLI to create a workspace, you should have a folder with a file called `feathr_config.yaml` in it with all the required configurations. Otherwise, update the configuration below.\n",
            "\n",
            "The code below will write this configuration string to a temporary location so that Feathr client can load it. Please refer to [feathr_config.yaml](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) for more details."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "c7cd2bc7-237c-4170-a9b7-ae94f279bbba",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "if FEATHR_CONFIG_PATH:\n",
            "    config_path = FEATHR_CONFIG_PATH\n",
            "else:\n",
            "    config_path = generate_config(\n",
            "        resource_prefix=RESOURCE_PREFIX,\n",
            "        project_name=PROJECT_NAME,\n",
            "        spark_config__spark_cluster=SPARK_CLUSTER,\n",
            "        spark_config__azure_synapse__dev_url=AZURE_SYNAPSE_URL,\n",
            "        spark_config__azure_synapse__pool_name=AZURE_SYNAPSE_SPARK_POOL,\n",
            "        spark_config__databricks__workspace_instance_url=SPARK_CONFIG__DATABRICKS__WORKSPACE_INSTANCE_URL,\n",
            "        databricks_cluster_id=DATABRICKS_CLUSTER_ID,\n",
            "    )\n",
            "\n",
            "with open(config_path, 'r') as f: \n",
            "    print(f.read())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "794492ed-66b0-4787-adc6-3f234c4739a9",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "# Initialize Feathr Client"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "0c748f9d-210b-4c1d-a414-b30328d5e219",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "client = FeathrClient(config_path=config_path, credential=credential)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare Datasets\n",
            "\n",
            "1. Download datasets\n",
            "2. Upload to cloud if necessary so that the target cluster can consume as the data source"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Use dbfs if the notebook is running on Databricks\n",
            "if is_databricks():\n",
            "    WORKING_DIR = f\"/dbfs/{PROJECT_NAME}\"\n",
            "else:\n",
            "    WORKING_DIR = PROJECT_NAME"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Download datasets\n",
            "user_observation_file_path = f\"{WORKING_DIR}/user_observation.csv\"\n",
            "user_profile_file_path = f\"{WORKING_DIR}/user_profile.csv\"\n",
            "user_purchase_history_file_path = f\"{WORKING_DIR}/user_purchase_history.csv\"\n",
            "product_detail_file_path = f\"{WORKING_DIR}/product_detail.csv\"\n",
            "maybe_download(\n",
            "    src_url=PRODUCT_RECOMMENDATION_USER_OBSERVATION_URL,\n",
            "    dst_filepath=user_observation_file_path,\n",
            ")\n",
            "maybe_download(\n",
            "    src_url=PRODUCT_RECOMMENDATION_USER_PROFILE_URL,\n",
            "    dst_filepath=user_profile_file_path,\n",
            ")\n",
            "maybe_download(\n",
            "    src_url=PRODUCT_RECOMMENDATION_USER_PURCHASE_HISTORY_URL,\n",
            "    dst_filepath=user_purchase_history_file_path,\n",
            ")\n",
            "maybe_download(\n",
            "    src_url=PRODUCT_RECOMMENDATION_PRODUCT_DETAIL_URL,\n",
            "    dst_filepath=product_detail_file_path,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Upload files to cloud if needed\n",
            "if client.spark_runtime == \"local\":\n",
            "    # In local mode, we can use the same data path as the source.\n",
            "    # If the notebook is running on databricks, DATA_FILE_PATH should be already a dbfs path.\n",
            "    user_observation_source_path = user_observation_file_path\n",
            "    user_profile_source_path = user_profile_file_path\n",
            "    user_purchase_history_source_path = user_purchase_history_file_path\n",
            "    product_detail_source_path = product_detail_file_path\n",
            "elif client.spark_runtime == \"databricks\" and is_databricks():\n",
            "    # If the notebook is running on databricks, we can use the same data path as the source.\n",
            "    user_observation_source_path = user_observation_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
            "    user_profile_source_path = user_profile_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
            "    user_purchase_history_source_path = user_purchase_history_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
            "    product_detail_source_path = product_detail_file_path.replace(\"/dbfs\", \"dbfs:\")\n",
            "else:\n",
            "    # Otherwise, upload the local file to the cloud storage (either dbfs or adls).\n",
            "    user_observation_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(user_observation_file_path)\n",
            "    user_profile_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(user_profile_file_path)\n",
            "    user_purchase_history_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(user_purchase_history_file_path)\n",
            "    product_detail_source_path = client.feathr_spark_launcher.upload_or_get_cloud_path(product_detail_file_path)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "46b45998-d933-4417-b152-7db091c0d5bd",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Explore the raw source data\n",
            "We have 4 datasets to work with: one observation dataset (a.k.a. label dataset), two raw datasets to generate features for users, one raw datasets to generate features for product."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "591b1801-5783-4d88-b7b7-ff3bbcfa0a9e",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# Observation dataset(a.k.a. label dataset)\n",
            "# Observation dataset usually comes with a event_timestamp to denote when the observation happened.\n",
            "# The label here is product_rating. Our model objective is to predict a user's rating for this product.\n",
            "pd.read_csv(user_observation_file_path).head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "11b8a74f-c0e1-4556-9a97-f17f8a90a795",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# User profile dataset\n",
            "# Used to generate user features\n",
            "pd.read_csv(user_profile_file_path).head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "12f237da-a7fb-48c2-985e-a8cdfa3bb3fc",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# User purchase history dataset.\n",
            "# Used to generate user features. This is activity type data, so we need to use aggregation to genearte features.\n",
            "pd.read_csv(user_purchase_history_file_path).head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "333ef001-50c8-4556-b484-78715b657dbb",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# Product detail dataset.\n",
            "# Used to generate product features.\n",
            "pd.read_csv(product_detail_file_path).head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "bdc5a2e1-ccd4-4d61-9168-b0e4f571587b",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Defining Features with Feathr\n",
            "Let's try to create features from those raw source data.\n",
            "In Feathr, a feature is viewed as a function, mapping from entity id or key, and timestamp to a feature value. For more details on feature definition, please refer to the [Feathr Feature Definition Guide](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/feature-definition.md)\n",
            "\n",
            "\n",
            "1. The typed key (a.k.a. entity key) identifies the subject of feature, e.g. a user id, 123.\n",
            "2. The feature name is the aspect of the entity that the feature is indicating, e.g. the age of the user.\n",
            "3. The feature value is the actual value of that aspect at a particular time, e.g. the value is 30 at year 2022.\n",
            "4. The timestamp indicates when the event happened. For example, the user purchased certain product on a certain timestamp. This is usually used for point-in-time join."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "30e2c57d-6487-4d72-bd78-80d17325f1a9",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "Note: in some cases, such as features defined on top of request data, may have no entity key or timestamp.\n",
            "It is merely a function/transformation executing against request data at runtime.\n",
            "For example, the day of week of the request, which is calculated by converting the request UNIX timestamp.\n",
            "(We won't cover this in the tutorial.)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "64fc4ef8-ccde-4724-8eff-1263c08de39f",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "### Define Sources Section with UDFs\n",
            "\n",
            "#### Define Anchors and Features\n",
            "A feature is called an anchored feature when the feature is directly extracted from the source data, rather than computed on top of other features. The latter case is called derived feature.\n",
            "\n",
            "#### Feature source\n",
            "A feature source is needed for anchored features that describes the raw data in which the feature values are computed from. See the python documentation to get the details on each input column."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "c32249b5-599b-4337-bebf-c33693354685",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "def feathr_udf_preprocessing(df: DataFrame) -> DataFrame:\n",
            "    from pyspark.sql.functions import col\n",
            "\n",
            "    df = df.withColumn(\"tax_rate_decimal\", col(\"tax_rate\") / 100)\n",
            "    return df\n",
            "\n",
            "\n",
            "batch_source = HdfsSource(\n",
            "    name=\"userProfileData\",\n",
            "    path=user_profile_source_path,\n",
            "    preprocessing=feathr_udf_preprocessing,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "2961afe9-4bdc-48ba-a63f-229081f557a3",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# Let's define some features for users so our recommendation can be customized for users.\n",
            "user_id = TypedKey(\n",
            "    key_column=\"user_id\",\n",
            "    key_column_type=ValueType.INT32,\n",
            "    description=\"user id\",\n",
            "    full_name=\"product_recommendation.user_id\",\n",
            ")\n",
            "\n",
            "feature_user_age = Feature(\n",
            "    name=\"feature_user_age\",\n",
            "    key=user_id,\n",
            "    feature_type=INT32,\n",
            "    transform=\"age\",\n",
            ")\n",
            "feature_user_tax_rate = Feature(\n",
            "    name=\"feature_user_tax_rate\",\n",
            "    key=user_id,\n",
            "    feature_type=FLOAT,\n",
            "    transform=\"tax_rate_decimal\",\n",
            ")\n",
            "feature_user_gift_card_balance = Feature(\n",
            "    name=\"feature_user_gift_card_balance\",\n",
            "    key=user_id,\n",
            "    feature_type=FLOAT,\n",
            "    transform=\"gift_card_balance\",\n",
            ")\n",
            "feature_user_has_valid_credit_card = Feature(\n",
            "    name=\"feature_user_has_valid_credit_card\",\n",
            "    key=user_id,\n",
            "    feature_type=BOOLEAN,\n",
            "    transform=\"number_of_credit_cards > 0\",\n",
            ")\n",
            "\n",
            "features = [\n",
            "    feature_user_age,\n",
            "    feature_user_tax_rate,\n",
            "    feature_user_gift_card_balance,\n",
            "    feature_user_has_valid_credit_card,\n",
            "]\n",
            "\n",
            "user_feature_anchor = FeatureAnchor(\n",
            "    name=\"anchored_features\", source=batch_source, features=features\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "4da453e8-a8fd-40b8-a1e6-2a0e7cac3f6e",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# Let's define some features for the products so our recommendation can be customized for proudcts.\n",
            "product_batch_source = HdfsSource(\n",
            "    name=\"productProfileData\",\n",
            "    path=product_detail_source_path,\n",
            ")\n",
            "\n",
            "product_id = TypedKey(\n",
            "    key_column=\"product_id\",\n",
            "    key_column_type=ValueType.INT32,\n",
            "    description=\"product id\",\n",
            "    full_name=\"product_recommendation.product_id\",\n",
            ")\n",
            "\n",
            "feature_product_quantity = Feature(\n",
            "    name=\"feature_product_quantity\",\n",
            "    key=product_id,\n",
            "    feature_type=FLOAT,\n",
            "    transform=\"quantity\",\n",
            ")\n",
            "feature_product_price = Feature(\n",
            "    name=\"feature_product_price\", key=product_id, feature_type=FLOAT, transform=\"price\"\n",
            ")\n",
            "\n",
            "product_features = [feature_product_quantity, feature_product_price]\n",
            "\n",
            "product_feature_anchor = FeatureAnchor(\n",
            "    name=\"product_anchored_features\",\n",
            "    source=product_batch_source,\n",
            "    features=product_features,\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "78e240b4-dcab-499f-b6ed-72a14bfab968",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "### Window aggregation features\n",
            "\n",
            "For window aggregation features, see the supported fields below:\n",
            "\n",
            "Note that the `agg_func` should be any of these:\n",
            "\n",
            "| Aggregation Type | Input Type | Description |\n",
            "| --- | --- | --- |\n",
            "|SUM, COUNT, MAX, MIN, AVG\t|Numeric|Applies the the numerical operation on the numeric inputs. |\n",
            "|MAX_POOLING, MIN_POOLING, AVG_POOLING\t| Numeric Vector | Applies the max/min/avg operation on a per entry bassis for a given a collection of numbers.|\n",
            "|LATEST| Any |Returns the latest not-null values from within the defined time window |\n",
            "\n",
            "\n",
            "After you have defined features and sources, bring them together to build an anchor:\n",
            "\n",
            "\n",
            "Note that if the data source is from the observation data, the `source` section should be `INPUT_CONTEXT` to indicate the source of those defined anchors."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "b62a9041-73dc-45e1-add5-8fe01ebf355f",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "purchase_history_data = HdfsSource(\n",
            "    name=\"purchase_history_data\",\n",
            "    path=user_purchase_history_source_path,\n",
            "    event_timestamp_column=\"purchase_date\",\n",
            "    timestamp_format=\"yyyy-MM-dd\",\n",
            ")\n",
            "\n",
            "agg_features = [\n",
            "    Feature(\n",
            "        name=\"feature_user_avg_purchase_for_90days\",\n",
            "        key=user_id,\n",
            "        feature_type=FLOAT,\n",
            "        transform=WindowAggTransformation(\n",
            "            agg_expr=\"cast_float(purchase_amount)\", agg_func=\"AVG\", window=\"90d\"\n",
            "        ),\n",
            "    )\n",
            "]\n",
            "\n",
            "user_agg_feature_anchor = FeatureAnchor(\n",
            "    name=\"aggregationFeatures\", source=purchase_history_data, features=agg_features\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "a04373b5-8ab9-4c36-892f-6aa8129df999",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "### Derived Features Section\n",
            "Derived features are the features that are computed from other features. They could be computed from anchored features, or other derived features."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "688a4562-d8e9-468a-a900-77e750a3c903",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "feature_user_purchasing_power = DerivedFeature(\n",
            "    name=\"feature_user_purchasing_power\",\n",
            "    key=user_id,\n",
            "    feature_type=FLOAT,\n",
            "    input_features=[feature_user_gift_card_balance, feature_user_has_valid_credit_card],\n",
            "    transform=\"feature_user_gift_card_balance + if(boolean(feature_user_has_valid_credit_card), 100, 0)\",\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "f4d8f829-bfbc-4d6f-bc32-3a419a32e3d3",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "And then we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features (which is not anchored to a source)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "4c617bb8-2605-4d40-acc9-2156c86dfc56",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "client.build_features(\n",
            "    anchor_list=[user_agg_feature_anchor, user_feature_anchor, product_feature_anchor],\n",
            "    derived_feature_list=[feature_user_purchasing_power],\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "6b2877d0-2ab8-4c07-99d4-effc7336ee8a",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Create training data using point-in-time correct feature join\n",
            "\n",
            "A training dataset usually contains entity id columns, multiple feature columns, event timestamp column and label/target column. \n",
            "\n",
            "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
            "what features and how these features should be joined to the observation data. \n",
            "\n",
            "To learn more on this topic, please refer to [Point-in-time Correctness](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/point-in-time-join.md)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "30302a53-561f-4b85-ba25-8de9fc843c63",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "user_feature_query = FeatureQuery(\n",
            "    feature_list=[\n",
            "        \"feature_user_age\",\n",
            "        \"feature_user_tax_rate\",\n",
            "        \"feature_user_gift_card_balance\",\n",
            "        \"feature_user_has_valid_credit_card\",\n",
            "        \"feature_user_avg_purchase_for_90days\",\n",
            "        \"feature_user_purchasing_power\",\n",
            "    ],\n",
            "    key=user_id,\n",
            ")\n",
            "\n",
            "product_feature_query = FeatureQuery(\n",
            "    feature_list=[\"feature_product_quantity\", \"feature_product_price\"], key=product_id\n",
            ")\n",
            "\n",
            "settings = ObservationSettings(\n",
            "    observation_path=user_observation_source_path,\n",
            "    event_timestamp_column=\"event_timestamp\",\n",
            "    timestamp_format=\"yyyy-MM-dd\",\n",
            ")\n",
            "client.get_offline_features(\n",
            "    observation_settings=settings,\n",
            "    feature_query=[user_feature_query, product_feature_query],\n",
            "    output_path=user_profile_source_path.rpartition(\"/\")[0] + f\"/product_recommendation_features.avro\",\n",
            ")\n",
            "client.wait_job_to_finish(timeout_sec=5000)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "cc7b6276-70c1-494f-83ca-53d442e3198a",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Download the training dataset and show the result\n",
            "\n",
            "Let's use the helper function `get_result_df` to download the result and view it:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "120c9a21-1e1d-4ef5-8fe9-00d35a93cbf1",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "res_df = get_result_df(client)\n",
            "res_df.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "497d6a3b-94e2-4087-94b1-0a5d7baf3ab3",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Train a machine learning model\n",
            "After getting all the features, let's train a machine learning model with the converted feature by Feathr:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "9bd661ae-430e-449b-9a62-9155828de099",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "from sklearn.ensemble import GradientBoostingRegressor\n",
            "from sklearn.metrics import mean_squared_error\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "\n",
            "final_df = (\n",
            "    res_df\n",
            "    .drop([\"event_timestamp\"], axis=1, errors=\"ignore\")\n",
            "    .fillna(0)\n",
            ")\n",
            "\n",
            "X_train, X_test, y_train, y_test = train_test_split(\n",
            "    final_df.drop([\"product_rating\"], axis=1),\n",
            "    final_df[\"product_rating\"].astype(\"float64\"),\n",
            "    test_size=0.2,\n",
            "    random_state=42,\n",
            ")\n",
            "model = GradientBoostingRegressor()\n",
            "model.fit(X_train, y_train)\n",
            "\n",
            "y_pred = model.predict(X_test)\n",
            "rmse = sqrt(mean_squared_error(y_test.values.flatten(), y_pred))\n",
            "\n",
            "print(f\"Root mean squared error: {rmse}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "fda62a21-e7d6-4044-879f-bc05f77d248e",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Materialize feature value into offline/online storage\n",
            "\n",
            "While Feathr can compute the feature value from the feature definition on-the-fly at request time, it can also pre-compute\n",
            "and materialize the feature value to offline and/or online storage. \n",
            "\n",
            "We can push the generated features to the online store like below:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.read_csv(user_profile_file_path).head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "3375f18d-cb64-4f13-8789-07b9d9c5835e",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# Materialize user features\n",
            "# Note, you can only materialize features of same entity key into one table.\n",
            "redisSink = RedisSink(table_name=\"user_features\")\n",
            "settings = MaterializationSettings(\n",
            "    name=\"user_feature_setting\",\n",
            "    sinks=[redisSink],\n",
            "    feature_names=[\"feature_user_age\", \"feature_user_gift_card_balance\"],\n",
            ")\n",
            "\n",
            "client.materialize_features(settings=settings, allow_materialize_non_agg_feature=True)\n",
            "client.wait_job_to_finish(timeout_sec=5000)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "7fb61ed8-6db4-461c-bd86-a5ff268a7c3d",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "We can then get the features from the online store (Redis):"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "ed5da7df-8095-403e-91a6-c5d2104eaf68",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Fetching feature value for online inference\n",
            "\n",
            "For features that are already materialized by the previous step, their latest value can be queried via the client's\n",
            "`get_online_features` or `multi_get_online_features` API."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "9d8f3710-d2d4-463a-b452-99bd56bb3482",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "client.get_online_features(\n",
            "    \"user_features\", \"2\", [\"feature_user_age\", \"feature_user_gift_card_balance\"]\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "e8aa6e5f-5b2d-4778-bafa-5a3a45fdd3b5",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "client.multi_get_online_features(\n",
            "    \"user_features\", [\"1\", \"2\"], [\"feature_user_age\", \"feature_user_gift_card_balance\"]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "b19b73c6-7b0e-4b22-8eb1-8afdc328df74",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Materialize product features\n",
            "\n",
            "We can also materialize product features into a separate table."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "7a28cc6f-06f7-4915-9f3e-0a057467b77b",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "# Materialize product features\n",
            "backfill_time = BackfillTime(\n",
            "    start=datetime(2020, 5, 20),\n",
            "    end=datetime(2020, 5, 20),\n",
            "    step=timedelta(days=1),\n",
            ")\n",
            "\n",
            "redisSink = RedisSink(table_name=\"product_features\")\n",
            "settings = MaterializationSettings(\n",
            "    \"product_feature_setting\",\n",
            "    backfill_time=backfill_time,\n",
            "    sinks=[redisSink],\n",
            "    feature_names=[\"feature_product_price\"],\n",
            ")\n",
            "\n",
            "client.materialize_features(settings, allow_materialize_non_agg_feature=True)\n",
            "client.wait_job_to_finish(timeout_sec=1000)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "8732aad1-7b22-4efc-8e2c-722030ae8bfb",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "client.get_online_features(\"product_features\", \"2\", [\"feature_product_price\"])"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "acd29f4d-715b-4889-954d-b648ea8e2a0f",
               "showTitle": false,
               "title": ""
            }
         },
         "source": [
            "## Registering and Fetching features\n",
            "\n",
            "We can also register the features with an Apache Atlas compatible service, such as Azure Purview, and share the registered features across teams:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "application/vnd.databricks.v1+cell": {
               "inputWidgets": {},
               "nuid": "1255ed12-5030-43b6-b733-5a467874b708",
               "showTitle": false,
               "title": ""
            }
         },
         "outputs": [],
         "source": [
            "if REGISTER_FEATURES:\n",
            "    try:\n",
            "        client.register_features()\n",
            "    except KeyError:\n",
            "        # TODO temporarily go around the \"Already exists\" error\n",
            "        pass\n",
            "    print(client.list_registered_features(project_name=PROJECT_NAME))"
         ]
      }
   ],
   "metadata": {
      "application/vnd.databricks.v1+notebook": {
         "dashboards": [],
         "language": "python",
         "notebookMetadata": {
            "pythonIndentUnit": 4
         },
         "notebookName": "product_recommendation_demo_advanced",
         "notebookOrigID": 411375353096492,
         "widgets": {}
      },
      "kernelspec": {
         "display_name": "feathr",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.8"
      },
      "vscode": {
         "interpreter": {
            "hash": "ddb0e38f168d5afaa0b8ab4851ddd8c14364f1d087c15de6ff2ee5a559aec1f2"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 0
}
