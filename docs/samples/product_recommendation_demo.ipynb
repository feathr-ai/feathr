{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo Notebook: Feathr Feature Store on Azure\n",
        "\n",
        "This notebook demonstrates how Feathr Feature Store can simplify and empower your model training and inference. You will learn:\n",
        "\n",
        "1. Define sharable features using Feathr API\n",
        "2. Create a training dataset via point-in-time feature join with Feathr API\n",
        "3. Materialize features to online store and then retrieve them with Feathr API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite: Use Quick Start Template to Provision Azure Resources\n",
        "\n",
        "First step is to provision required cloud resources if you want to use Feathr. Feathr provides a python based client to interact with cloud resources.\n",
        "\n",
        "Please follow the steps [here](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-arm.html) to provision required cloud resources. Due to the complexity of the possible cloud environment, it is almost impossible to create a script that works for all the use cases. Because of this, [azure_resource_provision.sh](https://github.com/feathr-ai/feathr/blob/main/docs/how-to-guides/azure_resource_provision.sh) is a full end to end command line to create all the required resources, and you can tailor the script as needed, while [the companion documentation](https://feathr-ai.github.io/feathr/how-to-guides/azure-deployment-cli.html) can be used as a complete guide for using that shell script. \n",
        "\n",
        "\n",
        "![Architecture](https://github.com/feathr-ai/feathr/blob/main/docs/images/architecture.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite: Install Feathr and Import Dependencies\n",
        "\n",
        "Install Feathr using pip:\n",
        "\n",
        "`pip install -U feathr pandavro scikit-learn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Dependencies\n",
        "import glob\n",
        "import os\n",
        "import tempfile\n",
        "from datetime import datetime, timedelta\n",
        "from math import sqrt\n",
        "\n",
        "import pandas as pd\n",
        "import pandavro as pdx\n",
        "from feathr import FeathrClient\n",
        "from feathr import BOOLEAN, FLOAT, INT32, ValueType\n",
        "from feathr import Feature, DerivedFeature, FeatureAnchor\n",
        "from feathr import BackfillTime, MaterializationSettings\n",
        "from feathr import FeatureQuery, ObservationSettings\n",
        "from feathr import RedisSink\n",
        "from feathr import INPUT_CONTEXT, HdfsSource\n",
        "from feathr import WindowAggTransformation\n",
        "from feathr import TypedKey\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.keyvault.secrets import SecretClient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite: Configure the required environment with Feathr Quick Start Template\n",
        "\n",
        "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. Run the code below to install Feathr, login to Azure to get the required credentials to access more cloud resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**REQUIRED STEP: Fill in the resource prefix when provisioning the resources**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resource_prefix = \"feathr_resource_prefix\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install feathr azure-cli  pandavro scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Login to Azure with a device code (You will see instructions in the output):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! az login --use-device-code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Permission**\n",
        "\n",
        "To proceed with the following steps, you may need additional permission: permission to access the keyvault, permission to access the Storage Blob as a Contributor and permission to submit jobs to Synapse cluster. Skip this step if you have already given yourself the access. Otherwise, run the following lines of command in the Cloud Shell before running the cell below.\n",
        "\n",
        "```\n",
        "userId=<email_id_of_account_requesting_access>\n",
        "resource_prefix=<resource_prefix>\n",
        "synapse_workspace_name=\"${resource_prefix}syws\"\n",
        "keyvault_name=\"${resource_prefix}kv\"\n",
        "objectId=$(az ad user show --id $userId --query id -o tsv)\n",
        "az keyvault update --name $keyvault_name --enable-rbac-authorization false\n",
        "az keyvault set-policy -n $keyvault_name --secret-permissions get list --object-id $objectId\n",
        "az role assignment create --assignee $userId --role \"Storage Blob Data Contributor\"\n",
        "az synapse role assignment create --workspace-name $synapse_workspace_name --role \"Synapse Contributor\" --assignee $userId\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get all the required credentials from Azure KeyVault"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all the required credentials from Azure Key Vault\n",
        "key_vault_name=resource_prefix+\"kv\"\n",
        "synapse_workspace_url=resource_prefix+\"syws\"\n",
        "adls_account=resource_prefix+\"dls\"\n",
        "adls_fs_name=resource_prefix+\"fs\"\n",
        "purview_name=resource_prefix+\"purview\"\n",
        "key_vault_uri = f\"https://{key_vault_name}.vault.azure.net\"\n",
        "credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n",
        "client = SecretClient(vault_url=key_vault_uri, credential=credential)\n",
        "secretName = \"FEATHR-ONLINE-STORE-CONN\"\n",
        "retrieved_secret = client.get_secret(secretName).value\n",
        "\n",
        "# Get redis credentials; This is to parse Redis connection string.\n",
        "redis_port=retrieved_secret.split(',')[0].split(\":\")[1]\n",
        "redis_host=retrieved_secret.split(',')[0].split(\":\")[0]\n",
        "redis_password=retrieved_secret.split(',')[1].split(\"password=\",1)[1]\n",
        "redis_ssl=retrieved_secret.split(',')[2].split(\"ssl=\",1)[1]\n",
        "\n",
        "# Set the resource link\n",
        "os.environ['spark_config__azure_synapse__dev_url'] = f'https://{synapse_workspace_url}.dev.azuresynapse.net'\n",
        "os.environ['spark_config__azure_synapse__pool_name'] = 'spark31'\n",
        "os.environ['spark_config__azure_synapse__workspace_dir'] = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_project'\n",
        "os.environ['online_store__redis__host'] = redis_host\n",
        "os.environ['online_store__redis__port'] = redis_port\n",
        "os.environ['online_store__redis__ssl_enabled'] = redis_ssl\n",
        "os.environ['REDIS_PASSWORD']=redis_password\n",
        "feathr_output_path = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_output'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite: Configure the required environment (Skip this step if using the above Quick Start Template)\n",
        "\n",
        "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. If you use Feathr CLI to create a workspace, you should have a folder with a file called `feathr_config.yaml` in it with all the required configurations. Otherwise, update the configuration below.\n",
        "\n",
        "The code below will write this configuration string to a temporary location and load it to Feathr. Please still refer to [feathr_config.yaml](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "yaml_config = \"\"\"\n",
        "# Please refer to https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml for explanations on the meaning of each field.\n",
        "api_version: 1\n",
        "project_config:\n",
        "  project_name: 'feathr_getting_started'\n",
        "  required_environment_variables:\n",
        "    - 'REDIS_PASSWORD'\n",
        "offline_store:\n",
        "# Please set 'enabled' flags as true (false by default) if any of items under the same paths are expected to be visited\n",
        "  adls:\n",
        "    adls_enabled: true\n",
        "  wasb:\n",
        "    wasb_enabled: true\n",
        "  s3:\n",
        "    s3_enabled: false\n",
        "    s3_endpoint: 's3.amazonaws.com'\n",
        "  jdbc:\n",
        "    jdbc_enabled: false\n",
        "    jdbc_database: 'feathrtestdb'\n",
        "    jdbc_table: 'feathrtesttable'\n",
        "  snowflake:\n",
        "    snowflake_enabled: false\n",
        "    url: \"<replace_with_your_snowflake_account>.snowflakecomputing.com\"\n",
        "    user: \"<replace_with_your_user>\"\n",
        "    role: \"<replace_with_your_user_role>\"\n",
        "spark_config:\n",
        "  spark_cluster: 'azure_synapse'\n",
        "  spark_result_output_parts: '1'\n",
        "  azure_synapse:\n",
        "    dev_url: 'https://feathrazuretest3synapse.dev.azuresynapse.net'\n",
        "    pool_name: 'spark3'\n",
        "    workspace_dir: 'abfss://feathrazuretest3fs@feathrazuretest3storage.dfs.core.windows.net/feathr_getting_started'\n",
        "    executor_size: 'Small'\n",
        "    executor_num: 1\n",
        "  databricks:\n",
        "    workspace_instance_url: 'https://adb-2474129336842816.16.azuredatabricks.net'\n",
        "    config_template: {'run_name':'','new_cluster':{'spark_version':'9.1.x-scala2.12','node_type_id':'Standard_D3_v2','num_workers':2,'spark_conf':{}},'libraries':[{'jar':''}],'spark_jar_task':{'main_class_name':'','parameters':['']}}\n",
        "    work_dir: 'dbfs:/feathr_getting_started'\n",
        "online_store:\n",
        "  redis:\n",
        "    host: 'feathrazuretest3redis.redis.cache.windows.net'\n",
        "    port: 6380\n",
        "    ssl_enabled: True\n",
        "feature_registry:\n",
        "  api_endpoint: \"https://feathr-sql-registry.azurewebsites.net/api/v1\"\n",
        "\"\"\"\n",
        "tmp = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
        "with open(tmp.name, \"w\") as text_file:\n",
        "    text_file.write(yaml_config)\n",
        "feathr_output_path = f'abfss://feathrazuretest3fs@feathrazuretest3storage.dfs.core.windows.net/feathr_output'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisite: Setup necessary environment variables (Skip this step if using the above Quick Start Template)\n",
        "\n",
        "You should setup the environment variables in order to run this sample. More environment variables can be set by referring to [feathr_config.yaml](https://github.com/feathr-ai/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It also has more explanations on the meaning of each variable.\n",
        "\n",
        "To run this notebook, for Azure users, you need REDIS_PASSWORD.\n",
        "To run this notebook, for Databricks useres, you need DATABRICKS_WORKSPACE_TOKEN_VALUE and REDIS_PASSWORD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define sharable features using Feathr API\n",
        "\n",
        "In this tutorial, we use Feathr Feature Store to help create a model that predicts users product rating. To make it simple, let's just predict users' rating for ONE product for an e-commerce website. (We have an [advanced demo](./product_recommendation_demo_advanced.ipynb) that predicts ratings for arbitrary products.)\n",
        "\n",
        "\n",
        "## Initialize Feathr Client\n",
        "\n",
        "Let's initialize a Feathr client first. The Feathr client provides all the APIs we need to interact with Feathr Feature Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feathr_client = FeathrClient(config_path=tmp.name, credential=credential)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understand the Raw Datasets\n",
        "We have 3 raw datasets to work with: one observation dataset(a.k.a. label dataset) and two raw datasets to generate features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Observation dataset(a.k.a. label dataset)\n",
        "# Observation dataset usually comes with a event_timestamp to denote when the observation happened.\n",
        "# The label here is product_rating. Our model objective is to predict a user's rating for this product.\n",
        "import pandas as pd\n",
        "pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/product_recommendation_sample/user_observation_mock_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User profile dataset\n",
        "# Used to generate user features\n",
        "import pandas as pd\n",
        "pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/product_recommendation_sample/user_profile_mock_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User purchase history dataset.\n",
        "# Used to generate user features. This is activity type data, so we need to use aggregation to genearte features.\n",
        "import pandas as pd\n",
        "pd.read_csv(\"https://azurefeathrstorage.blob.core.windows.net/public/sample_data/product_recommendation_sample/user_purchase_history_mock_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " After a bit of data exploration, we want to create a training dataset like this:\n",
        "\n",
        " \n",
        "![Feature Flow](https://github.com/feathr-ai/feathr/blob/main/docs/images/product_recommendation.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What's a Feature in Feathr\n",
        "A feature is an individual measurable property or characteristic of a phenomenon which is sometimes time-sensitive. \n",
        "\n",
        "In Feathr, feature can be defined by the following characteristics:\n",
        "1. The typed key (a.k.a. entity id): identifies the subject of feature, e.g. a user id of 123, a product id of SKU234456.\n",
        "2. The feature name: the unique identifier of the feature, e.g. user_age, total_spending_in_30_days.\n",
        "3. The feature value: the actual value of that aspect at a particular time, e.g. the feature value of the person's age is 30 at year 2022.\n",
        "\n",
        "You can feel that this is defined from a feature consumer(a person who wants to use a feature) perspective. It only tells us what a feature is like. In later sections, you can see how a feature consumer can access the features in a very simple way.\n",
        "\n",
        "To define a feature as well as how it can be produced, additionally we need:\n",
        "1. Feature source: what source data that this feature is based on\n",
        "2. Transformation: what transformation is used to transform the source data into feature. Transformation can be optional when you just want to take a column out from the source data.\n",
        "\n",
        "(For more details on feature definition, please refer to the [Feathr Feature Definition Guide](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/feature-definition.md))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Sources Section with Preprocssing\n",
        "A [feature source](https://feathr.readthedocs.io/en/latest/#feathr.Source) defines where to find the source data and how to use the source data for the upcoming feature transformation. There are different types of feature sources that you can use. HdfsSource is the most commonly used one that can connect you to data lake, Snowflake database tables etc. It's simliar to database connector.\n",
        "\n",
        "To define HdfsSource, we need:\n",
        "1. `name`: It's used for you to recognize it. It has to be unique among all other feature source. Here we use `userProfileData`. \n",
        "2. `path`: It points to the location that we can find the source data.\n",
        "3. `preprocessing`(optional): If you want some preprocessing other than provided transformation, you can do it here. This preprocessing will be applied all the transformations of this source.\n",
        "4. `event_timestamp_column`(optioanl): there are `event_timestamp_column` and `timestamp_format` used for point-in-time join and we will cover them later.\n",
        "\n",
        "See [the python API documentation](https://feathr.readthedocs.io/en/latest/#feathr.HdfsSource) to get the details of each input fields. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "def feathr_udf_preprocessing(df: DataFrame) -> DataFrame:\n",
        "    from pyspark.sql.functions import col\n",
        "    df = df.withColumn(\"tax_rate_decimal\", col(\"tax_rate\")/100)\n",
        "    df.show(10)\n",
        "    return df\n",
        "\n",
        "batch_source = HdfsSource(name=\"userProfileData\",\n",
        "                          path=\"wasbs://public@azurefeathrstorage.blob.core.windows.net/sample_data/product_recommendation_sample/user_profile_mock_data.csv\",\n",
        "                          preprocessing=feathr_udf_preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Features on Top of Data Sources\n",
        "To define features on top of the `HdfsSource`, we need to:\n",
        "1. specify the key of this feature: feature are like other data, they are keyed by some id. For example, user_id, product_id. You can also define compound keys.\n",
        "2. specify the name of the feature via `name` parameter and how to transform it from source data via `transform` parameter. Also some other metadata, like `feature_type`.\n",
        "3. group them together so we know it's from one `HdfsSource` via `FeatureAnchor`. Also give it a unique name via `name` parameter so we can recognize it.\n",
        "\n",
        "It's called FeatureAnchor since it's like this group of features are anchored to the source. There are other types of features that are computed on top of other features(a.k.a. derived feature which we will cover in next section)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_id = TypedKey(key_column=\"user_id\",\n",
        "                   key_column_type=ValueType.INT32,\n",
        "                   description=\"user id\",\n",
        "                   full_name=\"product_recommendation.user_id\")\n",
        "\n",
        "feature_user_age = Feature(name=\"feature_user_age\",\n",
        "                           key=user_id,\n",
        "                           feature_type=INT32, \n",
        "                           transform=\"age\")\n",
        "feature_user_tax_rate = Feature(name=\"feature_user_tax_rate\",\n",
        "                                key=user_id,\n",
        "                                feature_type=FLOAT,\n",
        "                                transform=\"tax_rate_decimal\")\n",
        "feature_user_gift_card_balance = Feature(name=\"feature_user_gift_card_balance\",\n",
        "                                    key=user_id,\n",
        "                                    feature_type=FLOAT,\n",
        "                                    transform=\"gift_card_balance\")\n",
        "feature_user_has_valid_credit_card = Feature(name=\"feature_user_has_valid_credit_card\",\n",
        "                                    key=user_id,\n",
        "                                    feature_type=BOOLEAN,\n",
        "                                    transform=\"number_of_credit_cards > 0\")\n",
        "                                    \n",
        "features = [\n",
        "    feature_user_age,\n",
        "    feature_user_tax_rate,\n",
        "    feature_user_gift_card_balance,\n",
        "    feature_user_has_valid_credit_card\n",
        "]\n",
        "\n",
        "request_anchor = FeatureAnchor(name=\"anchored_features\",\n",
        "                               source=batch_source,\n",
        "                               features=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window aggregation features\n",
        "\n",
        "Using [window aggregations](https://en.wikipedia.org/wiki/Window_function_%28SQL%29) can help us create more powerful features. A window aggregation feature compress large amount of information into one single feature value. Using our raw data as an example, we have the users' purchase history data that might be quite some rows, we want to create a window aggregation feature that represents their last 90 days of average purcahse amount.\n",
        "\n",
        "Feathr provides a nice API to help us create such window aggregation features.\n",
        "\n",
        "To create this window aggregation feature via Feathr, we just need to define the following parameters with `WindowAggTransformation` API:\n",
        "1. `agg_expr`: the field/column you want to aggregate. It can be a ANSI SQL expression. So we just write `cast_float(purchase_amount)`(the raw data might be in string form, let's cast_float).\n",
        "2. `agg_func`: the aggregation function you want. We want to use `AVG` here.\n",
        "3. `window`: the aggregation window size you want. Let's use `90d`. You can tune your windows to create different window aggregation features.\n",
        "\n",
        "For window aggregation functions, see the supported fields below:\n",
        "\n",
        "| Aggregation Type | Input Type | Description |\n",
        "| --- | --- | --- |\n",
        "|SUM, COUNT, MAX, MIN, AVG\t|Numeric|Applies the the numerical operation on the numeric inputs. |\n",
        "|MAX_POOLING, MIN_POOLING, AVG_POOLING\t| Numeric Vector | Applies the max/min/avg operation on a per entry bassis for a given a collection of numbers.|\n",
        "|LATEST| Any |Returns the latest not-null values from within the defined time window |\n",
        "\n",
        "(Note that the `agg_func` should be any of these.)\n",
        "\n",
        "After you have defined features and sources, bring them together to build an anchor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "purchase_history_data = HdfsSource(name=\"purchase_history_data\",\n",
        "                          path=\"wasbs://public@azurefeathrstorage.blob.core.windows.net/sample_data/product_recommendation_sample/user_purchase_history_mock_data.csv\",\n",
        "                          event_timestamp_column=\"purchase_date\",\n",
        "                          timestamp_format=\"yyyy-MM-dd\")\n",
        "                          \n",
        "agg_features = [Feature(name=\"feature_user_total_purchase_in_90days\",\n",
        "                        key=user_id,\n",
        "                        feature_type=FLOAT,\n",
        "                        transform=WindowAggTransformation(agg_expr=\"cast_float(purchase_amount)\",\n",
        "                                                          agg_func=\"AVG\",\n",
        "                                                          window=\"90d\"))\n",
        "                ]\n",
        "\n",
        "agg_anchor = FeatureAnchor(name=\"aggregationFeatures\",\n",
        "                           source=purchase_history_data,\n",
        "                           features=agg_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Derived Features Section\n",
        "Derived features are features that are computed from other Feathr features. They could be computed from anchored features, or other derived features.\n",
        "\n",
        "Typical usage includes feature cross(f1 * f2), or computing cosine similarity between two features.\n",
        "\n",
        "The syntax works in a similar way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_user_purchasing_power = DerivedFeature(name=\"feature_user_purchasing_power\",\n",
        "                                      key=user_id,\n",
        "                                      feature_type=FLOAT,\n",
        "                                      input_features=[feature_user_gift_card_balance, feature_user_has_valid_credit_card],\n",
        "                                      transform=\"feature_user_gift_card_balance + if_else(toBoolean(feature_user_has_valid_credit_card), 100, 0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Features\n",
        "Lastly, we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feathr_client.build_features(anchor_list=[agg_anchor, request_anchor], \n",
        "                      derived_feature_list=[feature_user_purchasing_power])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: A Special Type of Feature: Request Feature\n",
        "For advanced user cases, in some cases, features defined on top of request data(a.k.a. observation data) may have no entity key or timestamp.\n",
        "It is merely a function/transformation executing against request data at runtime.\n",
        "For example, the day of week of the request, which is calculated by converting the request UNIX timestamp.\n",
        "In this case, the `source` section should be `INPUT_CONTEXT` to indicate the source of those defined anchors.\n",
        "\n",
        "We won't cover the details it in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create training data using point-in-time correct feature join\n",
        "\n",
        "A training dataset usually contains entity id column(s), multiple feature columns, event timestamp column and label/target column. \n",
        "\n",
        "To create a training dataset using Feathr, we need to provide a feature join settings to specify\n",
        "what features and how these features should be joined to the observation data. \n",
        "\n",
        "(To learn more on this topic, please refer to [Point-in-time Correctness](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/point-in-time-join.md))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synapse and Databricks have different output path format\n",
        "if feathr_client.spark_runtime == 'databricks':\n",
        "    output_path = 'dbfs:/feathrazure_test.avro'\n",
        "else:\n",
        "    output_path = feathr_output_path\n",
        "\n",
        "# Features that we want to request\n",
        "feature_query = FeatureQuery(feature_list=[\"feature_user_age\", \n",
        "                                           \"feature_user_tax_rate\", \n",
        "                                           \"feature_user_gift_card_balance\", \n",
        "                                           \"feature_user_has_valid_credit_card\", \n",
        "                                           \"feature_user_total_purchase_in_90days\",\n",
        "                                           \"feature_user_purchasing_power\"], \n",
        "                             key=user_id)\n",
        "settings = ObservationSettings(\n",
        "    observation_path=\"wasbs://public@azurefeathrstorage.blob.core.windows.net/sample_data/product_recommendation_sample/user_observation_mock_data.csv\",\n",
        "    event_timestamp_column=\"event_timestamp\",\n",
        "    timestamp_format=\"yyyy-MM-dd\")\n",
        "feathr_client.get_offline_features(observation_settings=settings,\n",
        "                            feature_query=feature_query,\n",
        "                            output_path=output_path)\n",
        "feathr_client.wait_job_to_finish(timeout_sec=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the result and show the result\n",
        "\n",
        "Let's use the helper function `get_result_df` to download the result and view it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_result_df(client: FeathrClient) -> pd.DataFrame:\n",
        "    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n",
        "    res_url = client.get_job_result_uri(block=True, timeout_sec=600)\n",
        "    tmp_dir = tempfile.TemporaryDirectory()\n",
        "    client.feathr_spark_launcher.download_result(result_path=res_url, local_folder=tmp_dir.name)\n",
        "    dataframe_list = []\n",
        "    # assuming the result are in avro format\n",
        "    for file in glob.glob(os.path.join(tmp_dir.name, '*.avro')):\n",
        "        dataframe_list.append(pdx.read_avro(file))\n",
        "    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
        "    tmp_dir.cleanup()\n",
        "    return vertical_concat_df\n",
        "\n",
        "df_res = get_result_df(feathr_client)\n",
        "\n",
        "df_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a machine learning model\n",
        "After getting all the features, let's train a machine learning model with the converted feature by Feathr:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop non-feature columns\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "final_df = df_res\n",
        "final_df.drop([\"event_timestamp\"], axis=1, inplace=True, errors='ignore')\n",
        "final_df.fillna(0, inplace=True)\n",
        "final_df['product_rating'] = final_df['product_rating'].astype(\"float64\")\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(final_df.drop([\"product_rating\"], axis=1),\n",
        "                                                    final_df[\"product_rating\"],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "y_predict = model.predict(test_x)\n",
        "\n",
        "y_actual = test_y.values.flatten().tolist()\n",
        "rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
        "\n",
        "sum_actuals = sum_errors = 0\n",
        "\n",
        "for actual_val, predict_val in zip(y_actual, y_predict):\n",
        "    abs_error = actual_val - predict_val\n",
        "    if abs_error < 0:\n",
        "        abs_error = abs_error * -1\n",
        "\n",
        "    sum_errors = sum_errors + abs_error\n",
        "    sum_actuals = sum_actuals + actual_val\n",
        "\n",
        "mean_abs_percent_error = sum_errors / sum_actuals\n",
        "print(\"Model MAPE:\")\n",
        "print(mean_abs_percent_error)\n",
        "print()\n",
        "print(\"Model Accuracy:\")\n",
        "print(1 - mean_abs_percent_error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Materialize feature value into offline/online storage\n",
        "\n",
        "In the previous section, we demonstrated how Feathr can compute feature value to generate training dataset from feature definition on-they-fly.\n",
        "\n",
        "Now let's talk about how we can use the trained models. We can use the trained models for offline inference as well as online inference. In both cases, we need features to be feed into the models. For offline inference, you can compute and get the features on-demand; or you can store the computed features to some offline database for later offline inference.\n",
        "\n",
        "For online inference, we can use Feathr to compute and store the features in the online database. Then use it for online inference when the request comes.\n",
        "\n",
        "![img](../images/online_inference.jpg)\n",
        "\n",
        "\n",
        "In this section, we will focus on materialize features to online store. For materialization to offline store, you can check out our [user guide](https://github.com/feathr-ai/feathr/blob/main/docs/concepts/materializing-features.md#materializing-features-to-offline-store).\n",
        "\n",
        "We can push the computed features to the online store like below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backfill_time = BackfillTime(start=datetime(2020, 5, 20), \n",
        "                             end=datetime(2020, 5, 20), \n",
        "                             step=timedelta(days=1))\n",
        "redisSink = RedisSink(table_name=\"productRecommendationDemoFeature\")\n",
        "settings = MaterializationSettings(name=\"productRecommendationFeatureSetting\",\n",
        "                                   backfill_time=backfill_time,\n",
        "                                   sinks=[redisSink],\n",
        "                                   feature_names=[\"feature_user_age\", \"feature_user_gift_card_balance\"])\n",
        "\n",
        "feathr_client.materialize_features(settings)\n",
        "feathr_client.wait_job_to_finish(timeout_sec=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch feature value from online store\n",
        "We can then get the features from the online store (Redis) via the client's `get_online_features` or `multi_get_online_features` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feathr_client.get_online_features('productRecommendationDemoFeature', \n",
        "                           '2', \n",
        "                           ['feature_user_age', 'feature_user_gift_card_balance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feathr_client.multi_get_online_features('productRecommendationDemoFeature', \n",
        "                                 ['1', '2'], \n",
        "                                 ['feature_user_age', 'feature_user_gift_card_balance'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registering and Fetching features\n",
        "\n",
        "We can also register the features with an Apache Atlas compatible service, such as Azure Purview, and share the registered features across teams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feathr_client.register_features()\n",
        "feathr_client.list_registered_features(project_name=\"feathr_getting_started\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.12 ('ifelse_bug_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "6a6c366ec8f33a88299a9f856c1a3e4312616abcb6fcf46b22c3da0a923e63af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
